{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Processing Capabilities\n",
    "\n",
    "Processes both structured and unstructured clinical oncology data while maintaining semantic relationships\n",
    "\n",
    "## Text Extraction and Document Processing\n",
    "\n",
    "1. Supports multiple input formats: PDF, scanned images, EHR exports\n",
    "1. Implements multi-stage OCR pipeline using Tesseract with specialized medical dictionaries\n",
    "1. Includes image preprocessing (deskewing, noise reduction, binarization)\n",
    "1. Performs layout analysis to preserve document structure\n",
    "1. Implements post-processing with medical terminology verification\n",
    "1. Converts structured EHR data into standardized key-value pair representations\n",
    "1. Preserves semantic relationships in data structure conversion\n",
    "1. Performs document structure analysis to identify hierarchical organization\n",
    "1. Has specialized handlers for clinical document types (operative reports, pathology reports, consultation notes)\n",
    "1. Implements quality control measures and confidence scores for extraction quality\n",
    "\n",
    "## Tokenization and Language Model Integration\n",
    "\n",
    "1. Integrates with HuggingFace Transformers library\n",
    "1. Supports multiple pretrained biomedical tokenizers (BioClinicalBERT, PubMedBERT, GatorTron, ClinicalT5)\n",
    "1. Implements a multi-stage tokenization pipeline:\n",
    "    1. Preliminary text cleaning\n",
    "    1. Text segmentation into appropriate units\n",
    "    1. Subword tokenization using model-specific strategies\n",
    "    1. Special token handling\n",
    "    1. Sequence length management\n",
    "1. Implements strategies for handling long clinical documents:\n",
    "    1. Sliding window tokenization with configurable overlap\n",
    "    1. Hierarchical document segmentation preserving boundaries\n",
    "    1. Important segment identification using term density heuristics\n",
    "    1. Document summarization for extremely long texts\n",
    "1. Optimizes for batch processing with dynamic batch sizing\n",
    "\n",
    "## Clinical Entity Recognition and Normalization\n",
    "\n",
    "1. Utilizes both rule-based and deep learning approaches\n",
    "1. Integrates with medical ontologies (SNOMED CT, RxNorm, LOINC, ICD-O-3)\n",
    "1. Normalizes clinical entities to standardized concept identifiers\n",
    "1. Addresses challenges like synonymy, abbreviation expansion, and term disambiguation\n",
    "1. Implements specialized cancer-specific entity extraction for:\n",
    "    1. Tumor characteristics\n",
    "    1. Staging information\n",
    "    1. Biomarker status\n",
    "    1. Treatment details\n",
    "    1. Response assessment\n",
    "1.  Extracts and normalizes temporal information for patient timelines\n",
    "1. Maintains provenance links to source documents and original text\n",
    "1. Benchmarks performance metrics across public datasets\n",
    "\n",
    "These capabilities appear designed to create a comprehensive framework for processing clinical oncology data for research and analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlC l x70iCWL-I 4 UUID:BD8435C3-C525-4720-8843-EC2CAE22B785 TCGA- OR-A5L -01A-PR Redacted 111 111 III 1111111111 I 1111 I III I I II 1111111111111111111 11111111111111  II III III III 1 1 1 1 1 1 1 1 1 1III 110111111111111111111IIH I11111111111111II IllI111111111111 Pft Thespecimen of a to aly incest , 28g and rrs6m .iring 65x30 x3f rn with a rounded meduilary nom 38 It 4 x rm a grm3 !brr a cu t uface Tice a cc t Coro SWAMto nodulantyP1ght adrenal turnoc r production of testo and. ar crog ra-right al ?1 Ad enaE; adenoma acrflscoplc . riptt A- Representative aakr Ilicroscoplc- Descriptio . eaftm adranal0atti. sawe -W1tumscxUxd nodes composed of , markedly pleornorpt c epitheikd c efts.see photgreph Withabx ,darlt mmy, cytoplasm andonlyDelanl arms ofdeed cols. Up to acolts per W iPF are pies g tMth occasional atypical mitoses .No c sutar, sinusoidal or vacular Invasion is seen .Exdslon appears complete. COMMEN T : Theappearances are most consistent with an adrenal Garda  carcinoma. Or .  reports demonstrates 5 of the Vy sia mien of --- malignancy comprising a mitotic rateof 6 per smpf, edypical mitoses a diff use growth pattern, cytological atypla and ma ,tliccytoplasm. There is no vascular orskxnokW Irv n.There i snoextra. spread. There Is no emulative necrosis. T em Is no perineural growfticed onthese Bides local WW WW appeare complete. lmmunOhistOchemiecy is as follows Kt-67 proliferation Irxdex : 8 1GF2 Diffusely positive pednucloar dot-lice pSUBm Gluc ocortfcoid rte: Strong diffusely 4 positive Is immwnoistrhqmM.ai proff ie is typical of adowwW carcinoma and further supports the diatgtx:sis ofThe at renal lesion leaLOW GRADE ADRENAL. CORTICAL CARCINOMA. Themalignant ekdisplay onoocytic cytological features .The tumour malignancy. Final Summary ,WRENAL GLAND, RIGHT ADRENAL CORTICAL CARCINOMA; MARGINS CLEAR Page tof t sioferred br. ,. -....., ,....rA.i:wxarr ..r....:,:r.:S :.ta.3cai tT? uu-ie;;.;.?t:i:: Y. !:Y!.. :7if F?ttaCtri5:??:.Fir;2;,:;i..f..:.Requesting loators ikon: ROW Tc MISOPAThOLQOY F rucj tW: . MICRO SCOPIC: The adrenal lesion Is a LO WGRADE ADRENAL CORTICAL CARCINOMA. The mal jgnW cellsc p aya ofmcytologi cal featLires .The tumour demonstrates 5 of thecy compris ing arnitotla rate of 6 per 50 hpf,atypical mitoses a d g rowth pattern, cytological atypla and eosinophillo .icytoplasm .There is no vascular or sinusoidal invasion .There is noextra -adrenal spread. There is no coagulative necrosis.There Is noparineural growth .Based on these slides local excision appears complete. lmmunohist ochernistry Is as follows; l-67 proliferative index: 8 1GF Diffusely positive perinuclear do-like pattern Gluc or. orticoid receptor. Strongly diffusely 4 positive This immunchistoc herniaal profile is types of adre nal cortical carcinoma andfurther supports the diagnosis of malignancy. SUMMARY: Adrenal gland: Adrenal cortical carcinoma. REPORTING PATHOLOGIST: Eiactr nfc 5i tuie 71 DinFncsis Di;crNarry L ---- j Prmaty u,cr Site Disrrenry IH!P1 A Dicrrrpanty ---- -7--- Prioi t1? nsnrvi..n ..., --1 - 1Giterla IAM Dw01Syn nrunn narv Notr Case is circle: Rc J.e wer lnil r\n"
     ]
    }
   ],
   "source": [
    "from honeybee.loaders import PDF\n",
    "\n",
    "pdf_report = PDF(chunk_size=512, chunk_overlap=10)\n",
    "text = pdf_report.read(\"/mnt/d/TCGA/raw/TCGA-ACC/raw/TCGA-OR-A5JL/Pathology Report/c9f9dc8b-68ca-4a7e-be69-4d23df5a51a1/TCGA-OR-A5JL.BD8435C3-C525-472C-8B43-EC2CAE22B785.PDF\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 12:15:55,471 - __main__ - WARNING - Could not load medical dictionary: [Errno 2] No such file or directory: 'path/to/medical_dictionary'\n",
      "2025-03-16 12:15:55,472 - __main__ - INFO - Loading tokenizer: emilyalsentzer/Bio_ClinicalBERT\n",
      "2025-03-16 12:15:56,292 - __main__ - INFO - Loaded spaCy model: core_web_md\n",
      "2025-03-16 12:15:56,293 - __main__ - INFO - Loaded ontology: snomed_ct\n",
      "2025-03-16 12:15:56,293 - __main__ - INFO - Loaded ontology: rxnorm\n",
      "2025-03-16 12:15:56,297 - __main__ - INFO - Processing file: /mnt/d/TCGA/raw/TCGA-ACC/raw/TCGA-OR-A5JL/Pathology Report/c9f9dc8b-68ca-4a7e-be69-4d23df5a51a1/TCGA-OR-A5JL.BD8435C3-C525-472C-8B43-EC2CAE22B785.PDF\n",
      "2025-03-16 12:15:56,363 - __main__ - INFO - Results saved to /mnt/d/TCGA/raw/TCGA-ACC/raw/TCGA-OR-A5JL/Pathology Report/c9f9dc8b-68ca-4a7e-be69-4d23df5a51a1/TCGA-OR-A5JL.BD8435C3-C525-472C-8B43-EC2CAE22B785.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document type: None\n",
      "Entities found: 0\n",
      "\n",
      "Entities by type:\n",
      "\n",
      "Sample entities:\n",
      "\n",
      "Temporal timeline:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clinical Oncology Data Processing System\n",
    "\n",
    "This package implements comprehensive processing capabilities for oncology data,\n",
    "including text extraction, document processing, tokenization and entity recognition.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# External dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    BertTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    T5Tokenizer\n",
    ")\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Constants\n",
    "SUPPORTED_IMAGE_FORMATS = ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "SUPPORTED_EHR_FORMATS = ['.xml', '.json', '.csv', '.xlsx']\n",
    "CLINICAL_DOCUMENT_TYPES = ['operative_report', 'pathology_report', 'consultation_note', 'progress_note', 'discharge_summary']\n",
    "BIOMEDICAL_MODELS = {\n",
    "    'bioclinicalbert': 'emilyalsentzer/Bio_ClinicalBERT',\n",
    "    'pubmedbert': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "    'gatortron': 'UFNLP/gatortron-base',\n",
    "    'clinicalt5': 'healx/gpt-t5-clinical'\n",
    "}\n",
    "MEDICAL_ONTOLOGIES = {\n",
    "    'snomed_ct': 'SNOMEDCT_US',\n",
    "    'rxnorm': 'RXNORM',\n",
    "    'loinc': 'LNC',\n",
    "    'icd_o_3': 'ICD10CM'\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODULE 1: Text Extraction and Document Processing\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Base class for document processing capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def process(self, input_path: Union[str, Path]) -> Dict:\n",
    "        \"\"\"Process a document and return extracted data\"\"\"\n",
    "        input_path = Path(input_path)\n",
    "        \n",
    "        if not input_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "            \n",
    "        # Determine file type and process accordingly\n",
    "        if input_path.suffix.lower() in SUPPORTED_IMAGE_FORMATS:\n",
    "            return self._process_image_document(input_path)\n",
    "        elif input_path.suffix.lower() in SUPPORTED_EHR_FORMATS:\n",
    "            return self._process_ehr_document(input_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {input_path.suffix}\")\n",
    "    \n",
    "    def _process_image_document(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process image-based documents like PDFs and scanned images\"\"\"\n",
    "        self.logger.info(f\"Processing image document: {file_path}\")\n",
    "        # This would be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _process_ehr_document(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process structured EHR data exports\"\"\"\n",
    "        self.logger.info(f\"Processing EHR document: {file_path}\")\n",
    "        # This would be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def calculate_confidence_score(self, extracted_data: Dict) -> float:\n",
    "        \"\"\"Calculate confidence score for extraction quality\"\"\"\n",
    "        # Implementation would depend on specific metrics\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class OCRProcessor(DocumentProcessor):\n",
    "    \"\"\"Handles OCR processing for image-based documents with medical terminology verification\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__(config)\n",
    "        self.config = {\n",
    "            'tesseract_path': r'/usr/bin/tesseract',\n",
    "            'lang': 'eng+osd',\n",
    "            'config': '--psm 1 --oem 3',\n",
    "            'medical_dict_path': 'path/to/medical_dictionary',\n",
    "            **self.config\n",
    "        }\n",
    "        \n",
    "        # Initialize Tesseract with medical dictionaries\n",
    "        if self.config.get('tesseract_path'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = self.config['tesseract_path']\n",
    "            \n",
    "        # Load medical dictionary for post-processing\n",
    "        self.medical_terms = self._load_medical_dictionary()\n",
    "        \n",
    "    def _load_medical_dictionary(self) -> set:\n",
    "        \"\"\"Load medical dictionary for term verification\"\"\"\n",
    "        try:\n",
    "            with open(self.config['medical_dict_path'], 'r') as f:\n",
    "                return set(line.strip() for line in f)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not load medical dictionary: {e}\")\n",
    "            return set()\n",
    "            \n",
    "    def _process_image_document(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process an image document using OCR pipeline\"\"\"\n",
    "        # 1. Load image\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            images = self._extract_images_from_pdf(file_path)\n",
    "        else:\n",
    "            images = [Image.open(file_path)]\n",
    "            \n",
    "        # 2. Process each image\n",
    "        results = []\n",
    "        for i, img in enumerate(images):\n",
    "            self.logger.info(f\"Processing image {i+1}/{len(images)}\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            preprocessed_img = self._preprocess_image(img)\n",
    "            \n",
    "            # Perform OCR\n",
    "            ocr_text = pytesseract.image_to_string(\n",
    "                preprocessed_img, \n",
    "                lang=self.config['lang'],\n",
    "                config=self.config['config']\n",
    "            )\n",
    "            \n",
    "            # Get layout information\n",
    "            layout_info = self._analyze_layout(preprocessed_img)\n",
    "            \n",
    "            # Perform post-processing with medical terminology verification\n",
    "            processed_text = self._post_process_text(ocr_text)\n",
    "            \n",
    "            # Calculate confidence\n",
    "            confidence = self._calculate_ocr_confidence(ocr_text, preprocessed_img)\n",
    "            \n",
    "            results.append({\n",
    "                'page_num': i+1,\n",
    "                'text': processed_text,\n",
    "                'layout': layout_info,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "            \n",
    "        # 3. Integrate results\n",
    "        document_structure = self._analyze_document_structure(results)\n",
    "        \n",
    "        # Check if we have any results before calculating average confidence\n",
    "        overall_confidence = 0.0\n",
    "        if results:\n",
    "            overall_confidence = sum(r['confidence'] for r in results) / len(results)\n",
    "        \n",
    "        return {\n",
    "            'document_type': self._identify_document_type(results),\n",
    "            'pages': results,\n",
    "            'structure': document_structure,\n",
    "            'full_text': '\\n\\n'.join(r['text'] for r in results),\n",
    "            'overall_confidence': overall_confidence\n",
    "        }\n",
    "        \n",
    "    def _extract_images_from_pdf(self, pdf_path: Path) -> List[Image.Image]:\n",
    "        \"\"\"Extract images from PDF files\"\"\"\n",
    "        # Would use a library like pdf2image\n",
    "        # Placeholder implementation\n",
    "        return []\n",
    "        \n",
    "    def _preprocess_image(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Preprocess image for improved OCR\"\"\"\n",
    "        # Apply deskewing\n",
    "        img = self._deskew_image(img)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if img.mode != 'L':\n",
    "            img = img.convert('L')\n",
    "            \n",
    "        # Apply noise reduction\n",
    "        img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "        \n",
    "        # Enhance contrast\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img = enhancer.enhance(2)\n",
    "        \n",
    "        # Apply binarization\n",
    "        img = img.point(lambda p: p > 128 and 255)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    def _deskew_image(self, img: Image.Image) -> Image.Image:\n",
    "        \"\"\"Deskew image for better OCR results\"\"\"\n",
    "        # Convert PIL Image to cv2 format\n",
    "        cv_img = np.array(img)\n",
    "        \n",
    "        # Implementation would detect and correct skew angle\n",
    "        # Placeholder for actual implementation\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        return Image.fromarray(cv_img)\n",
    "        \n",
    "    def _analyze_layout(self, img: Image.Image) -> Dict:\n",
    "        \"\"\"Analyze document layout\"\"\"\n",
    "        # Use pytesseract to get bounding boxes and layout\n",
    "        layout_data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        # Extract columns, paragraphs, etc.\n",
    "        # Placeholder for actual implementation\n",
    "        \n",
    "        return {\n",
    "            'columns': [],\n",
    "            'paragraphs': [],\n",
    "            'tables': []\n",
    "        }\n",
    "        \n",
    "    def _post_process_text(self, text: str) -> str:\n",
    "        \"\"\"Apply post-processing with medical terminology verification\"\"\"\n",
    "        # Apply basic cleanup\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        \n",
    "        # Medical terminology verification\n",
    "        words = text.split()\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Check if word is in medical dictionary or apply correction\n",
    "            if word.lower() in self.medical_terms:\n",
    "                corrected_words.append(word)\n",
    "            else:\n",
    "                # Apply fuzzy matching or correction\n",
    "                # Placeholder for actual implementation\n",
    "                corrected_words.append(word)\n",
    "                \n",
    "        return ' '.join(corrected_words)\n",
    "        \n",
    "    def _calculate_ocr_confidence(self, text: str, img: Image.Image) -> float:\n",
    "        \"\"\"Calculate confidence score for OCR quality\"\"\"\n",
    "        # Get confidence scores from Tesseract\n",
    "        try:\n",
    "            data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "            if 'conf' in data:\n",
    "                # Filter out -1 values (which indicate no confidence available)\n",
    "                conf_values = [c for c in data['conf'] if c >= 0]\n",
    "                if conf_values:\n",
    "                    return sum(conf_values) / len(conf_values) / 100.0\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error calculating OCR confidence: {e}\")\n",
    "            \n",
    "        # Default confidence if calculation fails\n",
    "        return 0.5\n",
    "        \n",
    "    def _analyze_document_structure(self, pages: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze hierarchical document structure\"\"\"\n",
    "        # Identify sections, headers, etc.\n",
    "        return {\n",
    "            'sections': [],\n",
    "            'headers': []\n",
    "        }\n",
    "        \n",
    "    def _identify_document_type(self, pages: List[Dict]) -> str:\n",
    "        \"\"\"Identify clinical document type\"\"\"\n",
    "        full_text = ' '.join(page['text'] for page in pages)\n",
    "        \n",
    "        # Simple keyword-based approach\n",
    "        keywords = {\n",
    "            'operative_report': ['operation', 'procedure', 'surgeon', 'preoperative', 'postoperative'],\n",
    "            'pathology_report': ['specimen', 'microscopic', 'diagnosis', 'pathology'],\n",
    "            'consultation_note': ['consultation', 'reason for consultation', 'impression', 'plan'],\n",
    "            'progress_note': ['progress', 'subjective', 'objective', 'assessment', 'plan'],\n",
    "            'discharge_summary': ['discharge', 'hospital course', 'follow up', 'medications']\n",
    "        }\n",
    "        \n",
    "        scores = {}\n",
    "        for doc_type, words in keywords.items():\n",
    "            scores[doc_type] = sum(1 for word in words if re.search(r'\\b' + word + r'\\b', full_text, re.IGNORECASE))\n",
    "            \n",
    "        # Return the document type with the highest score\n",
    "        if not scores:\n",
    "            return 'unknown'\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "class EHRProcessor(DocumentProcessor):\n",
    "    \"\"\"Handles processing of structured EHR data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def _process_ehr_document(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process structured EHR data exports\"\"\"\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        if suffix == '.json':\n",
    "            data = self._process_json_ehr(file_path)\n",
    "        elif suffix == '.xml':\n",
    "            data = self._process_xml_ehr(file_path)\n",
    "        elif suffix in ['.csv', '.xlsx']:\n",
    "            data = self._process_tabular_ehr(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported EHR format: {suffix}\")\n",
    "            \n",
    "        # Convert to standardized key-value pairs\n",
    "        standardized_data = self._standardize_ehr_data(data)\n",
    "        \n",
    "        # Preserve semantic relationships\n",
    "        semantic_structure = self._preserve_semantic_relationships(standardized_data)\n",
    "        \n",
    "        # Document structure analysis\n",
    "        document_structure = self._analyze_document_structure(semantic_structure)\n",
    "        \n",
    "        return {\n",
    "            'raw_data': data,\n",
    "            'standardized_data': standardized_data,\n",
    "            'semantic_structure': semantic_structure,\n",
    "            'document_structure': document_structure,\n",
    "            'document_type': self._identify_document_type(standardized_data),\n",
    "            'confidence': self.calculate_confidence_score(standardized_data)\n",
    "        }\n",
    "        \n",
    "    def _process_json_ehr(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process JSON EHR data\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "            \n",
    "    def _process_xml_ehr(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process XML EHR data\"\"\"\n",
    "        import xml.etree.ElementTree as ET\n",
    "        \n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Convert XML to dictionary\n",
    "        # This is a simplified implementation - actual XML parsing\n",
    "        # would likely need to handle namespaces and complex structures\n",
    "        result = self._xml_to_dict(root)\n",
    "        return result\n",
    "    \n",
    "    def _xml_to_dict(self, element):\n",
    "        \"\"\"Convert XML element to dictionary recursively\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        # Add attributes\n",
    "        for key, value in element.attrib.items():\n",
    "            result[f\"@{key}\"] = value\n",
    "            \n",
    "        # Add child elements\n",
    "        for child in element:\n",
    "            child_dict = self._xml_to_dict(child)\n",
    "            \n",
    "            # Handle multiple elements with same tag\n",
    "            if child.tag in result:\n",
    "                if not isinstance(result[child.tag], list):\n",
    "                    result[child.tag] = [result[child.tag]]\n",
    "                result[child.tag].append(child_dict)\n",
    "            else:\n",
    "                result[child.tag] = child_dict\n",
    "                \n",
    "        # Add text content if element has no children\n",
    "        if not result and element.text and element.text.strip():\n",
    "            return element.text.strip()\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def _process_tabular_ehr(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Process tabular EHR data (CSV, Excel)\"\"\"\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        if suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif suffix == '.xlsx':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported tabular format: {suffix}\")\n",
    "            \n",
    "        # Convert DataFrame to dictionary\n",
    "        records = df.to_dict(orient='records')\n",
    "        \n",
    "        # Group by any identifiable categories\n",
    "        grouped_data = {}\n",
    "        \n",
    "        # Try to identify patient ID or similar grouping factor\n",
    "        id_columns = [col for col in df.columns if 'id' in col.lower() or 'patient' in col.lower()]\n",
    "        \n",
    "        if id_columns:\n",
    "            primary_key = id_columns[0]\n",
    "            for record in records:\n",
    "                key = record[primary_key]\n",
    "                if key not in grouped_data:\n",
    "                    grouped_data[key] = []\n",
    "                grouped_data[key].append(record)\n",
    "            return grouped_data\n",
    "        else:\n",
    "            # If no grouping factor found, return as is\n",
    "            return {\"records\": records}\n",
    "            \n",
    "    def _standardize_ehr_data(self, data: Dict) -> Dict:\n",
    "        \"\"\"Convert EHR data into standardized key-value pair representations\"\"\"\n",
    "        # Flatten and standardize terminology\n",
    "        standardized = {}\n",
    "        \n",
    "        def flatten_dict(d, parent_key='', sep='.'):\n",
    "            items = []\n",
    "            for k, v in d.items():\n",
    "                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "                \n",
    "                if isinstance(v, dict):\n",
    "                    items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "                elif isinstance(v, list):\n",
    "                    if all(isinstance(x, dict) for x in v):\n",
    "                        for i, item in enumerate(v):\n",
    "                            items.extend(flatten_dict(item, f\"{new_key}[{i}]\", sep=sep).items())\n",
    "                    else:\n",
    "                        items.append((new_key, v))\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            return dict(items)\n",
    "            \n",
    "        # Standardize keys\n",
    "        flattened = flatten_dict(data)\n",
    "        \n",
    "        # Apply standardization rules\n",
    "        for key, value in flattened.items():\n",
    "            # Convert keys to snake_case\n",
    "            std_key = re.sub(r'([A-Z])', r'_\\1', key).lower()\n",
    "            std_key = re.sub(r'[^a-z0-9_.]', '_', std_key)\n",
    "            std_key = re.sub(r'_{2,}', '_', std_key)\n",
    "            \n",
    "            # Standardize common terms\n",
    "            for old, new in [\n",
    "                ('patient_id', 'patient_identifier'),\n",
    "                ('dob', 'date_of_birth'),\n",
    "                ('gender', 'biological_sex'),\n",
    "                ('medication', 'drug'),\n",
    "                ('rx', 'prescription'),\n",
    "                ('dx', 'diagnosis')\n",
    "            ]:\n",
    "                std_key = re.sub(rf'\\b{old}\\b', new, std_key)\n",
    "                \n",
    "            standardized[std_key] = value\n",
    "            \n",
    "        return standardized\n",
    "        \n",
    "    def _preserve_semantic_relationships(self, data: Dict) -> Dict:\n",
    "        \"\"\"Preserve semantic relationships in data structure conversion\"\"\"\n",
    "        # Identify related fields\n",
    "        relationships = {}\n",
    "        \n",
    "        # Group related fields by common prefixes\n",
    "        prefixes = {}\n",
    "        for key in data.keys():\n",
    "            parts = key.split('.')\n",
    "            for i in range(1, len(parts)):\n",
    "                prefix = '.'.join(parts[:i])\n",
    "                if prefix not in prefixes:\n",
    "                    prefixes[prefix] = []\n",
    "                prefixes[prefix].append(key)\n",
    "                \n",
    "        # Create relationship groups\n",
    "        for prefix, keys in prefixes.items():\n",
    "            if len(keys) > 1:  # Only create group if multiple related fields\n",
    "                relationships[prefix] = {k: data[k] for k in keys}\n",
    "                \n",
    "        # Build a tree structure for hierarchical relationships\n",
    "        tree = {}\n",
    "        for key, value in data.items():\n",
    "            parts = key.split('.')\n",
    "            current = tree\n",
    "            for i, part in enumerate(parts):\n",
    "                if i == len(parts) - 1:\n",
    "                    current[part] = value\n",
    "                else:\n",
    "                    if part not in current:\n",
    "                        current[part] = {}\n",
    "                    current = current[part]\n",
    "                    \n",
    "        return {\n",
    "            'flat': data,\n",
    "            'relationships': relationships,\n",
    "            'hierarchical': tree\n",
    "        }\n",
    "        \n",
    "    def _analyze_document_structure(self, data: Dict) -> Dict:\n",
    "        \"\"\"Identify hierarchical organization in the document\"\"\"\n",
    "        # Extract sections from hierarchical data\n",
    "        sections = []\n",
    "        \n",
    "        def find_sections(d, path=''):\n",
    "            for key, value in d.items():\n",
    "                if isinstance(value, dict):\n",
    "                    sections.append({\n",
    "                        'name': key,\n",
    "                        'path': f\"{path}.{key}\" if path else key,\n",
    "                        'depth': len(path.split('.')) + 1 if path else 1\n",
    "                    })\n",
    "                    find_sections(value, f\"{path}.{key}\" if path else key)\n",
    "                    \n",
    "        find_sections(data.get('hierarchical', {}))\n",
    "        \n",
    "        # Sort sections by path for consistent order\n",
    "        sections.sort(key=lambda x: x['path'])\n",
    "        \n",
    "        return {\n",
    "            'sections': sections,\n",
    "            'max_depth': max(s['depth'] for s in sections) if sections else 0,\n",
    "            'section_count': len(sections)\n",
    "        }\n",
    "        \n",
    "    def _identify_document_type(self, data: Dict) -> str:\n",
    "        \"\"\"Identify clinical document type from standardized data\"\"\"\n",
    "        # Extract all keys and values to a single string for simple analysis\n",
    "        text = ' '.join([\n",
    "            f\"{k} {v}\" if isinstance(v, str) else k\n",
    "            for k, v in data.items()\n",
    "        ])\n",
    "        \n",
    "        # Simple keyword-based approach\n",
    "        keywords = {\n",
    "            'operative_report': ['operation', 'procedure', 'surgeon', 'preoperative', 'postoperative'],\n",
    "            'pathology_report': ['specimen', 'microscopic', 'diagnosis', 'pathology'],\n",
    "            'consultation_note': ['consultation', 'reason for consultation', 'impression', 'plan'],\n",
    "            'progress_note': ['progress', 'subjective', 'objective', 'assessment', 'plan'],\n",
    "            'discharge_summary': ['discharge', 'hospital course', 'follow up', 'medications']\n",
    "        }\n",
    "        \n",
    "        scores = {}\n",
    "        for doc_type, words in keywords.items():\n",
    "            scores[doc_type] = sum(1 for word in words if re.search(r'\\b' + word + r'\\b', text, re.IGNORECASE))\n",
    "            \n",
    "        # Return the document type with the highest score\n",
    "        if not scores:\n",
    "            return 'unknown'\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "    def calculate_confidence_score(self, data: Dict) -> float:\n",
    "        \"\"\"Calculate confidence score for extraction quality\"\"\"\n",
    "        # Check data completeness and consistency\n",
    "        if not data:\n",
    "            return 0.0\n",
    "            \n",
    "        # A simple scoring mechanism based on completeness\n",
    "        total_fields = 0\n",
    "        non_empty_fields = 0\n",
    "        \n",
    "        def count_fields(d):\n",
    "            nonlocal total_fields, non_empty_fields\n",
    "            \n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, dict):\n",
    "                    count_fields(v)\n",
    "                else:\n",
    "                    total_fields += 1\n",
    "                    if v is not None and (not isinstance(v, str) or v.strip()):\n",
    "                        non_empty_fields += 1\n",
    "                        \n",
    "        count_fields(data)\n",
    "        \n",
    "        if total_fields == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Basic completeness score\n",
    "        completeness = non_empty_fields / total_fields\n",
    "        \n",
    "        # Could be expanded with consistency checks\n",
    "        return completeness\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODULE 2: Tokenization and Language Model Integration\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class TokenizationProcessor:\n",
    "    \"\"\"\n",
    "    Handles tokenization and language model integration for clinical text\n",
    "    with support for multiple biomedical models and strategies for long documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'bioclinicalbert', config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Set defaults\n",
    "        self.config = {\n",
    "            'max_length': 512,\n",
    "            'stride': 128,\n",
    "            'batch_size': 16,\n",
    "            'dynamic_batch_sizing': True,\n",
    "            'segment_min_length': 10,\n",
    "            'segment_strategy': 'sentence',  # 'sentence', 'paragraph', 'fixed'\n",
    "            'special_token_handling': True,\n",
    "            'long_document_strategy': 'sliding_window',  # 'sliding_window', 'hierarchical', 'important_segments', 'summarize'\n",
    "            **self.config\n",
    "        }\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        if model_name not in BIOMEDICAL_MODELS:\n",
    "            self.logger.warning(f\"Unknown model: {model_name}, defaulting to bioclinicalbert\")\n",
    "            self.model_name = 'bioclinicalbert'\n",
    "            \n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "        \n",
    "    def _load_tokenizer(self):\n",
    "        \"\"\"Load the specified biomedical tokenizer\"\"\"\n",
    "        model_path = BIOMEDICAL_MODELS[self.model_name]\n",
    "        self.logger.info(f\"Loading tokenizer: {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            if 't5' in self.model_name.lower():\n",
    "                return T5Tokenizer.from_pretrained(model_path)\n",
    "            elif 'bert' in self.model_name.lower():\n",
    "                return BertTokenizer.from_pretrained(model_path)\n",
    "            else:\n",
    "                return AutoTokenizer.from_pretrained(model_path)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "            self.logger.info(\"Falling back to default tokenizer\")\n",
    "            return AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "            \n",
    "    def tokenize_document(self, document: Union[str, Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Tokenize a clinical document with a multi-stage tokenization pipeline\n",
    "        \n",
    "        Args:\n",
    "            document: Either a string (raw text) or a dictionary with processed text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with tokenization results\n",
    "        \"\"\"\n",
    "        # Extract text from document if it's a dictionary\n",
    "        if isinstance(document, dict):\n",
    "            text = document.get('full_text', '')\n",
    "            if not text and 'pages' in document:\n",
    "                text = '\\n\\n'.join(page.get('text', '') for page in document['pages'])\n",
    "            document_type = document.get('document_type', 'unknown')\n",
    "        else:\n",
    "            text = document\n",
    "            document_type = 'unknown'\n",
    "            \n",
    "        # Step 1: Preliminary text cleaning\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        \n",
    "        # Step 2: Text segmentation\n",
    "        segments = self._segment_text(cleaned_text)\n",
    "        \n",
    "        # Step 3: Apply tokenization strategy based on document length\n",
    "        if len(segments) * self.config['segment_min_length'] > self.config['max_length']:\n",
    "            # Long document\n",
    "            tokenized_result = self._handle_long_document(segments, document_type)\n",
    "        else:\n",
    "            # Short document - standard tokenization\n",
    "            tokenized_result = self._tokenize_segments(segments)\n",
    "            \n",
    "        return {\n",
    "            'tokens': tokenized_result['tokens'],\n",
    "            'token_ids': tokenized_result['token_ids'],\n",
    "            'segment_mapping': tokenized_result['segment_mapping'],\n",
    "            'attention_mask': tokenized_result['attention_mask'],\n",
    "            'special_tokens': tokenized_result.get('special_tokens', []),\n",
    "            'truncated': tokenized_result.get('truncated', False),\n",
    "            'num_segments': len(segments),\n",
    "            'document_type': document_type\n",
    "        }\n",
    "        \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Perform preliminary text cleaning\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Fix common OCR errors\n",
    "        text = re.sub(r'rnl', 'ml', text)  # Fix common OCR error for \"ml\"\n",
    "        text = re.sub(r'I<eukocytes', 'Leukocytes', text)  # Fix common OCR error\n",
    "        \n",
    "        # Remove non-printable characters\n",
    "        text = ''.join(c if c.isprintable() or c in ['\\n', '\\t'] else ' ' for c in text)\n",
    "        \n",
    "        # Normalize medical abbreviations\n",
    "        abbreviations = {\n",
    "            r'\\bpt\\b': 'patient',\n",
    "            r'\\bDx\\b': 'diagnosis',\n",
    "            r'\\bTx\\b': 'treatment',\n",
    "            r'\\bHx\\b': 'history',\n",
    "            r'\\bRx\\b': 'prescription',\n",
    "        }\n",
    "        \n",
    "        for abbr, expansion in abbreviations.items():\n",
    "            text = re.sub(abbr, expansion, text)\n",
    "            \n",
    "        return text\n",
    "        \n",
    "    def _segment_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Segment text into appropriate units\"\"\"\n",
    "        strategy = self.config['segment_strategy']\n",
    "        \n",
    "        if strategy == 'sentence':\n",
    "            # Split by sentences\n",
    "            segments = sent_tokenize(text)\n",
    "        elif strategy == 'paragraph':\n",
    "            # Split by paragraphs (double newlines)\n",
    "            segments = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
    "        elif strategy == 'fixed':\n",
    "            # Split by fixed character length\n",
    "            fixed_length = self.config.get('fixed_segment_length', 200)\n",
    "            segments = [text[i:i+fixed_length] for i in range(0, len(text), fixed_length)]\n",
    "        else:\n",
    "            # Default to sentence\n",
    "            segments = sent_tokenize(text)\n",
    "            \n",
    "        # Remove empty segments and ensure minimum length\n",
    "        min_length = self.config['segment_min_length']\n",
    "        segments = [s for s in segments if len(s) >= min_length]\n",
    "        \n",
    "        return segments\n",
    "        \n",
    "    def _tokenize_segments(self, segments: List[str]) -> Dict:\n",
    "        \"\"\"Tokenize a list of text segments\"\"\"\n",
    "        all_tokens = []\n",
    "        all_token_ids = []\n",
    "        segment_mapping = []  # Maps tokens to original segments\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            # Tokenize the segment\n",
    "            encoding = self.tokenizer(\n",
    "                segment,\n",
    "                add_special_tokens=self.config['special_token_handling'],\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=False,\n",
    "                return_offsets_mapping=False,\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            )\n",
    "            \n",
    "            # Get tokens\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "            \n",
    "            # Add to result\n",
    "            all_tokens.extend(tokens)\n",
    "            all_token_ids.extend(encoding['input_ids'])\n",
    "            segment_mapping.extend([i] * len(tokens))\n",
    "            \n",
    "        # Create attention mask (1 for all tokens)\n",
    "        attention_mask = [1] * len(all_token_ids)\n",
    "        \n",
    "        return {\n",
    "            'tokens': all_tokens,\n",
    "            'token_ids': all_token_ids,\n",
    "            'segment_mapping': segment_mapping,\n",
    "            'attention_mask': attention_mask,\n",
    "            'truncated': False\n",
    "        }\n",
    "        \n",
    "    def _handle_long_document(self, segments: List[str], document_type: str) -> Dict:\n",
    "        \"\"\"Handle tokenization for long documents\"\"\"\n",
    "        strategy = self.config['long_document_strategy']\n",
    "        \n",
    "        if strategy == 'sliding_window':\n",
    "            return self._sliding_window_tokenization(segments)\n",
    "        elif strategy == 'hierarchical':\n",
    "            return self._hierarchical_tokenization(segments, document_type)\n",
    "        elif strategy == 'important_segments':\n",
    "            return self._important_segments_tokenization(segments, document_type)\n",
    "        elif strategy == 'summarize':\n",
    "            return self._summarize_and_tokenize(segments)\n",
    "        else:\n",
    "            # Default to sliding window\n",
    "            return self._sliding_window_tokenization(segments)\n",
    "            \n",
    "    def _sliding_window_tokenization(self, segments: List[str]) -> Dict:\n",
    "        \"\"\"Apply sliding window tokenization for long documents\"\"\"\n",
    "        max_length = self.config['max_length']\n",
    "        stride = self.config['stride']\n",
    "        \n",
    "        # Combine segments into text\n",
    "        text = ' '.join(segments)\n",
    "        \n",
    "        # Tokenize with sliding window\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_overflowing_tokens=True,\n",
    "            stride=stride,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        all_tokens = []\n",
    "        all_token_ids = []\n",
    "        all_attention_masks = []\n",
    "        window_mapping = []  # Maps tokens to window\n",
    "        \n",
    "        for i, window_ids in enumerate(encoding['input_ids']):\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(window_ids)\n",
    "            all_tokens.extend(tokens)\n",
    "            all_token_ids.extend(window_ids)\n",
    "            all_attention_masks.extend(encoding['attention_mask'][i])\n",
    "            window_mapping.extend([i] * len(tokens))\n",
    "            \n",
    "        return {\n",
    "            'tokens': all_tokens,\n",
    "            'token_ids': all_token_ids,\n",
    "            'segment_mapping': window_mapping,  # Maps to window instead of original segment\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'windows': len(encoding['input_ids']),\n",
    "            'window_size': max_length,\n",
    "            'stride': stride,\n",
    "            'truncated': True  # Indicates that the document was processed in windows\n",
    "        }\n",
    "        \n",
    "    def _hierarchical_tokenization(self, segments: List[str], document_type: str) -> Dict:\n",
    "        \"\"\"Apply hierarchical tokenization preserving document boundaries\"\"\"\n",
    "        # Group segments into sections based on document type\n",
    "        sections = self._identify_document_sections(segments, document_type)\n",
    "        \n",
    "        all_tokens = []\n",
    "        all_token_ids = []\n",
    "        all_attention_masks = []\n",
    "        section_mapping = []  # Maps tokens to section\n",
    "        segment_mapping = []  # Maps tokens to original segments\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            # Tokenize each section\n",
    "            section_text = ' '.join(segments[i] for i in section['segment_indices'])\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                section_text,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            )\n",
    "            \n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "            \n",
    "            # Add to result\n",
    "            all_tokens.extend(tokens)\n",
    "            all_token_ids.extend(encoding['input_ids'])\n",
    "            all_attention_masks.extend(encoding['attention_mask'])\n",
    "            section_mapping.extend([section_idx] * len(tokens))\n",
    "            \n",
    "            # Map to original segments (approximate)\n",
    "            token_count = len(tokens)\n",
    "            segment_count = len(section['segment_indices'])\n",
    "            \n",
    "            if segment_count == 0:\n",
    "                continue\n",
    "                \n",
    "            tokens_per_segment = token_count / segment_count\n",
    "            \n",
    "            for i, seg_idx in enumerate(section['segment_indices']):\n",
    "                start = int(i * tokens_per_segment)\n",
    "                end = int((i + 1) * tokens_per_segment)\n",
    "                segment_mapping.extend([seg_idx] * (end - start))\n",
    "                \n",
    "            # Adjust if lengths don't match\n",
    "            while len(segment_mapping) < len(all_tokens):\n",
    "                segment_mapping.append(section['segment_indices'][-1])\n",
    "                \n",
    "        return {\n",
    "            'tokens': all_tokens,\n",
    "            'token_ids': all_token_ids,\n",
    "            'segment_mapping': segment_mapping,\n",
    "            'section_mapping': section_mapping,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'sections': sections,\n",
    "            'hierarchical': True,\n",
    "            'truncated': False\n",
    "        }\n",
    "        \n",
    "    def _identify_document_sections(self, segments: List[str], document_type: str) -> List[Dict]:\n",
    "        \"\"\"Identify document sections based on document type and content\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Simple heuristic section identification\n",
    "        section_headers = {\n",
    "            'operative_report': ['preoperative diagnosis', 'procedure', 'findings', 'postoperative diagnosis'],\n",
    "            'pathology_report': ['clinical history', 'gross description', 'microscopic description', 'diagnosis'],\n",
    "            'consultation_note': ['chief complaint', 'history', 'physical examination', 'assessment', 'plan'],\n",
    "            'progress_note': ['subjective', 'objective', 'assessment', 'plan'],\n",
    "            'discharge_summary': ['admission diagnosis', 'hospital course', 'discharge diagnosis', 'follow up']\n",
    "        }\n",
    "        \n",
    "        # Get section headers for document type\n",
    "        headers = section_headers.get(document_type, [])\n",
    "        if not headers:\n",
    "            # Generic section detection\n",
    "            headers = [h for sublist in section_headers.values() for h in sublist]\n",
    "            \n",
    "        # Find sections based on headers\n",
    "        current_section = {'name': 'preamble', 'segment_indices': []}\n",
    "        sections.append(current_section)\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            # Check if segment starts a new section\n",
    "            is_header = False\n",
    "            for header in headers:\n",
    "                if re.search(rf'\\b{re.escape(header)}\\b', segment.lower()):\n",
    "                    # Start new section\n",
    "                    current_section = {'name': header, 'segment_indices': []}\n",
    "                    sections.append(current_section)\n",
    "                    is_header = True\n",
    "                    break\n",
    "                    \n",
    "            # Add segment to current section\n",
    "            current_section['segment_indices'].append(i)\n",
    "            \n",
    "        return sections\n",
    "        \n",
    "    def _important_segments_tokenization(self, segments: List[str], document_type: str) -> Dict:\n",
    "        \"\"\"Tokenize document focusing on important segments using term density\"\"\"\n",
    "        # Identify important segments using term density heuristics\n",
    "        important_segments = self._identify_important_segments(segments, document_type)\n",
    "        \n",
    "        # Tokenize only important segments\n",
    "        selected_segments = [segments[i] for i in important_segments]\n",
    "        \n",
    "        # Standard tokenization for selected segments\n",
    "        result = self._tokenize_segments(selected_segments)\n",
    "        \n",
    "        # Map back to original segment indices\n",
    "        adjusted_mapping = [important_segments[idx] for idx in result['segment_mapping']]\n",
    "        result['segment_mapping'] = adjusted_mapping\n",
    "        result['important_segments'] = important_segments\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def _identify_important_segments(self, segments: List[str], document_type: str) -> List[int]:\n",
    "        \"\"\"Identify important segments using term density heuristics\"\"\"\n",
    "        # Terms to look for based on document type\n",
    "        important_terms = {\n",
    "            'operative_report': ['performed', 'surgery', 'incision', 'procedure', 'finding'],\n",
    "            'pathology_report': ['tumor', 'lesion', 'malignant', 'benign', 'margin', 'diagnosis'],\n",
    "            'consultation_note': ['assessment', 'recommend', 'impression', 'plan'],\n",
    "            'progress_note': ['assessment', 'plan', 'change', 'improve', 'worsen'],\n",
    "            'discharge_summary': ['discharge', 'follow-up', 'medication', 'instruction'],\n",
    "            'unknown': ['diagnosis', 'result', 'finding', 'treatment', 'plan']\n",
    "        }\n",
    "        \n",
    "        terms = important_terms.get(document_type, important_terms['unknown'])\n",
    "        \n",
    "        # Score segments by term density\n",
    "        scores = []\n",
    "        for i, segment in enumerate(segments):\n",
    "            text = segment.lower()\n",
    "            # Count term occurrences\n",
    "            count = sum(text.count(term) for term in terms)\n",
    "            # Calculate term density\n",
    "            density = count / (len(text.split()) + 1)  # +1 to avoid division by zero\n",
    "            scores.append((i, density))\n",
    "            \n",
    "        # Sort by score and take top segments up to max_length\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take top segments (limit to 70% of max_length)\n",
    "        max_segments = int(self.config['max_length'] * 0.7 / self.config['segment_min_length'])\n",
    "        \n",
    "        # Always include first and last segments (often contain key information)\n",
    "        top_segments = [idx for idx, _ in scores[:max_segments]]\n",
    "        if 0 not in top_segments and segments:\n",
    "            top_segments.append(0)\n",
    "        if len(segments) - 1 not in top_segments and len(segments) > 1:\n",
    "            top_segments.append(len(segments) - 1)\n",
    "            \n",
    "        # Sort by original order\n",
    "        top_segments.sort()\n",
    "        \n",
    "        return top_segments\n",
    "        \n",
    "    def _summarize_and_tokenize(self, segments: List[str]) -> Dict:\n",
    "        \"\"\"Summarize document and tokenize the summary for extremely long documents\"\"\"\n",
    "        # Simple extractive summarization\n",
    "        summary_segments = self._extractive_summarize(segments)\n",
    "        \n",
    "        # Tokenize summary\n",
    "        result = self._tokenize_segments(summary_segments)\n",
    "        \n",
    "        result['summarized'] = True\n",
    "        result['summary_ratio'] = len(summary_segments) / len(segments)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def _extractive_summarize(self, segments: List[str]) -> List[str]:\n",
    "        \"\"\"Simple extractive summarization\"\"\"\n",
    "        if len(segments) <= 10:\n",
    "            return segments\n",
    "            \n",
    "        # Include first two and last two segments\n",
    "        summary = [segments[0], segments[1]]\n",
    "        \n",
    "        # Add some segments from the middle\n",
    "        mid_segments = segments[2:-2]\n",
    "        \n",
    "        # Take every nth segment based on length\n",
    "        n = max(1, len(mid_segments) // 6)\n",
    "        summary.extend(mid_segments[::n])\n",
    "        \n",
    "        # Add last segments\n",
    "        if len(segments) > 3:\n",
    "            summary.append(segments[-2])\n",
    "        if len(segments) > 2:\n",
    "            summary.append(segments[-1])\n",
    "            \n",
    "        return summary\n",
    "        \n",
    "    def prepare_batch(self, documents: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepare a batch of documents for processing with dynamic batch sizing\n",
    "        \n",
    "        Args:\n",
    "            documents: List of tokenized documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with batched input tensors\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return None\n",
    "            \n",
    "        batch_size = self.config['batch_size']\n",
    "        dynamic_sizing = self.config['dynamic_batch_sizing']\n",
    "        \n",
    "        if dynamic_sizing:\n",
    "            # Adjust batch size based on document length\n",
    "            avg_length = sum(len(doc['token_ids']) for doc in documents) / len(documents)\n",
    "            adjusted_size = max(1, int(batch_size * (512 / avg_length)))\n",
    "            batch_size = min(adjusted_size, len(documents))\n",
    "            \n",
    "        # Take a subset of documents for current batch\n",
    "        batch_docs = documents[:batch_size]\n",
    "        \n",
    "        # Determine max sequence length in batch\n",
    "        max_length = max(len(doc['token_ids']) for doc in batch_docs)\n",
    "        \n",
    "        # Prepare input tensors\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        segment_ids = []\n",
    "        \n",
    "        for doc in batch_docs:\n",
    "            # Pad to max length\n",
    "            ids = doc['token_ids']\n",
    "            attn = doc.get('attention_mask', [1] * len(ids))\n",
    "            \n",
    "            # Padding\n",
    "            padding_length = max_length - len(ids)\n",
    "            ids = ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "            attn = attn + [0] * padding_length\n",
    "            \n",
    "            input_ids.append(ids)\n",
    "            attention_masks.append(attn)\n",
    "            segment_ids.append(doc.get('segment_mapping', [0] * len(ids)))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_masks),\n",
    "            'segment_ids': torch.tensor(segment_ids)\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODULE 3: Clinical Entity Recognition and Normalization\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class EntityRecognitionProcessor:\n",
    "    \"\"\"\n",
    "    Handles clinical entity recognition and normalization,\n",
    "    specializing in oncology-specific entities and temporal information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Set defaults\n",
    "        self.config = {\n",
    "            'use_spacy': True,\n",
    "            'use_rules': True,\n",
    "            'use_deep_learning': True,\n",
    "            'ontologies': ['snomed_ct', 'rxnorm', 'loinc', 'icd_o_3'],\n",
    "            'entity_types': ['condition', 'medication', 'procedure', 'observation', 'biomarker', 'tumor', 'staging'],\n",
    "            'abbreviation_expansion': True,\n",
    "            'term_disambiguation': True,\n",
    "            'cancer_specific_extraction': True,\n",
    "            'temporal_extraction': True,\n",
    "            **self.config\n",
    "        }\n",
    "        \n",
    "        # Initialize NLP components\n",
    "        self.nlp = self._initialize_nlp()\n",
    "        \n",
    "        # Load ontologies\n",
    "        self.ontologies = self._load_ontologies()\n",
    "        \n",
    "        # Load cancer-specific extractors\n",
    "        self.cancer_extractors = self._initialize_cancer_extractors()\n",
    "        \n",
    "    def _initialize_nlp(self):\n",
    "        \"\"\"Initialize NLP components\"\"\"\n",
    "        if not self.config['use_spacy']:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Load spaCy model\n",
    "            import spacy\n",
    "            try:\n",
    "                # Try to load clinical model first\n",
    "                nlp = spacy.load(\"en_core_sci_md\")\n",
    "            except OSError:\n",
    "                # Fall back to standard model\n",
    "                nlp = spacy.load(\"en_core_web_md\")\n",
    "                \n",
    "            self.logger.info(f\"Loaded spaCy model: {nlp.meta['name']}\")\n",
    "            return nlp\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load spaCy model: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def _load_ontologies(self) -> Dict:\n",
    "        \"\"\"Load medical ontologies for normalization\"\"\"\n",
    "        ontologies = {}\n",
    "        \n",
    "        for ontology_name in self.config['ontologies']:\n",
    "            ontology_key = ontology_name.lower()\n",
    "            \n",
    "            if ontology_key not in MEDICAL_ONTOLOGIES:\n",
    "                self.logger.warning(f\"Unknown ontology: {ontology_name}\")\n",
    "                continue\n",
    "                \n",
    "            ontology_id = MEDICAL_ONTOLOGIES[ontology_key]\n",
    "            \n",
    "            # This would normally load from UMLS or other sources\n",
    "            # For the purpose of this implementation, we'll create placeholders\n",
    "            ontologies[ontology_key] = {\n",
    "                'id': ontology_id,\n",
    "                'concepts': self._load_ontology_concepts(ontology_key),\n",
    "                'synonyms': self._load_ontology_synonyms(ontology_key),\n",
    "                'relations': self._load_ontology_relations(ontology_key)\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"Loaded ontology: {ontology_key}\")\n",
    "            \n",
    "        return ontologies\n",
    "        \n",
    "    def _load_ontology_concepts(self, ontology_name: str) -> Dict:\n",
    "        \"\"\"Load ontology concepts (placeholder)\"\"\"\n",
    "        # This would normally load from a database or file\n",
    "        # For demonstration, return minimal sample data\n",
    "        if ontology_name == 'snomed_ct':\n",
    "            return {\n",
    "                '254837009': {'name': 'Malignant neoplasm of breast', 'type': 'disorder'},\n",
    "                '13979006': {'name': 'Lung cancer', 'type': 'disorder'},\n",
    "                '118576006': {'name': 'Pathologic TNM stage', 'type': 'observable entity'}\n",
    "            }\n",
    "        elif ontology_name == 'rxnorm':\n",
    "            return {\n",
    "                '1309226': {'name': 'Tamoxifen', 'type': 'substance'},\n",
    "                '1309209': {'name': 'Docetaxel', 'type': 'substance'},\n",
    "                '1551504': {'name': 'Trastuzumab', 'type': 'substance'}\n",
    "            }\n",
    "        # Add more cases for other ontologies\n",
    "        return {}\n",
    "        \n",
    "    def _load_ontology_synonyms(self, ontology_name: str) -> Dict:\n",
    "        \"\"\"Load ontology synonyms (placeholder)\"\"\"\n",
    "        # This would normally load from a database or file\n",
    "        if ontology_name == 'snomed_ct':\n",
    "            return {\n",
    "                'breast cancer': ['254837009'],\n",
    "                'carcinoma of breast': ['254837009'],\n",
    "                'mammary carcinoma': ['254837009'],\n",
    "                'lung cancer': ['13979006'],\n",
    "                'carcinoma of lung': ['13979006'],\n",
    "                'bronchogenic carcinoma': ['13979006'],\n",
    "                'tnm staging': ['118576006'],\n",
    "                'tnm classification': ['118576006']\n",
    "            }\n",
    "        elif ontology_name == 'rxnorm':\n",
    "            return {\n",
    "                'tamoxifen': ['1309226'],\n",
    "                'nolvadex': ['1309226'],\n",
    "                'docetaxel': ['1309209'],\n",
    "                'taxotere': ['1309209'],\n",
    "                'trastuzumab': ['1551504'],\n",
    "                'herceptin': ['1551504']\n",
    "            }\n",
    "        # Add more cases for other ontologies\n",
    "        return {}\n",
    "        \n",
    "    def _load_ontology_relations(self, ontology_name: str) -> Dict:\n",
    "        \"\"\"Load ontology relationships (placeholder)\"\"\"\n",
    "        # This would normally load from a database or file\n",
    "        if ontology_name == 'snomed_ct':\n",
    "            return {\n",
    "                # Relation format: (source_id, type, target_id)\n",
    "                ('254837009', 'is_a', '363346000'),  # Breast cancer is_a malignant neoplasm\n",
    "                ('13979006', 'is_a', '363346000'),   # Lung cancer is_a malignant neoplasm\n",
    "            }\n",
    "        # Add more cases for other ontologies\n",
    "        return {}\n",
    "        \n",
    "    def _initialize_cancer_extractors(self) -> Dict:\n",
    "        \"\"\"Initialize specialized cancer-specific entity extractors\"\"\"\n",
    "        extractors = {}\n",
    "        \n",
    "        if self.config['cancer_specific_extraction']:\n",
    "            extractors = {\n",
    "                'tumor': TumorExtractor(),\n",
    "                'staging': StagingExtractor(),\n",
    "                'biomarker': BiomarkerExtractor(),\n",
    "                'treatment': TreatmentExtractor(),\n",
    "                'response': ResponseExtractor()\n",
    "            }\n",
    "            \n",
    "        return extractors\n",
    "        \n",
    "    def process(self, document: Union[str, Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Process clinical document for entity recognition and normalization\n",
    "        \n",
    "        Args:\n",
    "            document: Either raw text or a dictionary with processed text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with recognized entities and their properties\n",
    "        \"\"\"\n",
    "        # Extract text from document if it's a dictionary\n",
    "        if isinstance(document, dict):\n",
    "            text = document.get('full_text', '')\n",
    "            if not text and 'pages' in document:\n",
    "                text = '\\n\\n'.join(page.get('text', '') for page in document['pages'])\n",
    "            document_type = document.get('document_type', 'unknown')\n",
    "        else:\n",
    "            text = document\n",
    "            document_type = 'unknown'\n",
    "            \n",
    "        # Initialize results\n",
    "        entities = []\n",
    "        \n",
    "        # 1. Rule-based extraction\n",
    "        if self.config['use_rules']:\n",
    "            rule_entities = self._extract_with_rules(text, document_type)\n",
    "            entities.extend(rule_entities)\n",
    "            \n",
    "        # 2. spaCy-based extraction\n",
    "        if self.config['use_spacy'] and self.nlp:\n",
    "            spacy_entities = self._extract_with_spacy(text)\n",
    "            entities.extend(spacy_entities)\n",
    "            \n",
    "        # 3. Deep learning extraction\n",
    "        if self.config['use_deep_learning']:\n",
    "            dl_entities = self._extract_with_deep_learning(text, document_type)\n",
    "            entities.extend(dl_entities)\n",
    "            \n",
    "        # 4. Cancer-specific extraction\n",
    "        if self.config['cancer_specific_extraction']:\n",
    "            cancer_entities = self._extract_cancer_specific(text, document_type)\n",
    "            entities.extend(cancer_entities)\n",
    "            \n",
    "        # 5. Temporal information extraction\n",
    "        if self.config['temporal_extraction']:\n",
    "            temporal_entities = self._extract_temporal_information(text)\n",
    "            entities.extend(temporal_entities)\n",
    "            \n",
    "        # 6. Entity normalization\n",
    "        normalized_entities = self._normalize_entities(entities)\n",
    "        \n",
    "        # 7. Link to ontologies\n",
    "        linked_entities = self._link_to_ontologies(normalized_entities)\n",
    "        \n",
    "        # 8. Resolve entity relationships\n",
    "        entity_relationships = self._resolve_entity_relationships(linked_entities)\n",
    "        \n",
    "        # 9. Deduplicate entities\n",
    "        unique_entities = self._deduplicate_entities(linked_entities)\n",
    "        \n",
    "        return {\n",
    "            'entities': unique_entities,\n",
    "            'relationships': entity_relationships,\n",
    "            'document_type': document_type,\n",
    "            'entity_counts': self._count_entities_by_type(unique_entities),\n",
    "            'temporal_timeline': self._construct_timeline(unique_entities)\n",
    "        }\n",
    "        \n",
    "    def _extract_with_rules(self, text: str, document_type: str) -> List[Dict]:\n",
    "        \"\"\"Extract entities using rule-based approaches\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Simple pattern matching for common entities\n",
    "        # This would typically include more sophisticated rules\n",
    "        \n",
    "        # Medications\n",
    "        medication_patterns = [\n",
    "            r'\\b(\\w+)\\s+(\\d+\\s*(?:mg|mcg|g|ml))\\b',  # e.g., \"tamoxifen 20 mg\"\n",
    "            r'\\b(\\w+)\\s+(\\d+)\\s*(mg|mcg|g|ml)\\b',    # e.g., \"docetaxel 75 mg\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in medication_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                med_name = match.group(1)\n",
    "                dose = match.group(2)\n",
    "                \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'medication',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'name': med_name,\n",
    "                        'dosage': dose if len(match.groups()) > 1 else None,\n",
    "                        'source': 'rule-based'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Tumor location patterns\n",
    "        tumor_patterns = [\n",
    "            r'\\btumor\\s+in\\s+(?:the\\s+)?(\\w+)\\b',\n",
    "            r'\\b(\\w+)\\s+(?:tumor|mass|lesion|cancer|carcinoma)\\b',\n",
    "            r'\\bmalignant\\s+(?:tumor|mass|lesion)\\s+(?:in|of)\\s+(?:the\\s+)?(\\w+)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in tumor_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                location = match.group(1)\n",
    "                \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'tumor',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'location': location,\n",
    "                        'source': 'rule-based'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # TNM staging patterns\n",
    "        staging_patterns = [\n",
    "            r'\\b(T\\d+[a-z]?)(N\\d+[a-z]?)(M\\d+[a-z]?)\\b',  # e.g., \"T2N0M0\"\n",
    "            r'\\bstage\\s+([0IV]+[a-z]?)\\b',                # e.g., \"stage IIb\"\n",
    "            r'\\b(T\\d+[a-z]?)[\\s,]+(N\\d+[a-z]?)[\\s,]+(M\\d+[a-z]?)\\b'  # e.g., \"T2, N0, M0\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in staging_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                if 'stage' in pattern:\n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'staging',\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'stage': match.group(1),\n",
    "                            'source': 'rule-based'\n",
    "                        }\n",
    "                    })\n",
    "                else:\n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'staging',\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'T': match.group(1),\n",
    "                            'N': match.group(2),\n",
    "                            'M': match.group(3),\n",
    "                            'source': 'rule-based'\n",
    "                        }\n",
    "                    })\n",
    "                    \n",
    "        # Add more rule patterns as needed\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    def _extract_with_spacy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract entities using spaCy\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # Map spaCy entity types to our schema\n",
    "            entity_type = self._map_spacy_entity_type(ent.label_)\n",
    "            \n",
    "            if entity_type:\n",
    "                entities.append({\n",
    "                    'text': ent.text,\n",
    "                    'type': entity_type,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char,\n",
    "                    'properties': {\n",
    "                        'spacy_label': ent.label_,\n",
    "                        'source': 'spacy'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "    def _map_spacy_entity_type(self, spacy_label: str) -> Optional[str]:\n",
    "        \"\"\"Map spaCy entity type to our schema\"\"\"\n",
    "        # Default mapping from spaCy to our types\n",
    "        mapping = {\n",
    "            'DISEASE': 'condition',\n",
    "            'CHEMICAL': 'medication',\n",
    "            'PROCEDURE': 'procedure',\n",
    "            'TEST': 'observation',\n",
    "            'BODY_PART': 'anatomy',\n",
    "        }\n",
    "        \n",
    "        return mapping.get(spacy_label)\n",
    "        \n",
    "    def _extract_with_deep_learning(self, text: str, document_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract entities using deep learning models\n",
    "        \n",
    "        This is a placeholder that would normally use a trained NER model\n",
    "        \"\"\"\n",
    "        # In a real implementation, this would use a pretrained or fine-tuned\n",
    "        # clinical NER model specific to oncology\n",
    "        \n",
    "        # For the purpose of this implementation, return empty\n",
    "        return []\n",
    "        \n",
    "    def _extract_cancer_specific(self, text: str, document_type: str) -> List[Dict]:\n",
    "        \"\"\"Extract cancer-specific entities\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        for extractor_name, extractor in self.cancer_extractors.items():\n",
    "            extracted = extractor.extract(text, document_type)\n",
    "            \n",
    "            for entity in extracted:\n",
    "                # Ensure consistent format\n",
    "                entity['properties'] = entity.get('properties', {})\n",
    "                entity['properties']['source'] = f'cancer-{extractor_name}'\n",
    "                entities.append(entity)\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "    def _extract_temporal_information(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract temporal information for patient timelines\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Date patterns (various formats)\n",
    "        date_patterns = [\n",
    "            r'\\b(\\d{1,2})[/\\-](\\d{1,2})[/\\-](\\d{2,4})\\b',  # MM/DD/YYYY or DD/MM/YYYY\n",
    "            r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\.?\\s+(\\d{1,2})(?:st|nd|rd|th)?,?\\s+(\\d{4})\\b',  # Month DD, YYYY\n",
    "            r'\\b(\\d{1,2})(?:st|nd|rd|th)?\\s+(?:of\\s+)?(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\.?\\s+(\\d{4})\\b',  # DD Month YYYY\n",
    "        ]\n",
    "        \n",
    "        # Time expressions\n",
    "        time_expressions = [\n",
    "            r'\\b(\\d+)\\s+(?:day|week|month|year)s?\\s+(?:ago|before|prior)\\b',\n",
    "            r'\\b(?:last|previous)\\s+(?:day|week|month|year)\\b',\n",
    "            r'\\b(?:next|following)\\s+(?:day|week|month|year)\\b',\n",
    "            r'\\bin\\s+(\\d+)\\s+(?:day|week|month|year)s?\\b',\n",
    "            r'\\b(?:since|after|before)\\s+(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\.?\\s+(\\d{1,2})(?:st|nd|rd|th)?,?\\s+(\\d{4})\\b',\n",
    "        ]\n",
    "        \n",
    "        # Relative time markers\n",
    "        relative_markers = [\n",
    "            r'\\bcurrently\\b',\n",
    "            r'\\bpresently\\b',\n",
    "            r'\\bat\\s+presentation\\b',\n",
    "            r'\\bon\\s+admission\\b',\n",
    "            r'\\bduring\\s+(?:the\\s+)?(?:procedure|operation|treatment|hospitalization)\\b',\n",
    "            r'\\bpost[- ](?:operative|procedure|treatment|therapy)\\b',\n",
    "            r'\\bpre[- ](?:operative|procedure|treatment|therapy)\\b',\n",
    "        ]\n",
    "        \n",
    "        # Extract dates\n",
    "        for pattern in date_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'temporal',\n",
    "                    'subtype': 'date',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'value': match.group(0),\n",
    "                        'source': 'temporal'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Extract time expressions\n",
    "        for pattern in time_expressions:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'temporal',\n",
    "                    'subtype': 'time_expression',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'value': match.group(0),\n",
    "                        'source': 'temporal'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Extract relative markers\n",
    "        for pattern in relative_markers:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'temporal',\n",
    "                    'subtype': 'relative_marker',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'value': match.group(0),\n",
    "                        'source': 'temporal'\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "    def _normalize_entities(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Normalize clinical entities\"\"\"\n",
    "        normalized = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Create a copy to avoid modifying original\n",
    "            norm_entity = entity.copy()\n",
    "            \n",
    "            # Handle abbreviations if enabled\n",
    "            if self.config['abbreviation_expansion'] and 'text' in entity:\n",
    "                expanded = self._expand_abbreviation(entity['text'])\n",
    "                if expanded != entity['text']:\n",
    "                    if 'properties' not in norm_entity:\n",
    "                        norm_entity['properties'] = {}\n",
    "                    norm_entity['properties']['expanded'] = expanded\n",
    "                    \n",
    "            # Apply normalization based on entity type\n",
    "            if entity.get('type') == 'medication':\n",
    "                self._normalize_medication(norm_entity)\n",
    "            elif entity.get('type') == 'condition':\n",
    "                self._normalize_condition(norm_entity)\n",
    "            elif entity.get('type') == 'procedure':\n",
    "                self._normalize_procedure(norm_entity)\n",
    "            elif entity.get('type') == 'tumor':\n",
    "                self._normalize_tumor(norm_entity)\n",
    "            elif entity.get('type') == 'staging':\n",
    "                self._normalize_staging(norm_entity)\n",
    "            elif entity.get('type') == 'biomarker':\n",
    "                self._normalize_biomarker(norm_entity)\n",
    "            elif entity.get('type') == 'temporal':\n",
    "                self._normalize_temporal(norm_entity)\n",
    "                \n",
    "            normalized.append(norm_entity)\n",
    "            \n",
    "        # Apply term disambiguation if enabled\n",
    "        if self.config['term_disambiguation']:\n",
    "            normalized = self._disambiguate_terms(normalized)\n",
    "            \n",
    "        return normalized\n",
    "        \n",
    "    def _expand_abbreviation(self, text: str) -> str:\n",
    "        \"\"\"Expand medical abbreviations\"\"\"\n",
    "        # Common oncology abbreviations\n",
    "        abbreviations = {\n",
    "            # General medical\n",
    "            'pt': 'patient',\n",
    "            'hx': 'history',\n",
    "            'dx': 'diagnosis',\n",
    "            'tx': 'treatment',\n",
    "            'rx': 'prescription',\n",
    "            'sx': 'symptom',\n",
    "            'fx': 'fracture',\n",
    "            \n",
    "            # Oncology specific\n",
    "            'ca': 'cancer',\n",
    "            'mets': 'metastasis',\n",
    "            'chemo': 'chemotherapy',\n",
    "            'rt': 'radiation therapy',\n",
    "            'xrt': 'radiation therapy',\n",
    "            'gy': 'gray',\n",
    "            'cr': 'complete response',\n",
    "            'pr': 'partial response',\n",
    "            'sd': 'stable disease',\n",
    "            'pd': 'progressive disease',\n",
    "            \n",
    "            # Cancer types\n",
    "            'nsclc': 'non-small cell lung cancer',\n",
    "            'sclc': 'small cell lung cancer',\n",
    "            'crc': 'colorectal cancer',\n",
    "            'hcc': 'hepatocellular carcinoma',\n",
    "        }\n",
    "        \n",
    "        # For exact matches only\n",
    "        if text.lower() in abbreviations:\n",
    "            return abbreviations[text.lower()]\n",
    "            \n",
    "        # For more context-aware expansion, we would need more complex algorithms\n",
    "        return text\n",
    "        \n",
    "    def _normalize_medication(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize medication entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Extract and normalize dosage information\n",
    "        if 'text' in entity:\n",
    "            # Extract dosage patterns\n",
    "            dosage_match = re.search(r'(\\d+\\.?\\d*)\\s*(mg|mcg|g|ml|mg/m2)', entity['text'], re.IGNORECASE)\n",
    "            if dosage_match:\n",
    "                entity['properties']['dosage'] = dosage_match.group(1)\n",
    "                entity['properties']['unit'] = dosage_match.group(2).lower()\n",
    "                \n",
    "            # Normalize drug name (lowercase for consistency)\n",
    "            if 'name' in entity['properties']:\n",
    "                entity['properties']['name'] = entity['properties']['name'].lower()\n",
    "                \n",
    "    def _normalize_condition(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize condition entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Extract laterality (left/right)\n",
    "        if 'text' in entity:\n",
    "            laterality_match = re.search(r'\\b(left|right)\\b', entity['text'], re.IGNORECASE)\n",
    "            if laterality_match:\n",
    "                entity['properties']['laterality'] = laterality_match.group(1).lower()\n",
    "                \n",
    "    def _normalize_tumor(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize tumor entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Extract and normalize size information\n",
    "        if 'text' in entity:\n",
    "            size_match = re.search(r'(\\d+\\.?\\d*)\\s*(mm|cm|centimeter|millimeter)', entity['text'], re.IGNORECASE)\n",
    "            if size_match:\n",
    "                entity['properties']['size'] = float(size_match.group(1))\n",
    "                unit = size_match.group(2).lower()\n",
    "                # Convert to standard unit (cm)\n",
    "                if unit.startswith('mm'):\n",
    "                    entity['properties']['size'] /= 10\n",
    "                entity['properties']['size_unit'] = 'cm'\n",
    "                \n",
    "            # Extract location (if not already present)\n",
    "            if 'location' not in entity['properties']:\n",
    "                for organ in ['breast', 'lung', 'liver', 'colon', 'prostate', 'kidney', 'bladder', 'pancreas', 'brain']:\n",
    "                    if re.search(r'\\b' + organ + r'\\b', entity['text'], re.IGNORECASE):\n",
    "                        entity['properties']['location'] = organ\n",
    "                        break\n",
    "                        \n",
    "    def _normalize_staging(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize staging entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Standardize TNM format\n",
    "        for prop in ['T', 'N', 'M']:\n",
    "            if prop in entity['properties']:\n",
    "                # Ensure uppercase and strip spaces\n",
    "                entity['properties'][prop] = entity['properties'][prop].upper().strip()\n",
    "                \n",
    "        # For clinical stage (I, II, III, IV)\n",
    "        if 'stage' in entity['properties']:\n",
    "            # Ensure uppercase roman numerals\n",
    "            stage = entity['properties']['stage']\n",
    "            entity['properties']['stage'] = stage.upper()\n",
    "            \n",
    "    def _normalize_biomarker(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize biomarker entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Standardize biomarker status\n",
    "        if 'status' in entity['properties']:\n",
    "            status = entity['properties']['status'].lower()\n",
    "            \n",
    "            # Map various expressions to standard values\n",
    "            if status in ['positive', 'pos', '+', 'overexpressed']:\n",
    "                entity['properties']['status'] = 'positive'\n",
    "            elif status in ['negative', 'neg', '-', 'not expressed']:\n",
    "                entity['properties']['status'] = 'negative'\n",
    "            elif status in ['equivocal', 'borderline', 'indeterminate']:\n",
    "                entity['properties']['status'] = 'equivocal'\n",
    "                \n",
    "    def _normalize_temporal(self, entity: Dict) -> None:\n",
    "        \"\"\"Normalize temporal entities\"\"\"\n",
    "        if 'properties' not in entity:\n",
    "            entity['properties'] = {}\n",
    "            \n",
    "        # Standardize date formats if possible\n",
    "        if entity.get('subtype') == 'date' and 'value' in entity['properties']:\n",
    "            date_text = entity['properties']['value']\n",
    "            \n",
    "            # Try to parse with various formats\n",
    "            from datetime import datetime\n",
    "            \n",
    "            date_formats = [\n",
    "                '%m/%d/%Y', '%d/%m/%Y',            # MM/DD/YYYY or DD/MM/YYYY\n",
    "                '%m-%d-%Y', '%d-%m-%Y',            # MM-DD-YYYY or DD-MM-YYYY \n",
    "                '%B %d, %Y', '%b %d, %Y',          # Month DD, YYYY\n",
    "                '%d %B %Y', '%d %b %Y'             # DD Month YYYY\n",
    "            ]\n",
    "            \n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(date_text, fmt)\n",
    "                    entity['properties']['normalized_date'] = parsed_date.strftime('%Y-%m-%d')\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "    def _disambiguate_terms(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Disambiguate terms with multiple possible meanings\"\"\"\n",
    "        # For this implementation, focus on common ambiguous terms in oncology\n",
    "        \n",
    "        # Example: \"cold\" could be temperature or common cold\n",
    "        # Example: \"discharge\" could be a substance or the act of leaving hospital\n",
    "        \n",
    "        # This would normally use context information and ML models\n",
    "        # For simplicity, return the original entities\n",
    "        return entities\n",
    "        \n",
    "    def _link_to_ontologies(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Link entities to ontology concepts\"\"\"\n",
    "        linked_entities = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Copy entity to avoid modifying the original\n",
    "            linked_entity = entity.copy()\n",
    "            \n",
    "            # Add ontology_links field if not exists\n",
    "            if 'properties' not in linked_entity:\n",
    "                linked_entity['properties'] = {}\n",
    "                \n",
    "            if 'ontology_links' not in linked_entity['properties']:\n",
    "                linked_entity['properties']['ontology_links'] = []\n",
    "                \n",
    "            entity_text = entity.get('text', '').lower()\n",
    "            entity_type = entity.get('type', '')\n",
    "            \n",
    "            # Attempt to match with ontologies based on entity type\n",
    "            for ontology_name, ontology in self.ontologies.items():\n",
    "                # Check entity text against synonyms dictionary\n",
    "                if entity_text in ontology['synonyms']:\n",
    "                    concept_ids = ontology['synonyms'][entity_text]\n",
    "                    \n",
    "                    for concept_id in concept_ids:\n",
    "                        if concept_id in ontology['concepts']:\n",
    "                            concept = ontology['concepts'][concept_id]\n",
    "                            \n",
    "                            # Add link to ontology\n",
    "                            linked_entity['properties']['ontology_links'].append({\n",
    "                                'ontology': ontology_name,\n",
    "                                'concept_id': concept_id,\n",
    "                                'concept_name': concept['name'],\n",
    "                                'concept_type': concept.get('type', '')\n",
    "                            })\n",
    "                            \n",
    "            linked_entities.append(linked_entity)\n",
    "            \n",
    "        return linked_entities\n",
    "        \n",
    "    def _resolve_entity_relationships(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Identify relationships between entities\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Map entities by type for easier access\n",
    "        entity_by_type = {}\n",
    "        for i, entity in enumerate(entities):\n",
    "            entity_type = entity.get('type', '')\n",
    "            if entity_type not in entity_by_type:\n",
    "                entity_by_type[entity_type] = []\n",
    "            entity_by_type[entity_type].append((i, entity))\n",
    "            \n",
    "        # Relate biomarkers to tumors\n",
    "        if 'biomarker' in entity_by_type and 'tumor' in entity_by_type:\n",
    "            for biomarker_idx, biomarker in entity_by_type['biomarker']:\n",
    "                for tumor_idx, tumor in entity_by_type['tumor']:\n",
    "                    # Check if they might be related (e.g., by proximity in text)\n",
    "                    if abs(biomarker.get('start', 0) - tumor.get('start', 0)) < 500:  # Within 500 chars\n",
    "                        relationships.append({\n",
    "                            'source': biomarker_idx,\n",
    "                            'target': tumor_idx,\n",
    "                            'type': 'associated_with'\n",
    "                        })\n",
    "                        \n",
    "        # Relate staging to tumors\n",
    "        if 'staging' in entity_by_type and 'tumor' in entity_by_type:\n",
    "            for staging_idx, staging in entity_by_type['staging']:\n",
    "                for tumor_idx, tumor in entity_by_type['tumor']:\n",
    "                    if abs(staging.get('start', 0) - tumor.get('start', 0)) < 500:\n",
    "                        relationships.append({\n",
    "                            'source': staging_idx,\n",
    "                            'target': tumor_idx,\n",
    "                            'type': 'classifies'\n",
    "                        })\n",
    "                        \n",
    "        # Relate treatments to conditions\n",
    "        if 'medication' in entity_by_type and 'condition' in entity_by_type:\n",
    "            for med_idx, med in entity_by_type['medication']:\n",
    "                for cond_idx, cond in entity_by_type['condition']:\n",
    "                    if abs(med.get('start', 0) - cond.get('start', 0)) < 500:\n",
    "                        relationships.append({\n",
    "                            'source': med_idx,\n",
    "                            'target': cond_idx,\n",
    "                            'type': 'treats'\n",
    "                        })\n",
    "                        \n",
    "        # Relate temporal information to other entities\n",
    "        if 'temporal' in entity_by_type:\n",
    "            for temp_idx, temp in entity_by_type['temporal']:\n",
    "                # Find entities that are mentioned close to this temporal marker\n",
    "                for other_type, other_entities in entity_by_type.items():\n",
    "                    if other_type != 'temporal':\n",
    "                        for other_idx, other in other_entities:\n",
    "                            if abs(temp.get('start', 0) - other.get('start', 0)) < 200:  # Closer proximity\n",
    "                                relationships.append({\n",
    "                                    'source': other_idx,\n",
    "                                    'target': temp_idx,\n",
    "                                    'type': 'occurred_at'\n",
    "                                })\n",
    "                                \n",
    "        return relationships\n",
    "        \n",
    "    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove duplicate entities\"\"\"\n",
    "        unique_entities = []\n",
    "        seen_spans = set()\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Create a key based on start, end and type\n",
    "            span_key = (entity.get('start'), entity.get('end'), entity.get('type'))\n",
    "            \n",
    "            if span_key not in seen_spans:\n",
    "                seen_spans.add(span_key)\n",
    "                unique_entities.append(entity)\n",
    "                \n",
    "        return unique_entities\n",
    "        \n",
    "    def _count_entities_by_type(self, entities: List[Dict]) -> Dict:\n",
    "        \"\"\"Count entities by type\"\"\"\n",
    "        counts = {}\n",
    "        \n",
    "        for entity in entities:\n",
    "            entity_type = entity.get('type', 'unknown')\n",
    "            if entity_type not in counts:\n",
    "                counts[entity_type] = 0\n",
    "            counts[entity_type] += 1\n",
    "            \n",
    "        return counts\n",
    "        \n",
    "    def _construct_timeline(self, entities: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Construct a patient timeline from temporal and related entities\"\"\"\n",
    "        timeline = []\n",
    "        \n",
    "        # Filter temporal entities\n",
    "        temporal_entities = [e for e in entities if e.get('type') == 'temporal']\n",
    "        \n",
    "        # Create timeline events\n",
    "        for i, entity in enumerate(entities):\n",
    "            if entity.get('type') == 'temporal':\n",
    "                # Find related non-temporal entities\n",
    "                related_entities = []\n",
    "                \n",
    "                for j, other in enumerate(entities):\n",
    "                    if other.get('type') != 'temporal':\n",
    "                        # Check if they are close in the text\n",
    "                        if abs(entity.get('start', 0) - other.get('start', 0)) < 200:\n",
    "                            related_entities.append(j)\n",
    "                            \n",
    "                # Create timeline event\n",
    "                timeline_event = {\n",
    "                    'temporal_entity': i,\n",
    "                    'temporal_text': entity.get('text', ''),\n",
    "                    'temporal_value': entity.get('properties', {}).get('value', ''),\n",
    "                    'normalized_date': entity.get('properties', {}).get('normalized_date', ''),  # Add default empty string\n",
    "                    'related_entities': related_entities\n",
    "                }\n",
    "                \n",
    "                timeline.append(timeline_event)\n",
    "                \n",
    "        # Sort timeline by date if possible\n",
    "        # Use a helper function to provide a key that handles None values\n",
    "        def sort_key(x):\n",
    "            date_val = x.get('normalized_date', '')\n",
    "            return date_val if date_val else ''  # Return empty string for None values\n",
    "            \n",
    "        timeline.sort(key=sort_key)\n",
    "        \n",
    "        return timeline\n",
    "        \n",
    "\n",
    "# Custom extractors for cancer-specific entity types\n",
    "\n",
    "class TumorExtractor:\n",
    "    \"\"\"Extracts tumor characteristics\"\"\"\n",
    "    \n",
    "    def extract(self, text: str, document_type: str) -> List[Dict]:\n",
    "        entities = []\n",
    "        \n",
    "        # Tumor location\n",
    "        location_patterns = [\n",
    "            r'\\b(tumor|mass|lesion|carcinoma)\\s+(?:in|of|on|at)\\s+(?:the\\s+)?(\\w+)\\b',\n",
    "            r'\\b(\\w+)\\s+(tumor|mass|lesion|carcinoma)\\b',\n",
    "            r'\\b(\\w+)\\s+cancer\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in location_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                if 'tumor|mass|lesion|carcinoma' in pattern:\n",
    "                    tumor_type = match.group(1)\n",
    "                    location = match.group(2)\n",
    "                else:\n",
    "                    tumor_type = 'cancer'\n",
    "                    location = match.group(1)\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'tumor',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'location': location,\n",
    "                        'tumor_type': tumor_type\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Tumor size\n",
    "        size_patterns = [\n",
    "            r'(tumor|mass|lesion)\\s+(?:size|measuring)\\s+(\\d+\\.?\\d*)\\s*(mm|cm)',\n",
    "            r'(\\d+\\.?\\d*)\\s*(mm|cm)\\s+(tumor|mass|lesion)',\n",
    "            r'(tumor|mass|lesion)\\s+(\\d+\\.?\\d*)\\s*[x]\\s*(\\d+\\.?\\d*)\\s*[x]?\\s*(?:(\\d+\\.?\\d*)\\s*)?(mm|cm)'  # 3D measurements\n",
    "        ]\n",
    "        \n",
    "        for pattern in size_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                # Handle different pattern matches\n",
    "                if '[x]' in pattern:  # 3D measurement\n",
    "                    tumor_type = match.group(1)\n",
    "                    length = match.group(2)\n",
    "                    width = match.group(3)\n",
    "                    depth = match.group(4) if match.group(4) else None\n",
    "                    unit = match.group(5)\n",
    "                    \n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'tumor',\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'tumor_type': tumor_type,\n",
    "                            'length': float(length),\n",
    "                            'width': float(width),\n",
    "                            'depth': float(depth) if depth else None,\n",
    "                            'unit': unit\n",
    "                        }\n",
    "                    })\n",
    "                else:  # Simple size\n",
    "                    try:\n",
    "                        if match.group(1) in ['tumor', 'mass', 'lesion']:\n",
    "                            tumor_type = match.group(1)\n",
    "                            size = match.group(2)\n",
    "                            unit = match.group(3)\n",
    "                        else:\n",
    "                            size = match.group(1)\n",
    "                            unit = match.group(2)\n",
    "                            tumor_type = match.group(3)\n",
    "                            \n",
    "                        entities.append({\n",
    "                            'text': match.group(0),\n",
    "                            'type': 'tumor',\n",
    "                            'start': match.start(),\n",
    "                            'end': match.end(),\n",
    "                            'properties': {\n",
    "                                'tumor_type': tumor_type,\n",
    "                                'size': float(size),\n",
    "                                'unit': unit\n",
    "                            }\n",
    "                        })\n",
    "                    except (IndexError, ValueError):\n",
    "                        # Skip if pattern matching doesn't work as expected\n",
    "                        pass\n",
    "                        \n",
    "        return entities\n",
    "        \n",
    "\n",
    "class StagingExtractor:\n",
    "    \"\"\"Extracts cancer staging information\"\"\"\n",
    "    \n",
    "    def extract(self, text: str, document_type: str) -> List[Dict]:\n",
    "        entities = []\n",
    "        \n",
    "        # TNM staging\n",
    "        tnm_patterns = [\n",
    "            r'\\b(c|p|y|r|a|u)?T(\\d+[a-z]?)N(\\d+[a-z]?)M(\\d+[a-z]?)\\b',  # Combined format: T2N0M0\n",
    "            r'\\b(?:clinical|pathologic|post-treatment)?\\s*(?:stage|classification)?\\s*:?\\s*(T(\\d+[a-z]?))\\s*,?\\s*(N(\\d+[a-z]?))\\s*,?\\s*(M(\\d+[a-z]?))\\b',  # Separated: T2, N0, M0\n",
    "        ]\n",
    "        \n",
    "        for pattern in tnm_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                if 'c|p|y|r|a|u' in pattern:  # Combined format with prefix\n",
    "                    prefix = match.group(1) or ''\n",
    "                    t_stage = match.group(2)\n",
    "                    n_stage = match.group(3)\n",
    "                    m_stage = match.group(4)\n",
    "                else:  # Separated format\n",
    "                    prefix = ''\n",
    "                    t_stage = match.group(2)\n",
    "                    n_stage = match.group(4)\n",
    "                    m_stage = match.group(6)\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'staging',\n",
    "                    'subtype': 'tnm',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'prefix': prefix.upper() if prefix else None,\n",
    "                        'T': t_stage,\n",
    "                        'N': n_stage,\n",
    "                        'M': m_stage\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # General stage\n",
    "        stage_patterns = [\n",
    "            r'\\b(?:clinical|pathologic|pathological)?\\s*stage\\s*:?\\s*(0|[IVX]+[a-z]?)\\b',\n",
    "            r'\\bstage\\s+(?:is|as)\\s+(0|[IVX]+[a-z]?)\\b',\n",
    "            r'\\b(?:AJCC|FIGO)\\s+stage\\s+(0|[IVX]+[a-z]?)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in stage_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                stage = match.group(1)\n",
    "                \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'staging',\n",
    "                    'subtype': 'stage',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'stage': stage.upper(),\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Grade information\n",
    "        grade_patterns = [\n",
    "            r'\\bgrade\\s+(\\d+)\\b',\n",
    "            r'\\bwell\\s+differentiated\\b',\n",
    "            r'\\bmoderately\\s+differentiated\\b',\n",
    "            r'\\bpoorly\\s+differentiated\\b',\n",
    "            r'\\bundifferentiated\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in grade_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                if 'grade' in pattern:\n",
    "                    grade = match.group(1)\n",
    "                    grade_text = f\"Grade {grade}\"\n",
    "                else:\n",
    "                    grade_text = match.group(0)\n",
    "                    grade = {\n",
    "                        'well differentiated': '1',\n",
    "                        'moderately differentiated': '2',\n",
    "                        'poorly differentiated': '3',\n",
    "                        'undifferentiated': '4'\n",
    "                    }.get(grade_text.lower(), None)\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'staging',\n",
    "                    'subtype': 'grade',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'grade': grade,\n",
    "                        'grade_text': grade_text\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "\n",
    "class BiomarkerExtractor:\n",
    "    \"\"\"Extracts biomarker status information\"\"\"\n",
    "    \n",
    "    def extract(self, text: str, document_type: str) -> List[Dict]:\n",
    "        entities = []\n",
    "        \n",
    "        # Common biomarkers in oncology\n",
    "        biomarkers = {\n",
    "            'breast': ['er', 'estrogen receptor', 'pr', 'progesterone receptor', 'her2', 'her-2', 'brca1', 'brca2'],\n",
    "            'lung': ['egfr', 'alk', 'ros1', 'pd-l1', 'pdl1', 'kras', 'braf'],\n",
    "            'colorectal': ['kras', 'nras', 'braf', 'msi', 'microsatellite instability', 'msi-h', 'mss'],\n",
    "            'melanoma': ['braf', 'nras', 'c-kit'],\n",
    "            'general': ['pd-l1', 'pdl1', 'tmb', 'tumor mutational burden', 'msi', 'microsatellite instability']\n",
    "        }\n",
    "        \n",
    "        # Flatten biomarker list\n",
    "        all_biomarkers = []\n",
    "        for category, markers in biomarkers.items():\n",
    "            all_biomarkers.extend(markers)\n",
    "            \n",
    "        # Remove duplicates\n",
    "        all_biomarkers = list(set(all_biomarkers))\n",
    "        \n",
    "        # Biomarker status patterns\n",
    "        for biomarker in all_biomarkers:\n",
    "            # Use word boundary for shorter biomarkers\n",
    "            if len(biomarker) <= 3:\n",
    "                biomarker_pattern = r'\\b' + re.escape(biomarker) + r'\\b'\n",
    "            else:\n",
    "                biomarker_pattern = re.escape(biomarker)\n",
    "                \n",
    "            status_patterns = [\n",
    "                biomarker_pattern + r'\\s+(?:is|was)?\\s*(positive|negative|equivocal|overexpressed|not expressed|borderline)',\n",
    "                biomarker_pattern + r'(?:\\s+status)?(?:\\s+is|\\:)?\\s*(positive|negative|\\+|\\-|equivocal|overexpressed|not expressed|borderline)',\n",
    "                r'(positive|negative|equivocal|overexpressed|not expressed|borderline)\\s+(?:for)?\\s+' + biomarker_pattern,\n",
    "                biomarker_pattern + r'\\s+expression(?:\\s+is)?\\s*(positive|negative|high|low|equivocal|borderline)',\n",
    "                r'(high|low|positive|negative)\\s+expression\\s+of\\s+' + biomarker_pattern\n",
    "            ]\n",
    "            \n",
    "            for pattern in status_patterns:\n",
    "                for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                    # Identify which group contains the status\n",
    "                    status_group = None\n",
    "                    for i in range(1, match.lastindex + 1 if match.lastindex else 1):\n",
    "                        if match.group(i) and match.group(i).lower() in ['positive', 'negative', 'equivocal', '+', '-', 'overexpressed', 'not expressed', 'borderline', 'high', 'low']:\n",
    "                            status_group = i\n",
    "                            break\n",
    "                            \n",
    "                    if status_group is None:\n",
    "                        continue\n",
    "                        \n",
    "                    status = match.group(status_group)\n",
    "                    \n",
    "                    # Map status values to standard terms\n",
    "                    if status.lower() in ['+', 'overexpressed', 'high']:\n",
    "                        status = 'positive'\n",
    "                    elif status.lower() in ['-', 'not expressed', 'low']:\n",
    "                        status = 'negative'\n",
    "                        \n",
    "                    # Determine which category this biomarker belongs to\n",
    "                    category = 'general'\n",
    "                    for cat, markers in biomarkers.items():\n",
    "                        if biomarker in markers:\n",
    "                            category = cat\n",
    "                            break\n",
    "                            \n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'biomarker',\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'name': biomarker,\n",
    "                            'status': status.lower(),\n",
    "                            'category': category\n",
    "                        }\n",
    "                    })\n",
    "                    \n",
    "        # Percentage patterns (e.g., 'PD-L1 90%')\n",
    "        percentage_patterns = [\n",
    "            r'(' + '|'.join(re.escape(b) for b in all_biomarkers) + r')\\s+(\\d+(?:\\.\\d+)?)%',\n",
    "            r'(\\d+(?:\\.\\d+)?)%\\s+(' + '|'.join(re.escape(b) for b in all_biomarkers) + r')'\n",
    "        ]\n",
    "        \n",
    "        for pattern in percentage_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                if match.group(1) in all_biomarkers:\n",
    "                    biomarker = match.group(1)\n",
    "                    percentage = float(match.group(2))\n",
    "                else:\n",
    "                    biomarker = match.group(2)\n",
    "                    percentage = float(match.group(1))\n",
    "                    \n",
    "                # Determine category\n",
    "                category = 'general'\n",
    "                for cat, markers in biomarkers.items():\n",
    "                    if biomarker in markers:\n",
    "                        category = cat\n",
    "                        break\n",
    "                        \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'biomarker',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'name': biomarker,\n",
    "                        'percentage': percentage,\n",
    "                        'status': 'positive' if percentage > 0 else 'negative',\n",
    "                        'category': category\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "\n",
    "class TreatmentExtractor:\n",
    "    \"\"\"Extracts treatment details\"\"\"\n",
    "    \n",
    "    def extract(self, text: str, document_type: str) -> List[Dict]:\n",
    "        entities = []\n",
    "        \n",
    "        # Common cancer treatments\n",
    "        treatments = {\n",
    "            'chemotherapy': [\n",
    "                'doxorubicin', 'cyclophosphamide', 'paclitaxel', 'docetaxel', 'carboplatin', 'cisplatin',\n",
    "                'gemcitabine', '5-fu', '5-fluorouracil', 'capecitabine', 'adriamycin', 'taxol',\n",
    "                'taxotere', 'folfox', 'folfiri', 'ac-t', 'tc', 'capeox'\n",
    "            ],\n",
    "            'targeted_therapy': [\n",
    "                'trastuzumab', 'herceptin', 'pertuzumab', 'perjeta', 'lapatinib', 'tykerb',\n",
    "                'imatinib', 'gleevec', 'erlotinib', 'tarceva', 'gefitinib', 'iressa',\n",
    "                'bevacizumab', 'avastin', 'rituximab', 'rituxan', 'cetuximab', 'erbitux'\n",
    "            ],\n",
    "            'immunotherapy': [\n",
    "                'pembrolizumab', 'keytruda', 'nivolumab', 'opdivo', 'atezolizumab', 'tecentriq',\n",
    "                'durvalumab', 'imfinzi', 'ipilimumab', 'yervoy', 'immune checkpoint inhibitor',\n",
    "                'pd-1 inhibitor', 'pd-l1 inhibitor', 'ctla-4 inhibitor'\n",
    "            ],\n",
    "            'radiation': [\n",
    "                'radiation therapy', 'radiotherapy', 'external beam radiation', 'imrt', 'sbrt',\n",
    "                'brachytherapy', 'gamma knife', 'stereotactic radiosurgery', 'proton therapy',\n",
    "                'xrt', 'radiation'\n",
    "            ],\n",
    "            'surgery': [\n",
    "                'surgery', 'resection', 'mastectomy', 'lumpectomy', 'partial mastectomy', 'lobectomy',\n",
    "                'radical mastectomy', 'colectomy', 'hemicolectomy', 'prostatectomy',\n",
    "                'orchiectomy', 'hysterectomy', 'oophorectomy', 'nephrectomy', 'pneumonectomy'\n",
    "            ],\n",
    "            'hormone_therapy': [\n",
    "                'tamoxifen', 'nolvadex', 'aromatase inhibitor', 'letrozole', 'femara',\n",
    "                'anastrozole', 'arimidex', 'exemestane', 'aromasin', 'fulvestrant',\n",
    "                'faslodex', 'bicalutamide', 'casodex', 'leuprolide', 'lupron'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Flatten treatment lists\n",
    "        all_treatments = {}\n",
    "        for category, drugs in treatments.items():\n",
    "            for drug in drugs:\n",
    "                all_treatments[drug] = category\n",
    "                \n",
    "        # Treatment patterns\n",
    "        for treatment, category in all_treatments.items():\n",
    "            # Short treatments need word boundaries\n",
    "            if len(treatment) <= 3:\n",
    "                treatment_pattern = r'\\b' + re.escape(treatment) + r'\\b'\n",
    "            else:\n",
    "                treatment_pattern = re.escape(treatment)\n",
    "                \n",
    "            # Look for treatment mentions with dosage\n",
    "            dosage_patterns = [\n",
    "                treatment_pattern + r'\\s+(\\d+(?:\\.\\d+)?)\\s*(mg|mcg|g|mg/m2|mg/kg)',\n",
    "                treatment_pattern + r'\\s+(\\d+(?:\\.\\d+)?)\\s*(mg|mcg|g|mg/m2|mg/kg)(?:/(?:day|week|cycle))?',\n",
    "                r'(\\d+(?:\\.\\d+)?)\\s*(mg|mcg|g|mg/m2|mg/kg)\\s+(?:of\\s+)?' + treatment_pattern\n",
    "            ]\n",
    "            \n",
    "            for pattern in dosage_patterns:\n",
    "                for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                    # Extract treatment name and dosage\n",
    "                    if match.group(1) and match.group(1).replace('.', '', 1).isdigit():\n",
    "                        # First group is dosage\n",
    "                        dosage = match.group(1)\n",
    "                        unit = match.group(2)\n",
    "                        treatment_name = treatment\n",
    "                    else:\n",
    "                        # First group is treatment\n",
    "                        treatment_name = treatment\n",
    "                        try:\n",
    "                            dosage = match.group(2)\n",
    "                            unit = match.group(3)\n",
    "                        except (IndexError, AttributeError):\n",
    "                            dosage = None\n",
    "                            unit = None\n",
    "                            \n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'treatment',\n",
    "                        'subtype': category,\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'name': treatment_name,\n",
    "                            'dosage': float(dosage) if dosage and dosage.replace('.', '', 1).isdigit() else None,\n",
    "                            'unit': unit,\n",
    "                            'category': category\n",
    "                        }\n",
    "                    })\n",
    "                    \n",
    "            # Simple treatment mentions\n",
    "            simple_pattern = treatment_pattern\n",
    "            for match in re.finditer(simple_pattern, text, re.IGNORECASE):\n",
    "                # Check if this match overlaps with any existing entities\n",
    "                overlap = False\n",
    "                for entity in entities:\n",
    "                    if (match.start() >= entity['start'] and match.start() < entity['end']) or \\\n",
    "                       (match.end() > entity['start'] and match.end() <= entity['end']):\n",
    "                        overlap = True\n",
    "                        break\n",
    "                        \n",
    "                if not overlap:\n",
    "                    entities.append({\n",
    "                        'text': match.group(0),\n",
    "                        'type': 'treatment',\n",
    "                        'subtype': category,\n",
    "                        'start': match.start(),\n",
    "                        'end': match.end(),\n",
    "                        'properties': {\n",
    "                            'name': treatment,\n",
    "                            'category': category\n",
    "                        }\n",
    "                    })\n",
    "                    \n",
    "        # Treatment regimen patterns\n",
    "        regimen_patterns = [\n",
    "            r'\\b(AC-T|TC|FOLFOX|FOLFIRI|FEC|CMF|TAC|TCH|CHOP|R-CHOP|ABVD|BEP|MVAC|ECF)\\b',\n",
    "            r'\\b((?:neo)?adjuvant\\s+(?:chemo|radio|hormone)?therapy)\\b',\n",
    "            r'\\b(systemic\\s+therapy)\\b',\n",
    "            r'\\b((?:neo)?adjuvant\\s+treatment)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in regimen_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                regimen = match.group(1)\n",
    "                \n",
    "                # Determine category based on regimen name\n",
    "                category = 'regimen'\n",
    "                if any(chemo in regimen.lower() for chemo in ['chemo', 'folfox', 'folfiri', 'ac-t', 'tc']):\n",
    "                    category = 'chemotherapy'\n",
    "                elif any(rad in regimen.lower() for rad in ['radio']):\n",
    "                    category = 'radiation'\n",
    "                elif any(hormone in regimen.lower() for hormone in ['hormone']):\n",
    "                    category = 'hormone_therapy'\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'treatment',\n",
    "                    'subtype': 'regimen',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'name': regimen,\n",
    "                        'category': category\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "        \n",
    "\n",
    "class ResponseExtractor:\n",
    "    \"\"\"Extracts treatment response assessments\"\"\"\n",
    "    \n",
    "    def extract(self, text: str, document_type: str) -> List[Dict]:\n",
    "        entities = []\n",
    "        \n",
    "        # Response patterns (RECIST criteria)\n",
    "        response_patterns = [\n",
    "            r'\\b(complete\\s+(?:response|remission))\\b',\n",
    "            r'\\b(partial\\s+response)\\b',\n",
    "            r'\\b(stable\\s+disease)\\b',\n",
    "            r'\\b(progressive\\s+disease|progression)\\b',\n",
    "            r'\\bcr\\b',\n",
    "            r'\\bpr\\b',\n",
    "            r'\\bsd\\b',\n",
    "            r'\\bpd\\b',\n",
    "            r'\\bno\\s+evidence\\s+of\\s+(?:disease|recurrence)\\b',\n",
    "            r'\\bned\\b',\n",
    "            r'\\bresponse\\s+to\\s+(?:treatment|therapy)\\s+(?:was|is)?\\s+(complete|partial|minimal|none)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in response_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                response_text = match.group(0)\n",
    "                \n",
    "                # Determine response category\n",
    "                if re.search(r'complete|cr\\b|no\\s+evidence|ned', response_text, re.IGNORECASE):\n",
    "                    response_type = 'complete response'\n",
    "                elif re.search(r'partial|pr\\b', response_text, re.IGNORECASE):\n",
    "                    response_type = 'partial response'\n",
    "                elif re.search(r'stable|sd\\b', response_text, re.IGNORECASE):\n",
    "                    response_type = 'stable disease'\n",
    "                elif re.search(r'progress|pd\\b', response_text, re.IGNORECASE):\n",
    "                    response_type = 'progressive disease'\n",
    "                else:\n",
    "                    response_type = 'unspecified response'\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': response_text,\n",
    "                    'type': 'response',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': {\n",
    "                        'response_type': response_type,\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "        # Measurement-based responses\n",
    "        measurement_patterns = [\n",
    "            r'(tumor|mass|lesion)\\s+(?:size|diameter|measurement)?\\s+(?:has|is)?\\s+(decreased|increased|unchanged)',\n",
    "            r'(tumor|mass|lesion)\\s+(?:has|shows)\\s+(decreased|increased)\\s+(?:in\\s+)?(?:size|diameter)',\n",
    "            r'((\\d+)%)\\s+(reduction|increase|decrease)\\s+in\\s+(tumor|mass|lesion)',\n",
    "            r'(tumor|mass|lesion)\\s+(reduction|increase|decrease)\\s+(?:of|by)\\s+((\\d+)%)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in measurement_patterns:\n",
    "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                # Handle different pattern types\n",
    "                if '%' in pattern:\n",
    "                    if match.group(1) and '%' in match.group(1):\n",
    "                        percent = match.group(2)\n",
    "                        direction = match.group(3)\n",
    "                        subject = match.group(4)\n",
    "                    else:\n",
    "                        subject = match.group(1)\n",
    "                        direction = match.group(2)\n",
    "                        percent = match.group(4)\n",
    "                else:\n",
    "                    subject = match.group(1)\n",
    "                    direction = match.group(2)\n",
    "                    percent = None\n",
    "                    \n",
    "                # Map direction to response type\n",
    "                if direction.lower() in ['decreased', 'reduction', 'decrease']:\n",
    "                    response_type = 'partial response' if percent and int(percent) >= 30 else 'minor response'\n",
    "                elif direction.lower() in ['increased', 'increase']:\n",
    "                    response_type = 'progressive disease' if percent and int(percent) >= 20 else 'minor progression'\n",
    "                else:\n",
    "                    response_type = 'stable disease'\n",
    "                    \n",
    "                properties = {\n",
    "                    'response_type': response_type,\n",
    "                    'subject': subject,\n",
    "                    'direction': direction\n",
    "                }\n",
    "                \n",
    "                if percent:\n",
    "                    properties['percent_change'] = int(percent)\n",
    "                    \n",
    "                entities.append({\n",
    "                    'text': match.group(0),\n",
    "                    'type': 'response',\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'properties': properties\n",
    "                })\n",
    "                \n",
    "        return entities\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Processor: Combines all modules\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ClinicalOncologyProcessor:\n",
    "    \"\"\"\n",
    "    Main processor that combines all modules for end-to-end processing of \n",
    "    clinical oncology data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Set defaults\n",
    "        self.config = {\n",
    "            'document_processor': {\n",
    "                'use_ocr': True,\n",
    "                'use_ehr': True\n",
    "            },\n",
    "            'tokenization': {\n",
    "                'model': 'bioclinicalbert',\n",
    "                'max_length': 512,\n",
    "                'segment_strategy': 'sentence',\n",
    "                'long_document_strategy': 'sliding_window'\n",
    "            },\n",
    "            'entity_recognition': {\n",
    "                'use_rules': True,\n",
    "                'use_spacy': True,\n",
    "                'use_deep_learning': False,\n",
    "                'cancer_specific_extraction': True,\n",
    "                'temporal_extraction': True,\n",
    "                'ontologies': ['snomed_ct', 'rxnorm']\n",
    "            },\n",
    "            'processing_pipeline': ['document', 'tokenization', 'entity_recognition'],\n",
    "            'output': {\n",
    "                'include_raw_text': True,\n",
    "                'include_tokens': True,\n",
    "                'include_entities': True,\n",
    "                'include_document_structure': True,\n",
    "                'include_temporal_timeline': True\n",
    "            },\n",
    "            **self.config\n",
    "        }\n",
    "        \n",
    "        # Initialize processors\n",
    "        self._init_processors()\n",
    "        \n",
    "    def _init_processors(self):\n",
    "        \"\"\"Initialize all processor components\"\"\"\n",
    "        # Document processors\n",
    "        self.ocr_processor = OCRProcessor(self.config.get('document_processor', {})) if self.config['document_processor']['use_ocr'] else None\n",
    "        self.ehr_processor = EHRProcessor(self.config.get('document_processor', {})) if self.config['document_processor']['use_ehr'] else None\n",
    "        \n",
    "        # Tokenization processor\n",
    "        self.tokenization_processor = TokenizationProcessor(\n",
    "            self.config['tokenization'].get('model', 'bioclinicalbert'),\n",
    "            self.config.get('tokenization', {})\n",
    "        )\n",
    "        \n",
    "        # Entity recognition processor\n",
    "        self.entity_processor = EntityRecognitionProcessor(self.config.get('entity_recognition', {}))\n",
    "        \n",
    "    def process(self, input_path: Union[str, Path], save_output: bool = False, output_path: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a clinical oncology document.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input document\n",
    "            save_output: Whether to save processing output to file\n",
    "            output_path: Path to save output (if None, use input_path with .json extension)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        input_path = Path(input_path)\n",
    "        \n",
    "        if not input_path.exists():\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "            \n",
    "        self.logger.info(f\"Processing file: {input_path}\")\n",
    "        \n",
    "        # Initialize results\n",
    "        results = {\n",
    "            'file_path': str(input_path),\n",
    "            'file_name': input_path.name,\n",
    "            'file_extension': input_path.suffix,\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Step 1: Document Processing\n",
    "        if 'document' in self.config['processing_pipeline']:\n",
    "            document_result = self._process_document(input_path)\n",
    "            results['document'] = document_result\n",
    "            \n",
    "            # Include raw text if configured\n",
    "            if self.config['output']['include_raw_text']:\n",
    "                results['text'] = document_result.get('full_text', '')\n",
    "                \n",
    "            # Include document structure if configured\n",
    "            if self.config['output']['include_document_structure']:\n",
    "                results['document_structure'] = document_result.get('structure', {})\n",
    "                \n",
    "        # Step 2: Tokenization\n",
    "        if 'tokenization' in self.config['processing_pipeline']:\n",
    "            tokenization_result = self._process_tokenization(document_result if 'document' in self.config['processing_pipeline'] else input_path)\n",
    "            \n",
    "            # Include tokens if configured\n",
    "            if self.config['output']['include_tokens']:\n",
    "                results['tokenization'] = tokenization_result\n",
    "                \n",
    "        # Step 3: Entity Recognition\n",
    "        if 'entity_recognition' in self.config['processing_pipeline']:\n",
    "            entity_result = self._process_entity_recognition(\n",
    "                document_result if 'document' in self.config['processing_pipeline'] else input_path\n",
    "            )\n",
    "            \n",
    "            # Include entities if configured\n",
    "            if self.config['output']['include_entities']:\n",
    "                results['entities'] = entity_result['entities']\n",
    "                results['entity_relationships'] = entity_result['relationships']\n",
    "                \n",
    "            # Include temporal timeline if configured\n",
    "            if self.config['output']['include_temporal_timeline']:\n",
    "                results['temporal_timeline'] = entity_result['temporal_timeline']\n",
    "                \n",
    "        # Save output if requested\n",
    "        if save_output:\n",
    "            if output_path is None:\n",
    "                output_path = input_path.with_suffix('.json')\n",
    "            else:\n",
    "                output_path = Path(output_path)\n",
    "                \n",
    "            self._save_results(results, output_path)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def _process_document(self, input_path: Path) -> Dict:\n",
    "        \"\"\"Process document with appropriate processor\"\"\"\n",
    "        suffix = input_path.suffix.lower()\n",
    "        \n",
    "        if suffix in SUPPORTED_IMAGE_FORMATS and self.ocr_processor:\n",
    "            return self.ocr_processor.process(input_path)\n",
    "        elif suffix in SUPPORTED_EHR_FORMATS and self.ehr_processor:\n",
    "            return self.ehr_processor.process(input_path)\n",
    "        else:\n",
    "            raise ValueError(f\"No suitable processor for file type: {suffix}\")\n",
    "            \n",
    "    def _process_tokenization(self, document: Union[Path, Dict]) -> Dict:\n",
    "        \"\"\"Process document with tokenization processor\"\"\"\n",
    "        return self.tokenization_processor.tokenize_document(document)\n",
    "        \n",
    "    def _process_entity_recognition(self, document: Union[Path, Dict]) -> Dict:\n",
    "        \"\"\"Process document with entity recognition processor\"\"\"\n",
    "        return self.entity_processor.process(document)\n",
    "        \n",
    "    def _save_results(self, results: Dict, output_path: Path) -> None:\n",
    "        \"\"\"Save processing results to file\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "            \n",
    "        self.logger.info(f\"Results saved to {output_path}\")\n",
    "        \n",
    "    def process_batch(self, input_dir: Union[str, Path], file_pattern: str = '*', save_output: bool = False, output_dir: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a batch of clinical oncology documents.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: Directory containing input documents\n",
    "            file_pattern: Glob pattern for selecting files\n",
    "            save_output: Whether to save processing output to files\n",
    "            output_dir: Directory to save output (if None, use input_dir)\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with processing results\n",
    "        \"\"\"\n",
    "        input_dir = Path(input_dir)\n",
    "        \n",
    "        if not input_dir.exists() or not input_dir.is_dir():\n",
    "            raise NotADirectoryError(f\"Input directory not found: {input_dir}\")\n",
    "            \n",
    "        # Get list of files matching pattern\n",
    "        files = list(input_dir.glob(file_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            self.logger.warning(f\"No files found matching pattern: {file_pattern}\")\n",
    "            return []\n",
    "            \n",
    "        self.logger.info(f\"Processing {len(files)} files\")\n",
    "        \n",
    "        # Process each file\n",
    "        results = []\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                if output_dir:\n",
    "                    output_path = Path(output_dir) / file_path.with_suffix('.json').name\n",
    "                else:\n",
    "                    output_path = file_path.with_suffix('.json')\n",
    "                    \n",
    "                result = self.process(file_path, save_output=save_output, output_path=output_path)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing {file_path}: {e}\")\n",
    "                \n",
    "        return results\n",
    "        \n",
    "    def process_text(self, text: str, document_type: str = 'unknown') -> Dict:\n",
    "        \"\"\"\n",
    "        Process raw text.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text to process\n",
    "            document_type: Type of clinical document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        # Create a document-like structure\n",
    "        document = {\n",
    "            'full_text': text,\n",
    "            'document_type': document_type\n",
    "        }\n",
    "        \n",
    "        # Initialize results\n",
    "        results = {\n",
    "            'document_type': document_type,\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Include raw text if configured\n",
    "        if self.config['output']['include_raw_text']:\n",
    "            results['text'] = text\n",
    "            \n",
    "        # Step 1: Tokenization\n",
    "        if 'tokenization' in self.config['processing_pipeline']:\n",
    "            tokenization_result = self.tokenization_processor.tokenize_document(document)\n",
    "            \n",
    "            # Include tokens if configured\n",
    "            if self.config['output']['include_tokens']:\n",
    "                results['tokenization'] = tokenization_result\n",
    "                \n",
    "        # Step 2: Entity Recognition\n",
    "        if 'entity_recognition' in self.config['processing_pipeline']:\n",
    "            entity_result = self.entity_processor.process(document)\n",
    "            \n",
    "            # Include entities if configured\n",
    "            if self.config['output']['include_entities']:\n",
    "                results['entities'] = entity_result['entities']\n",
    "                results['entity_relationships'] = entity_result['relationships']\n",
    "                \n",
    "            # Include temporal timeline if configured\n",
    "            if self.config['output']['include_temporal_timeline']:\n",
    "                results['temporal_timeline'] = entity_result['temporal_timeline']\n",
    "                \n",
    "        return results\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_config(config_path: Union[str, Path]) -> Dict:\n",
    "    \"\"\"Load configuration from JSON file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def initialize_processor(config_path: Optional[Union[str, Path]] = None) -> ClinicalOncologyProcessor:\n",
    "    \"\"\"Initialize processor with optional configuration\"\"\"\n",
    "    if config_path:\n",
    "        config = load_config(config_path)\n",
    "    else:\n",
    "        config = {}\n",
    "        \n",
    "    return ClinicalOncologyProcessor(config)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Processor: Combines all modules\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ClinicalOncologyProcessor:\n",
    "    \"\"\"\n",
    "    Main processor that combines all modules for end-to-end processing of \n",
    "    clinical oncology data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Set defaults\n",
    "        self.config = {\n",
    "            'document_processor': {\n",
    "                'use_ocr': True,\n",
    "                'use_ehr': True\n",
    "            },\n",
    "            'tokenization': {\n",
    "                'model': 'bioclinicalbert',\n",
    "                'max_length': 512,\n",
    "                'segment_strategy': 'sentence',\n",
    "                'long_document_strategy': 'sliding_window'\n",
    "            },\n",
    "            'entity_recognition': {\n",
    "                'use_rules': True,\n",
    "                'use_spacy': True,\n",
    "                'use_deep_learning': False,\n",
    "                'cancer_specific_extraction': True,\n",
    "                'temporal_extraction': True,\n",
    "                'ontologies': ['snomed_ct', 'rxnorm']\n",
    "            },\n",
    "            'processing_pipeline': ['document', 'tokenization', 'entity_recognition'],\n",
    "            'output': {\n",
    "                'include_raw_text': True,\n",
    "                'include_tokens': True,\n",
    "                'include_entities': True,\n",
    "                'include_document_structure': True,\n",
    "                'include_temporal_timeline': True\n",
    "            },\n",
    "            **self.config\n",
    "        }\n",
    "        \n",
    "        # Initialize processors\n",
    "        self._init_processors()\n",
    "        \n",
    "    def _init_processors(self):\n",
    "        \"\"\"Initialize all processor components\"\"\"\n",
    "        # Document processors\n",
    "        self.ocr_processor = OCRProcessor(self.config.get('document_processor', {})) if self.config['document_processor']['use_ocr'] else None\n",
    "        self.ehr_processor = EHRProcessor(self.config.get('document_processor', {})) if self.config['document_processor']['use_ehr'] else None\n",
    "        \n",
    "        # Tokenization processor\n",
    "        self.tokenization_processor = TokenizationProcessor(\n",
    "            self.config['tokenization'].get('model', 'bioclinicalbert'),\n",
    "            self.config.get('tokenization', {})\n",
    "        )\n",
    "        \n",
    "        # Entity recognition processor\n",
    "        self.entity_processor = EntityRecognitionProcessor(self.config.get('entity_recognition', {}))\n",
    "        \n",
    "    def process(self, input_path: Union[str, Path], save_output: bool = False, output_path: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a clinical oncology document.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input document\n",
    "            save_output: Whether to save processing output to file\n",
    "            output_path: Path to save output (if None, use input_path with .json extension)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        input_path = Path(input_path)\n",
    "        \n",
    "        if not input_path.exists():\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "            \n",
    "        self.logger.info(f\"Processing file: {input_path}\")\n",
    "        \n",
    "        # Initialize results\n",
    "        results = {\n",
    "            'file_path': str(input_path),\n",
    "            'file_name': input_path.name,\n",
    "            'file_extension': input_path.suffix,\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Step 1: Document Processing\n",
    "        if 'document' in self.config['processing_pipeline']:\n",
    "            document_result = self._process_document(input_path)\n",
    "            results['document'] = document_result\n",
    "            \n",
    "            # Include raw text if configured\n",
    "            if self.config['output']['include_raw_text']:\n",
    "                results['text'] = document_result.get('full_text', '')\n",
    "                \n",
    "            # Include document structure if configured\n",
    "            if self.config['output']['include_document_structure']:\n",
    "                results['document_structure'] = document_result.get('structure', {})\n",
    "                \n",
    "        # Step 2: Tokenization\n",
    "        if 'tokenization' in self.config['processing_pipeline']:\n",
    "            tokenization_result = self._process_tokenization(document_result if 'document' in self.config['processing_pipeline'] else input_path)\n",
    "            \n",
    "            # Include tokens if configured\n",
    "            if self.config['output']['include_tokens']:\n",
    "                results['tokenization'] = tokenization_result\n",
    "                \n",
    "        # Step 3: Entity Recognition\n",
    "        if 'entity_recognition' in self.config['processing_pipeline']:\n",
    "            entity_result = self._process_entity_recognition(\n",
    "                document_result if 'document' in self.config['processing_pipeline'] else input_path\n",
    "            )\n",
    "            \n",
    "            # Include entities if configured\n",
    "            if self.config['output']['include_entities']:\n",
    "                results['entities'] = entity_result['entities']\n",
    "                results['entity_relationships'] = entity_result['relationships']\n",
    "                \n",
    "            # Include temporal timeline if configured\n",
    "            if self.config['output']['include_temporal_timeline']:\n",
    "                results['temporal_timeline'] = entity_result['temporal_timeline']\n",
    "                \n",
    "        # Save output if requested\n",
    "        if save_output:\n",
    "            if output_path is None:\n",
    "                output_path = input_path.with_suffix('.json')\n",
    "            else:\n",
    "                output_path = Path(output_path)\n",
    "                \n",
    "            self._save_results(results, output_path)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def _process_document(self, input_path: Path) -> Dict:\n",
    "        \"\"\"Process document with appropriate processor\"\"\"\n",
    "        suffix = input_path.suffix.lower()\n",
    "        \n",
    "        if suffix in SUPPORTED_IMAGE_FORMATS and self.ocr_processor:\n",
    "            return self.ocr_processor.process(input_path)\n",
    "        elif suffix in SUPPORTED_EHR_FORMATS and self.ehr_processor:\n",
    "            return self.ehr_processor.process(input_path)\n",
    "        else:\n",
    "            raise ValueError(f\"No suitable processor for file type: {suffix}\")\n",
    "            \n",
    "    def _process_tokenization(self, document: Union[Path, Dict]) -> Dict:\n",
    "        \"\"\"Process document with tokenization processor\"\"\"\n",
    "        return self.tokenization_processor.tokenize_document(document)\n",
    "        \n",
    "    def _process_entity_recognition(self, document: Union[Path, Dict]) -> Dict:\n",
    "        \"\"\"Process document with entity recognition processor\"\"\"\n",
    "        return self.entity_processor.process(document)\n",
    "        \n",
    "    def _save_results(self, results: Dict, output_path: Path) -> None:\n",
    "        \"\"\"Save processing results to file\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "            \n",
    "        self.logger.info(f\"Results saved to {output_path}\")\n",
    "        \n",
    "    def process_batch(self, input_dir: Union[str, Path], file_pattern: str = '*', save_output: bool = False, output_dir: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a batch of clinical oncology documents.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: Directory containing input documents\n",
    "            file_pattern: Glob pattern for selecting files\n",
    "            save_output: Whether to save processing output to files\n",
    "            output_dir: Directory to save output (if None, use input_dir)\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with processing results\n",
    "        \"\"\"\n",
    "        input_dir = Path(input_dir)\n",
    "        \n",
    "        if not input_dir.exists() or not input_dir.is_dir():\n",
    "            raise NotADirectoryError(f\"Input directory not found: {input_dir}\")\n",
    "            \n",
    "        # Get list of files matching pattern\n",
    "        files = list(input_dir.glob(file_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            self.logger.warning(f\"No files found matching pattern: {file_pattern}\")\n",
    "            return []\n",
    "            \n",
    "        self.logger.info(f\"Processing {len(files)} files\")\n",
    "        \n",
    "        # Process each file\n",
    "        results = []\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                if output_dir:\n",
    "                    output_path = Path(output_dir) / file_path.with_suffix('.json').name\n",
    "                else:\n",
    "                    output_path = file_path.with_suffix('.json')\n",
    "                    \n",
    "                result = self.process(file_path, save_output=save_output, output_path=output_path)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing {file_path}: {e}\")\n",
    "                \n",
    "        return results\n",
    "        \n",
    "    def process_text(self, text: str, document_type: str = 'unknown') -> Dict:\n",
    "        \"\"\"\n",
    "        Process raw text.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text to process\n",
    "            document_type: Type of clinical document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processing results\n",
    "        \"\"\"\n",
    "        # Create a document-like structure\n",
    "        document = {\n",
    "            'full_text': text,\n",
    "            'document_type': document_type\n",
    "        }\n",
    "        \n",
    "        # Initialize results\n",
    "        results = {\n",
    "            'document_type': document_type,\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        # Include raw text if configured\n",
    "        if self.config['output']['include_raw_text']:\n",
    "            results['text'] = text\n",
    "            \n",
    "        # Step 1: Tokenization\n",
    "        if 'tokenization' in self.config['processing_pipeline']:\n",
    "            tokenization_result = self.tokenization_processor.tokenize_document(document)\n",
    "            \n",
    "            # Include tokens if configured\n",
    "            if self.config['output']['include_tokens']:\n",
    "                results['tokenization'] = tokenization_result\n",
    "                \n",
    "        # Step 2: Entity Recognition\n",
    "        if 'entity_recognition' in self.config['processing_pipeline']:\n",
    "            entity_result = self.entity_processor.process(document)\n",
    "            \n",
    "            # Include entities if configured\n",
    "            if self.config['output']['include_entities']:\n",
    "                results['entities'] = entity_result['entities']\n",
    "                results['entity_relationships'] = entity_result['relationships']\n",
    "                \n",
    "            # Include temporal timeline if configured\n",
    "            if self.config['output']['include_temporal_timeline']:\n",
    "                results['temporal_timeline'] = entity_result['temporal_timeline']\n",
    "                \n",
    "        return results\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_config(config_path: Union[str, Path]) -> Dict:\n",
    "    \"\"\"Load configuration from JSON file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def initialize_processor(config_path: Optional[Union[str, Path]] = None) -> ClinicalOncologyProcessor:\n",
    "    \"\"\"Initialize processor with optional configuration\"\"\"\n",
    "    if config_path:\n",
    "        config = load_config(config_path)\n",
    "    else:\n",
    "        config = {}\n",
    "        \n",
    "    return ClinicalOncologyProcessor(config)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example Usage\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the Clinical Oncology Data Processing System\"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Load configuration (optional)\n",
    "    config = {\n",
    "        'document_processor': {\n",
    "            'use_ocr': True,\n",
    "            'use_ehr': True\n",
    "        },\n",
    "        'tokenization': {\n",
    "            'model': 'bioclinicalbert',\n",
    "            'max_length': 512,\n",
    "            'segment_strategy': 'sentence',\n",
    "            'long_document_strategy': 'sliding_window'\n",
    "        },\n",
    "        'entity_recognition': {\n",
    "            'use_rules': True,\n",
    "            'use_spacy': True,\n",
    "            'use_deep_learning': True,\n",
    "            'cancer_specific_extraction': True,\n",
    "            'temporal_extraction': True,\n",
    "            'ontologies': ['snomed_ct', 'rxnorm']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ClinicalOncologyProcessor(config)\n",
    "    \n",
    "    # Example 1: Process a single file\n",
    "    result = processor.process(\"/mnt/d/TCGA/raw/TCGA-ACC/raw/TCGA-OR-A5JL/Pathology Report/c9f9dc8b-68ca-4a7e-be69-4d23df5a51a1/TCGA-OR-A5JL.BD8435C3-C525-472C-8B43-EC2CAE22B785.PDF\", save_output=True)\n",
    "    \n",
    "    # Example 2: Process a batch of files\n",
    "    # results = processor.process_batch('path/to/documents', file_pattern='*.pdf', save_output=True)\n",
    "    \n",
    "    # Example 3: Process raw text\n",
    "    sample_text = \"\"\"\n",
    "    Patient is a 58-year-old female with stage IIB (T2N1M0) infiltrating ductal carcinoma of the left breast.\n",
    "    Diagnosis was made on 04/15/2023. Initial surgical management included a left mastectomy on 05/02/2023.\n",
    "    Pathology showed a 3.2 cm tumor with 2/15 positive lymph nodes. Biomarker testing showed ER positive (95%),\n",
    "    PR positive (80%), and HER2 negative. Patient started adjuvant chemotherapy with AC-T regimen on 06/01/2023.\n",
    "    Completed 4 cycles of AC followed by 12 weeks of paclitaxel 80 mg/m2. Post-treatment imaging on 09/28/2023\n",
    "    showed no evidence of disease. Patient started tamoxifen 20 mg daily on 10/10/2023 and will continue for 5 years.\n",
    "    Follow-up appointment is scheduled for 01/15/2024.\n",
    "    \"\"\"\n",
    "    \n",
    "    # result = processor.process_text(sample_text, document_type='progress_note')\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(f\"Document type: {result.get('document_type')}\")\n",
    "    print(f\"Entities found: {len(result.get('entities', []))}\")\n",
    "    \n",
    "    # Print identified entities by type\n",
    "    entity_types = {}\n",
    "    for entity in result.get('entities', []):\n",
    "        entity_type = entity.get('type', 'unknown')\n",
    "        if entity_type not in entity_types:\n",
    "            entity_types[entity_type] = []\n",
    "        entity_types[entity_type].append(entity)\n",
    "        \n",
    "    print(\"\\nEntities by type:\")\n",
    "    for entity_type, entities in entity_types.items():\n",
    "        print(f\"  {entity_type}: {len(entities)}\")\n",
    "        \n",
    "    # Print first few entities of each type\n",
    "    print(\"\\nSample entities:\")\n",
    "    for entity_type, entities in entity_types.items():\n",
    "        print(f\"\\n  {entity_type.upper()}:\")\n",
    "        for i, entity in enumerate(entities[:3]):  # Show first 3 of each type\n",
    "            print(f\"    - {entity.get('text')}\")\n",
    "            \n",
    "    # Print timeline events\n",
    "    print(\"\\nTemporal timeline:\")\n",
    "    for i, event in enumerate(result.get('temporal_timeline', [])[:5]):  # Show first 5 events\n",
    "        print(f\"  Event {i+1}: {event.get('temporal_text')}\")\n",
    "        related_entities = []\n",
    "        for idx in event.get('related_entities', []):\n",
    "            if idx < len(result.get('entities', [])):\n",
    "                related_entities.append(result['entities'][idx].get('text', ''))\n",
    "        if related_entities:\n",
    "            print(f\"    Related: {', '.join(related_entities)}\")\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HoneyBee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
