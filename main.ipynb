{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(\"./data/parquet/clinical.parquet\")\n",
    "embeddings_series = df[\"clinical_embedding\"]\n",
    "\n",
    "all_embeddings = []\n",
    "for i in range(len(embeddings_series)):\n",
    "    embeddings_objects = np.array(embeddings_series[i].tolist(), dtype=object)\n",
    "    all_embeddings.append(embeddings_objects)\n",
    "all_embeddings = np.array(all_embeddings).reshape(-1, 1024)\n",
    "print(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(\"./data/parquet/pathology_reports.parquet\")\n",
    "embeddings_series = df[\"report_embedding\"]\n",
    "\n",
    "all_embeddings = []\n",
    "for i in range(len(embeddings_series)):\n",
    "    embeddings_objects = np.array(embeddings_series[i].tolist(), dtype=object)\n",
    "    all_embeddings.append(embeddings_objects)\n",
    "    print(embeddings_objects.shape)\n",
    "# all_embeddings = np.array(all_embeddings).reshape(-1, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_parquet(\"./data/parquet/clinical.parquet\")\n",
    "embeddings_series = df[\"clinical_embedding\"]\n",
    "\n",
    "all_embeddings = []\n",
    "for i in range(len(embeddings_series)):\n",
    "    embeddings_objects = np.array(embeddings_series[i].tolist(), dtype=object)\n",
    "    all_embeddings.append(embeddings_objects)\n",
    "all_embeddings = np.array(all_embeddings).reshape(-1, 1024)\n",
    "print(all_embeddings.shape)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_obj = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "tsne_df = pd.DataFrame(\n",
    "    {\n",
    "        \"X\": tsne_obj[:, 0],\n",
    "        \"Y\": tsne_obj[:, 1],\n",
    "        \"label\": df[\"year_of_diagnosis\"],\n",
    "    }\n",
    ")\n",
    "# Filter out None values, then sort and set as categories\n",
    "valid_labels = tsne_df[\"label\"].dropna().unique()\n",
    "sorted_labels = sorted(valid_labels, key=lambda x: (str(x).isdigit(), x))\n",
    "\n",
    "# Set the 'label' column as an ordered categorical type with the sorted labels\n",
    "tsne_df[\"label\"] = pd.Categorical(\n",
    "    tsne_df[\"label\"], ordered=True, categories=sorted_labels\n",
    ")\n",
    "\n",
    "# Proceed with plotting\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.scatterplot(\n",
    "    x=\"X\",\n",
    "    y=\"Y\",\n",
    "    data=tsne_df,\n",
    "    hue=\"label\",\n",
    "    legend=\"full\",\n",
    "    palette=\"Spectral\",\n",
    "    alpha=1,\n",
    "    marker=\"o\",\n",
    "    s=150,\n",
    ")\n",
    "plt.title(\n",
    "    \"t-SNE of TCGA-LUAD Patients Clinical Embeddings\", fontsize=24\n",
    ")  # Larger title font size\n",
    "plt.xlabel(\"t-SNE Dimension 1\", fontsize=24)  # X-axis label with font size\n",
    "plt.ylabel(\"t-SNE Dimension 2\", fontsize=24)  # Y-axis label with font size\n",
    "plt.xticks(fontsize=24)  # Larger x-tick labels\n",
    "plt.yticks(fontsize=24)  # Larger y-tick labels\n",
    "plt.grid(True)  # Add gridlines\n",
    "plt.legend(\n",
    "    title=\"Year of Diagnosis\",\n",
    "    title_fontsize=\"16\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    "    fontsize=16,\n",
    ")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626def1a5124fc2b82447c56a479fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'label', 'data'],\n",
      "    num_rows: 585\n",
      "})\n",
      "{'id': 'ID_0', 'label': 'D', 'data': tensor([[[[0.8399, 0.5693, 0.3032,  ..., 0.1226, 0.7706, 0.5793],\n",
      "          [0.0378, 0.8379, 0.2756,  ..., 0.2207, 0.0626, 0.0852],\n",
      "          [0.3668, 0.1106, 0.3341,  ..., 0.1359, 0.2585, 0.7778],\n",
      "          ...,\n",
      "          [0.3451, 0.2968, 0.0931,  ..., 0.7341, 0.1598, 0.4960],\n",
      "          [0.1226, 0.0465, 0.3575,  ..., 0.9395, 0.4018, 0.9023],\n",
      "          [0.8692, 0.8688, 0.4421,  ..., 0.3952, 0.2855, 0.4411]],\n",
      "\n",
      "         [[0.4677, 0.9430, 0.1870,  ..., 0.0442, 0.4276, 0.9601],\n",
      "          [0.0126, 0.1896, 0.3887,  ..., 0.4643, 0.5717, 0.8621],\n",
      "          [0.1299, 0.0903, 0.7232,  ..., 0.1370, 0.1709, 0.2838],\n",
      "          ...,\n",
      "          [0.5206, 0.3902, 0.4402,  ..., 0.0113, 0.9322, 0.8847],\n",
      "          [0.6994, 0.8671, 0.0250,  ..., 0.7464, 0.6148, 0.8130],\n",
      "          [0.9712, 0.1913, 0.2831,  ..., 0.1075, 0.5943, 0.0333]],\n",
      "\n",
      "         [[0.3705, 0.5122, 0.6124,  ..., 0.4444, 0.8500, 0.8437],\n",
      "          [0.4241, 0.1114, 0.6810,  ..., 0.3229, 0.3705, 0.0247],\n",
      "          [0.0234, 0.1810, 0.1161,  ..., 0.5699, 0.2233, 0.0285],\n",
      "          ...,\n",
      "          [0.2379, 0.8740, 0.7705,  ..., 0.4816, 0.6522, 0.3958],\n",
      "          [0.2850, 0.4229, 0.5671,  ..., 0.2398, 0.7834, 0.4733],\n",
      "          [0.0262, 0.8070, 0.5034,  ..., 0.0418, 0.7750, 0.2850]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1602, 0.7616, 0.7895,  ..., 0.8080, 0.2952, 0.3076],\n",
      "          [0.8862, 0.9913, 0.1794,  ..., 0.3328, 0.8863, 0.0431],\n",
      "          [0.1740, 0.2765, 0.3667,  ..., 0.1865, 0.8717, 0.0704],\n",
      "          ...,\n",
      "          [0.9680, 0.0481, 0.6861,  ..., 0.2390, 0.6881, 0.3209],\n",
      "          [0.0518, 0.9655, 0.6624,  ..., 0.0699, 0.5115, 0.1570],\n",
      "          [0.8746, 0.4958, 0.6624,  ..., 0.1184, 0.6444, 0.0816]],\n",
      "\n",
      "         [[0.6712, 0.0211, 0.7116,  ..., 0.9782, 0.9128, 0.6208],\n",
      "          [0.8701, 0.7305, 0.8892,  ..., 0.0656, 0.9663, 0.7205],\n",
      "          [0.2018, 0.8578, 0.5466,  ..., 0.6121, 0.9693, 0.9477],\n",
      "          ...,\n",
      "          [0.9314, 0.0188, 0.5436,  ..., 0.7458, 0.5155, 0.6492],\n",
      "          [0.4033, 0.6543, 0.6767,  ..., 0.6589, 0.4116, 0.1071],\n",
      "          [0.2170, 0.8420, 0.0454,  ..., 0.9750, 0.6162, 0.7051]],\n",
      "\n",
      "         [[0.4121, 0.6204, 0.1961,  ..., 0.7405, 0.4914, 0.5634],\n",
      "          [0.9507, 0.9405, 0.1434,  ..., 0.0907, 0.1448, 0.4872],\n",
      "          [0.0086, 0.9208, 0.5571,  ..., 0.0828, 0.2029, 0.2192],\n",
      "          ...,\n",
      "          [0.3964, 0.6520, 0.2722,  ..., 0.3228, 0.8160, 0.9433],\n",
      "          [0.4262, 0.4753, 0.4923,  ..., 0.1172, 0.9425, 0.9305],\n",
      "          [0.3708, 0.5048, 0.1576,  ..., 0.4277, 0.1912, 0.1764]]],\n",
      "\n",
      "\n",
      "        [[[0.4223, 0.4301, 0.6650,  ..., 0.4626, 0.8255, 0.9430],\n",
      "          [0.2356, 0.8668, 0.8215,  ..., 0.4672, 0.9564, 0.9651],\n",
      "          [0.9705, 0.4610, 0.0199,  ..., 0.4400, 0.2896, 0.6881],\n",
      "          ...,\n",
      "          [0.6368, 0.1358, 0.7009,  ..., 0.5037, 0.6778, 0.5845],\n",
      "          [0.4256, 0.7338, 0.2156,  ..., 0.9284, 0.6309, 0.1782],\n",
      "          [0.0740, 0.8734, 0.8013,  ..., 0.5508, 0.0815, 0.2391]],\n",
      "\n",
      "         [[0.7566, 0.8903, 0.8904,  ..., 0.0684, 0.5067, 0.0553],\n",
      "          [0.7881, 0.6559, 0.4373,  ..., 0.9941, 0.6378, 0.1833],\n",
      "          [0.9913, 0.6512, 0.3744,  ..., 0.0946, 0.1299, 0.5784],\n",
      "          ...,\n",
      "          [0.0296, 0.0656, 0.8453,  ..., 0.9424, 0.1445, 0.4651],\n",
      "          [0.7313, 0.1749, 0.8416,  ..., 0.8885, 0.4817, 0.3294],\n",
      "          [0.5098, 0.1726, 0.9003,  ..., 0.7160, 0.8840, 0.9329]],\n",
      "\n",
      "         [[0.4894, 0.1526, 0.8561,  ..., 0.1397, 0.8026, 0.2326],\n",
      "          [0.9492, 0.6252, 0.6223,  ..., 0.8626, 0.4218, 0.7995],\n",
      "          [0.7996, 0.0262, 0.7597,  ..., 0.9349, 0.4137, 0.6061],\n",
      "          ...,\n",
      "          [0.7165, 0.9456, 0.7997,  ..., 0.4374, 0.3497, 0.7193],\n",
      "          [0.8961, 0.7254, 0.0404,  ..., 0.7031, 0.3829, 0.0161],\n",
      "          [0.6397, 0.0133, 0.4042,  ..., 0.3850, 0.8228, 0.8808]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5658, 0.0817, 0.9823,  ..., 0.3517, 0.0416, 0.1743],\n",
      "          [0.1759, 0.4478, 0.2265,  ..., 0.2342, 0.3750, 0.6350],\n",
      "          [0.7255, 0.8498, 0.6129,  ..., 0.5742, 0.9726, 0.8439],\n",
      "          ...,\n",
      "          [0.8899, 0.1980, 0.6381,  ..., 0.4047, 0.3989, 0.4702],\n",
      "          [0.5768, 0.0895, 0.9516,  ..., 0.2150, 0.6053, 0.0765],\n",
      "          [0.6447, 0.4431, 0.3866,  ..., 0.4558, 0.0029, 0.2715]],\n",
      "\n",
      "         [[0.1431, 0.1360, 0.5078,  ..., 0.0571, 0.8264, 0.3649],\n",
      "          [0.5832, 0.4940, 0.7240,  ..., 0.5358, 0.7158, 0.3544],\n",
      "          [0.5860, 0.1994, 0.3018,  ..., 0.3142, 0.8736, 0.0554],\n",
      "          ...,\n",
      "          [0.0384, 0.7776, 0.5728,  ..., 0.7037, 0.4743, 0.4336],\n",
      "          [0.9410, 0.3230, 0.2821,  ..., 0.9933, 0.5254, 0.2221],\n",
      "          [0.0265, 0.2386, 0.0373,  ..., 0.7660, 0.5337, 0.3382]],\n",
      "\n",
      "         [[0.0547, 0.0225, 0.7269,  ..., 0.2964, 0.0394, 0.8989],\n",
      "          [0.6106, 0.1849, 0.7589,  ..., 0.3763, 0.6756, 0.1235],\n",
      "          [0.9825, 0.7034, 0.3421,  ..., 0.3351, 0.1805, 0.7529],\n",
      "          ...,\n",
      "          [0.1836, 0.4614, 0.4384,  ..., 0.2035, 0.4962, 0.6200],\n",
      "          [0.2091, 0.4726, 0.6447,  ..., 0.2658, 0.7144, 0.5059],\n",
      "          [0.5571, 0.0738, 0.3594,  ..., 0.9529, 0.0741, 0.1898]]],\n",
      "\n",
      "\n",
      "        [[[0.4911, 0.1648, 0.6001,  ..., 0.3130, 0.1516, 0.3594],\n",
      "          [0.5112, 0.0042, 0.1273,  ..., 0.4531, 0.6849, 0.8754],\n",
      "          [0.9361, 0.2707, 0.6547,  ..., 0.7756, 0.3686, 0.2658],\n",
      "          ...,\n",
      "          [0.6179, 0.2021, 0.4598,  ..., 0.6075, 0.1733, 0.8105],\n",
      "          [0.8097, 0.4405, 0.0868,  ..., 0.3960, 0.0194, 0.2064],\n",
      "          [0.6342, 0.6938, 0.6087,  ..., 0.6130, 0.1546, 0.8445]],\n",
      "\n",
      "         [[0.0564, 0.5382, 0.4768,  ..., 0.5540, 0.9089, 0.1924],\n",
      "          [0.0591, 0.5899, 0.9416,  ..., 0.8248, 0.4197, 0.5746],\n",
      "          [0.9380, 0.0680, 0.2806,  ..., 0.9524, 0.4652, 0.7709],\n",
      "          ...,\n",
      "          [0.7023, 0.1786, 0.2787,  ..., 0.1077, 0.8770, 0.3811],\n",
      "          [0.1601, 0.7112, 0.1086,  ..., 0.3760, 0.6198, 0.6590],\n",
      "          [0.6552, 0.8535, 0.9068,  ..., 0.2520, 0.4019, 0.4300]],\n",
      "\n",
      "         [[0.1287, 0.1962, 0.4522,  ..., 0.5632, 0.8271, 0.1568],\n",
      "          [0.0129, 0.9727, 0.8563,  ..., 0.5734, 0.8719, 0.4684],\n",
      "          [0.1107, 0.5370, 0.5711,  ..., 0.6752, 0.5674, 0.3525],\n",
      "          ...,\n",
      "          [0.2904, 0.9808, 0.4562,  ..., 0.9319, 0.9101, 0.5851],\n",
      "          [0.9040, 0.0095, 0.4345,  ..., 0.9924, 0.9368, 0.6757],\n",
      "          [0.8804, 0.3890, 0.7098,  ..., 0.0090, 0.9209, 0.0410]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.8337, 0.8664, 0.7613,  ..., 0.3225, 0.0726, 0.8129],\n",
      "          [0.8463, 0.2671, 0.9944,  ..., 0.0900, 0.0888, 0.6684],\n",
      "          [0.2401, 0.6657, 0.7607,  ..., 0.4125, 0.5608, 0.2685],\n",
      "          ...,\n",
      "          [0.2218, 0.4942, 0.6045,  ..., 0.9495, 0.4529, 0.6143],\n",
      "          [0.7534, 0.6926, 0.0199,  ..., 0.7062, 0.1505, 0.7425],\n",
      "          [0.0687, 0.0332, 0.6819,  ..., 0.7723, 0.3031, 0.8621]],\n",
      "\n",
      "         [[0.3550, 0.8206, 0.9518,  ..., 0.0439, 0.7723, 0.2004],\n",
      "          [0.3416, 0.0126, 0.9829,  ..., 0.1240, 0.1332, 0.0036],\n",
      "          [0.4979, 0.0778, 0.8370,  ..., 0.6089, 0.8308, 0.3034],\n",
      "          ...,\n",
      "          [0.0388, 0.1186, 0.8140,  ..., 0.4832, 0.6932, 0.2921],\n",
      "          [0.5162, 0.7042, 0.7405,  ..., 0.9933, 0.1057, 0.2562],\n",
      "          [0.3433, 0.3292, 0.5492,  ..., 0.7248, 0.2982, 0.6440]],\n",
      "\n",
      "         [[0.0104, 0.5792, 0.8522,  ..., 0.4049, 0.0381, 0.8677],\n",
      "          [0.9162, 0.0649, 0.1092,  ..., 0.4289, 0.6125, 0.7311],\n",
      "          [0.1073, 0.4909, 0.8743,  ..., 0.2456, 0.8567, 0.7554],\n",
      "          ...,\n",
      "          [0.8714, 0.6574, 0.7427,  ..., 0.0043, 0.4442, 0.8530],\n",
      "          [0.6747, 0.0464, 0.9147,  ..., 0.8688, 0.5882, 0.0072],\n",
      "          [0.3966, 0.0923, 0.0441,  ..., 0.7362, 0.1799, 0.3318]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.9605, 0.6782, 0.6587,  ..., 0.1605, 0.0320, 0.5803],\n",
      "          [0.5499, 0.2429, 0.9799,  ..., 0.7845, 0.4383, 0.3812],\n",
      "          [0.6453, 0.5904, 0.1614,  ..., 0.4926, 0.0320, 0.2116],\n",
      "          ...,\n",
      "          [0.0760, 0.5234, 0.4384,  ..., 0.0910, 0.5654, 0.7794],\n",
      "          [0.0973, 0.5358, 0.8796,  ..., 0.1919, 0.5031, 0.5770],\n",
      "          [0.9805, 0.5427, 0.7387,  ..., 0.3755, 0.9653, 0.5382]],\n",
      "\n",
      "         [[0.8463, 0.5573, 0.5948,  ..., 0.8632, 0.8234, 0.7636],\n",
      "          [0.7524, 0.2605, 0.9503,  ..., 0.2838, 0.3186, 0.6601],\n",
      "          [0.6689, 0.0967, 0.3759,  ..., 0.8632, 0.1417, 0.5503],\n",
      "          ...,\n",
      "          [0.3012, 0.2790, 0.9815,  ..., 0.5755, 0.7219, 0.6795],\n",
      "          [0.4726, 0.1428, 0.9572,  ..., 0.4910, 0.4752, 0.7689],\n",
      "          [0.2216, 0.8639, 0.1003,  ..., 0.9390, 0.4465, 0.7126]],\n",
      "\n",
      "         [[0.9061, 0.1965, 0.5603,  ..., 0.9663, 0.0527, 0.0925],\n",
      "          [0.8309, 0.3640, 0.6322,  ..., 0.7077, 0.2880, 0.5561],\n",
      "          [0.4023, 0.2897, 0.5617,  ..., 0.7103, 0.5043, 0.3532],\n",
      "          ...,\n",
      "          [0.1824, 0.6349, 0.8597,  ..., 0.9628, 0.2030, 0.4776],\n",
      "          [0.1946, 0.8328, 0.7656,  ..., 0.0897, 0.4305, 0.5321],\n",
      "          [0.4300, 0.6420, 0.2303,  ..., 0.9439, 0.0492, 0.7811]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.8506, 0.4391, 0.0671,  ..., 0.0390, 0.8458, 0.2615],\n",
      "          [0.0270, 0.1599, 0.6417,  ..., 0.1922, 0.4438, 0.7030],\n",
      "          [0.1209, 0.7324, 0.0662,  ..., 0.1209, 0.4274, 0.6253],\n",
      "          ...,\n",
      "          [0.5268, 0.3087, 0.5755,  ..., 0.7363, 0.5947, 0.2873],\n",
      "          [0.1648, 0.0887, 0.9409,  ..., 0.8106, 0.0365, 0.6714],\n",
      "          [0.3572, 0.8846, 0.6847,  ..., 0.5742, 0.4855, 0.9017]],\n",
      "\n",
      "         [[0.3110, 0.3673, 0.2758,  ..., 0.5413, 0.9067, 0.7256],\n",
      "          [0.6707, 0.8051, 0.5616,  ..., 0.7594, 0.2066, 0.1167],\n",
      "          [0.1913, 0.5014, 0.5974,  ..., 0.0173, 0.1909, 0.5439],\n",
      "          ...,\n",
      "          [0.0282, 0.2514, 0.7373,  ..., 0.2843, 0.1995, 0.7259],\n",
      "          [0.5514, 0.0550, 0.6993,  ..., 0.2783, 0.7209, 0.6173],\n",
      "          [0.6799, 0.0784, 0.0618,  ..., 0.8026, 0.2586, 0.7720]],\n",
      "\n",
      "         [[0.0231, 0.6626, 0.8444,  ..., 0.6716, 0.8858, 0.2924],\n",
      "          [0.8975, 0.2028, 0.4603,  ..., 0.7480, 0.1260, 0.1997],\n",
      "          [0.5944, 0.0330, 0.4815,  ..., 0.8696, 0.2098, 0.6135],\n",
      "          ...,\n",
      "          [0.4196, 0.5024, 0.8428,  ..., 0.5101, 0.8892, 0.5398],\n",
      "          [0.9605, 0.8212, 0.5948,  ..., 0.4680, 0.9454, 0.1478],\n",
      "          [0.3163, 0.1687, 0.3662,  ..., 0.2859, 0.1142, 0.0865]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "max_patch_size = 10\n",
    "embeddings = []\n",
    "ids = []\n",
    "labels = []\n",
    "\n",
    "# Define some fake categories for labels\n",
    "fake_categories = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "for i in range(585):\n",
    "    random_patch_size = np.random.randint(1, max_patch_size + 1)\n",
    "    data = np.random.rand(random_patch_size, 7, 7, 2048).astype(np.float32)\n",
    "    if random_patch_size < max_patch_size:\n",
    "        padding = np.zeros((max_patch_size - random_patch_size, 7, 7, 2048), dtype=np.float32)\n",
    "        data = np.concatenate([data, padding], axis=0)\n",
    "    embeddings.append(data)\n",
    "    ids.append(f\"ID_{i}\")\n",
    "    labels.append(random.choice(fake_categories))\n",
    "\n",
    "embeddings = np.array(embeddings, dtype=object)\n",
    "\n",
    "# Generate a DataFrame with fake data\n",
    "df = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Now, combine this information with the embeddings to create the dataset\n",
    "ds = Dataset.from_dict({\n",
    "    \"id\": df[\"id\"].tolist(),\n",
    "    \"label\": df[\"label\"].tolist(),\n",
    "    \"data\": embeddings\n",
    "})\n",
    "ds = ds.with_format(\"torch\")\n",
    "\n",
    "# Save the dataset to disk\n",
    "ds.save_to_disk(\"./testdata\")\n",
    "\n",
    "# Load the dataset from disk and verify its structure\n",
    "dataset = Dataset.load_from_disk(\"./testdata\")\n",
    "print(dataset)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"testdata.json\")\n",
    "data0 = dataset[\"train\"][\"data\"][0]\n",
    "data0 = np.array(data0)\n",
    "print(data0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0686872622bc437c9365f4bf50b11a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"arrow\", data_files=\"testdata/data-00000-of-00001.arrow\")\n",
    "data0 = dataset[\"train\"][\"data\"][0]\n",
    "data0 = np.array(data0)\n",
    "print(data0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"Aakash-Tripathi/TCGA-LUAD-minds\", \"clinical\")\n",
    "print(np.array(dataset[\"train\"]['clinical_embedding'][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 7, 7, 2048])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"Aakash-Tripathi/TCGA-LUAD-minds\", \"test\")\n",
    "data0 = dataset[\"train\"][\"data\"][0]\n",
    "# data0 = np.array(data0)\n",
    "# print(data0.shape)\n",
    "\n",
    "# convert to torch tensor\n",
    "data0 = torch.tensor(data0)\n",
    "print(data0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "def download_wikipedia_data(\n",
    "    data_path: str = \"../../data/\",\n",
    "    download_path: str = \"./\",\n",
    "    file_name: str = \"vector_database_wikipedia_articles_embedded\",\n",
    ") -> pd.DataFrame:\n",
    "    data_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n",
    "\n",
    "    csv_file_path = os.path.join(data_path, file_name + \".csv\")\n",
    "    zip_file_path = os.path.join(download_path, file_name + \".zip\")\n",
    "    if os.path.isfile(csv_file_path):\n",
    "        print(\"File Downloaded\")\n",
    "    else:\n",
    "        if os.path.isfile(zip_file_path):\n",
    "            print(\"Zip downloaded but not unzipped, unzipping now...\")\n",
    "        else:\n",
    "            print(\"File not found, downloading now...\")\n",
    "            # Download the data\n",
    "            wget.download(data_url, out=download_path)\n",
    "\n",
    "        # Unzip the data\n",
    "        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_path)\n",
    "\n",
    "        # Remove the zip file\n",
    "        os.remove(\"vector_database_wikipedia_articles_embedded.zip\")\n",
    "        print(f\"File downloaded to {data_path}\")\n",
    "\n",
    "\n",
    "def read_wikipedia_data(\n",
    "    data_path: str = \"../../data/\",\n",
    "    file_name: str = \"vector_database_wikipedia_articles_embedded\",\n",
    ") -> pd.DataFrame:\n",
    "    csv_file_path = os.path.join(data_path, file_name + \".csv\")\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    if \"title_vector\" in data.columns:\n",
    "        data = data.drop(columns=[\"title_vector\"])\n",
    "    if \"content_vector\" in data.columns:\n",
    "        data = data.drop(columns=[\"content_vector\"])\n",
    "    if \"vector_id\" in data.columns:\n",
    "        data = data.drop(columns=[\"vector_id\"])\n",
    "\n",
    "    # save only the first 500 rows\n",
    "    data = data.head(500)\n",
    "\n",
    "    client = Client(host=\"http://localhost:11434\")\n",
    "    client.pull(\"all-minilm\")\n",
    "\n",
    "    data[\"title_vector\"] = \"\"\n",
    "    data[\"content_vector\"] = \"\"\n",
    "    for i, row in data.iterrows():\n",
    "        title_embedding = client.embeddings(model=\"all-minilm\", prompt=row[\"title\"])[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "        content_embedding = client.embeddings(model=\"all-minilm\", prompt=row[\"text\"])[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "        # add the embeddings to the dataframe\n",
    "        data.at[i, \"title_vector\"] = title_embedding\n",
    "        data.at[i, \"content_vector\"] = content_embedding\n",
    "\n",
    "    # save the data to a csv file\n",
    "    data.to_csv(data_path + file_name + \".csv\", index=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "download_wikipedia_data()\n",
    "data = read_wikipedia_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"F:\\Projects\\HoneyBee\\data\\dataframe\\Pathology Report.csv\")\n",
    "\n",
    "# add to the data dataframe the column \"extracted_text\" and \"extracted_text_embedding\"\n",
    "data[\"extracted_text\"] = \"\"\n",
    "data[\"extracted_text_embedding\"] = \"\"\n",
    "\n",
    "\n",
    "REDIS_HOST = \"10.0.1.16\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"\"  # default for passwordless Redis\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)\n",
    "redis_client.ping()\n",
    "\n",
    "# Constants\n",
    "VECTOR_DIM = 1024  # length of the vectors\n",
    "VECTOR_NUMBER = len(data)  # initial number of vectors\n",
    "INDEX_NAME = \"embeddings-index\"  # name of the search index\n",
    "PREFIX = \"pathology_report\"  # prefix for the document keys\n",
    "DISTANCE_METRIC = \"COSINE\"  # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# # Define RediSearch fields for each of the columns in the dataset\n",
    "# title = TextField(name=\"title\")\n",
    "# url = TextField(name=\"url\")\n",
    "# text = TextField(name=\"extracted_text\")\n",
    "# extracted_text_embedding = VectorField(\n",
    "#     \"extracted_text_embedding\",\n",
    "#     \"FLAT\",\n",
    "#     {\n",
    "#         \"TYPE\": \"FLOAT32\",\n",
    "#         \"DIM\": VECTOR_DIM,\n",
    "#         \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "#         \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "#     },\n",
    "# )\n",
    "# fields = [title, url, text, extracted_text_embedding]\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "fields = []\n",
    "# Dynamically add TextField for each column, except for special handling columns\n",
    "for column in data.columns:\n",
    "    if column not in [\"extracted_text\", \"extracted_text_embedding\"]:\n",
    "        fields.append(TextField(name=column))\n",
    "# Add special fields for 'extracted_text' and 'extracted_text_embedding'\n",
    "fields.append(TextField(name=\"extracted_text\"))\n",
    "fields.append(\n",
    "    VectorField(\n",
    "        \"extracted_text_embedding\",\n",
    "        \"FLAT\",\n",
    "        {\n",
    "            \"TYPE\": \"FLOAT32\",\n",
    "            \"DIM\": VECTOR_DIM,\n",
    "            \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "            \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check if index exists\n",
    "try:\n",
    "    redis_client.ft(INDEX_NAME).info()\n",
    "    print(\"Index already exists\")\n",
    "    # drop the index\n",
    "    redis_client.flushall()\n",
    "    print(\"Index dropped\")\n",
    "    # Create RediSearch Index\n",
    "    redis_client.ft(INDEX_NAME).create_index(\n",
    "        fields=fields,\n",
    "        definition=IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH),\n",
    "    )\n",
    "    print(\"Index created\")\n",
    "except:\n",
    "    # Create RediSearch Index\n",
    "    redis_client.ft(INDEX_NAME).create_index(\n",
    "        fields=fields,\n",
    "        definition=IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH),\n",
    "    )\n",
    "    print(\"Index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n",
    "    records = documents.to_dict(\"records\")\n",
    "    for doc in records:\n",
    "        key = f\"{prefix}:{str(doc['id'])}\"\n",
    "\n",
    "        # create byte vectors for title and content\n",
    "        # extracted_text_embedding = np.array(\n",
    "        #     doc[\"extracted_text_embedding\"], dtype=np.float32\n",
    "        # ).tobytes()\n",
    "\n",
    "        # replace list of floats with byte vectors\n",
    "        # doc[\"extracted_text_embedding\"] = extracted_text_embedding\n",
    "        client.hset(key, mapping=doc)\n",
    "\n",
    "\n",
    "index_documents(redis_client, PREFIX, data)\n",
    "print(\n",
    "    f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Search Queries with Ollama Query Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "client.pull(\"all-minilm\")\n",
    "\n",
    "\n",
    "def search_redis(\n",
    "    redis_client: redis.Redis,\n",
    "    user_query: str,\n",
    "    index_name: str = \"embeddings-index\",\n",
    "    vector_field: str = \"title_vector\",\n",
    "    return_fields: list = [\"title\", \"url\", \"text\", \"vector_score\"],\n",
    "    hybrid_fields=\"*\",\n",
    "    k: int = 20,\n",
    "    print_results: bool = True,\n",
    ") -> List[dict]:\n",
    "    # Creates embedding vector from user query\n",
    "    embedded_query = client.embeddings(\n",
    "        prompt=user_query,\n",
    "        model=\"all-minilm\",\n",
    "    )[\"embedding\"]\n",
    "\n",
    "    # Prepare the Query\n",
    "    base_query = f\"{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]\"\n",
    "    query = (\n",
    "        Query(base_query)\n",
    "        .return_fields(*return_fields)\n",
    "        .sort_by(\"vector_score\")\n",
    "        .paging(0, k)\n",
    "        .dialect(2)\n",
    "    )\n",
    "    params_dict = {\n",
    "        \"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()\n",
    "    }\n",
    "\n",
    "    # perform vector search\n",
    "    results = redis_client.ft(index_name).search(query, params_dict)\n",
    "    if print_results:\n",
    "        for i, article in enumerate(results.docs):\n",
    "            score = 1 - float(article.vector_score)\n",
    "            print(f\"{i}. {article.title} (Score: {round(score ,3) })\")\n",
    "    return results.docs\n",
    "\n",
    "\n",
    "results = search_redis(redis_client, \"Art\", k=10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# load F:\\Projects\\HoneyBee\\pathology_report.csv\n",
    "df = pd.read_csv(r\"F:\\Projects\\HoneyBee\\pathology_report.csv\")\n",
    "\n",
    "\n",
    "# Assuming `df` is your loaded DataFrame\n",
    "df[\"report_embedding\"] = df[\"report_embedding\"].apply(\n",
    "    lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df[\"report_embedding\"][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"\"\"EXAMINATION: CT TAP  w/Cont (Thorax, Abdomen,Pelvis)\n",
    "                CLINICAL HISTORY:   with provided history of \"Ovarian cancer, monitor\"\n",
    "                PRIOR EXAM(s): Comparison with CT 2/28/2019 and abdomen MRI 10/21/2021.\n",
    "                TECHNIQUE: CT TAP  w/Cont (Thorax, Abdomen,Pelvis). CT imaging of the chest, abdomen and pelvis was performed after intravenous administration of contrast.\n",
    "                Note: This report may employ disease-specific or contextual approach to highlight not just descriptive details, but relevant observations of the patient's oncologic imaging status for informed decision-making. Additional findings not pertinent to oncologic assessment may have been previously described and are discussed when clinically relevant.\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "model = ChatOllama(model=\"llama2\")\n",
    "\n",
    "\n",
    "class Patient(BaseModel):\n",
    "    summary: str = Field(description=\"summary:\")\n",
    "\n",
    "\n",
    "patient_query = \"\"\"EXAMINATION: CT TAP  w/Cont (Thorax, Abdomen,Pelvis)\n",
    "                CLINICAL HISTORY:   with provided history of \"Ovarian cancer, monitor\"\n",
    "                PRIOR EXAM(s): Comparison with CT 2/28/2019 and abdomen MRI 10/21/2021.\n",
    "                TECHNIQUE: CT TAP  w/Cont (Thorax, Abdomen,Pelvis). CT imaging of the chest, abdomen and pelvis was performed after intravenous administration of contrast.\n",
    "                Note: This report may employ disease-specific or contextual approach to highlight not just descriptive details, but relevant observations of the patient's oncologic imaging status for informed decision-making. Additional findings not pertinent to oncologic assessment may have been previously described and are discussed when clinically relevant.\n",
    "                \"\"\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Patient)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "print(chain)\n",
    "\n",
    "try:\n",
    "    formatted_output = chain.invoke({\"query\": patient_query})\n",
    "    print(formatted_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOllama())\n",
    "    chain = prompt | model | new_parser\n",
    "    misformatted = chain.invoke({\"query\": patient_query})\n",
    "    print(misformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honeybee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
