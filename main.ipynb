{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoneyBee Test File\n",
    "\n",
    "This file is here just to test the HoneyBee functions as I am writing them. This will be deleted once the functions are working properly and a more formal test file will be created in the CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Data Loaders\n",
    "\n",
    "1. Loads the data based on the data type\n",
    "1. Then after validating the data, returns an array of embeddings using the model provided\n",
    "1. The embeddings can then be\n",
    "     - stored locally\n",
    "     - stored in a Vector Database\n",
    "     - used for further processing, i.e. making a dataframe with more columns for other metadata or data \n",
    "     - be uploaded to huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path/to/svs_file.svs\n",
      "path/to/dicom_file.dcm\n",
      "path/to/pdf_file.pdf\n"
     ]
    }
   ],
   "source": [
    "from honeybee.loaders import SVSLoader, DICOMLoader, PDFLoader, MINDSLoader\n",
    "\n",
    "# Load SVS\n",
    "svs_loader = SVSLoader()\n",
    "svs_data = svs_loader.load(\"path/to/svs_file.svs\")\n",
    "\n",
    "# Load DICOM\n",
    "dicom_loader = DICOMLoader(embedding_model_path=\"path/to/embedding_model\")\n",
    "dicom_data = dicom_loader.load(\"path/to/dicom_file.dcm\")\n",
    "\n",
    "# Load PDF\n",
    "pdf_loader = PDFLoader()\n",
    "pdf_data = pdf_loader.load(\"path/to/pdf_file.pdf\")\n",
    "\n",
    "# Load data from MINDS\n",
    "minds_loader = MINDSLoader(data_dir=\"path/to/minds_data\")\n",
    "\n",
    "print(svs_data)\n",
    "print(dicom_data)\n",
    "print(pdf_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution (VectorDB, Local, Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Applications (RAG, Instruction Tuning, PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Applications (Advanced RAG, Federated Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "\n",
    "\n",
    "def web_loader_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(web_paths=(url,), bs_kwargs=dict())\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def pdf_loader_and_retrieve_docs(path):\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def rag_chain(url, question):\n",
    "    if url.endswith(\".pdf\"):\n",
    "        retriever = pdf_loader_and_retrieve_docs(url)\n",
    "    else:\n",
    "        retriever = web_loader_and_retrieve_docs(url)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Use the RAG chain\n",
    "result = rag_chain(\"http://www.espn.com\", \"what is the top news on espn?\")\n",
    "# result = rag_chain(\"/mnt/f/Projects/HoneyBee/data/llama2.pdf\", \"What is llama 2?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import pymongo\n",
    "\n",
    "\n",
    "def get_mongo_client(mongo_uri):\n",
    "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        print(\"Connection to MongoDB successful\")\n",
    "        return client\n",
    "    except pymongo.errors.ConnectionFailure as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    try:\n",
    "        embedding = ollama.embeddings(model=\"mistral\", prompt=text)\n",
    "        embedding = embedding[\"embedding\"]\n",
    "\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"plot_embedding_optimised\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 5,  # Return top 5 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"plot\": 1,  # Include the plot field\n",
    "                \"title\": 1,  # Include the title field\n",
    "                \"genres\": 1,  # Include the genres field\n",
    "                \"score\": {\n",
    "                    \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
    "                },\n",
    "                \"plot_embedding_optimised\": 1,\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "def handle_user_query(query, collection):\n",
    "    get_knowledge = vector_search(query, collection)\n",
    "\n",
    "    search_result = \"\"\n",
    "    for result in get_knowledge:\n",
    "        search_result += (\n",
    "            f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('plot', 'N/A')}\\n\"\n",
    "        )\n",
    "\n",
    "    completion = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a movie recommendation system.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Answer this user query: \"\n",
    "                + query\n",
    "                + \" with the following context: \"\n",
    "                + search_result,\n",
    "            },\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    return completion, search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"AIatMongoDB/embedded_movies\")\n",
    "dataset_df = pd.DataFrame(dataset[\"train\"])\n",
    "dataset_df = dataset_df.dropna(subset=[\"plot\"])\n",
    "dataset_df = dataset_df.drop(columns=[\"plot_embedding\"])\n",
    "dataset_df[\"plot_embedding_optimised\"] = dataset_df[\"plot\"].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = get_mongo_client(\n",
    "    \"mongodb://root:root@localhost:27778/?directConnection=true\"\n",
    ")\n",
    "db = mongo_client[\"movies\"]\n",
    "collection = db[\"movie_collection\"]\n",
    "collection.delete_many({})\n",
    "documents = dataset_df.to_dict(\"records\")\n",
    "collection.insert_many(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Conduct query with retrival of sources\n",
    "query = \"What is the best romantic movie to watch?\"\n",
    "response, source_information = handle_user_query(query, collection)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "\n",
    "print(f\"Source Information: \\n{source_information}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honeybee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
