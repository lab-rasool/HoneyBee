{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def run_experiments(n_runs, X, y, study):\n",
    "    accuracies = []\n",
    "    random_seeds = np.random.randint(0, 10000, size=n_runs)\n",
    "    for seed in tqdm(random_seeds, desc=f\"Running {study} Experiments\"):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=seed\n",
    "        )\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    print(f\"{study} Mean Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def generate_finetuned_embeddings(gatortron_pretrained_data):\n",
    "    if not os.path.exists(FINE_TUNED_MODEL):\n",
    "        dataset = gatortron_pretrained_data\n",
    "        model = AutoModel.from_pretrained(FINE_TUNED_MODEL)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "            )\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "        dataloader = DataLoader(tokenized_dataset, batch_size=16)\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating Embeddings\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        dataset = dataset.to_pandas()\n",
    "        dataset[\"embeddings\"] = list(map(list, embeddings))\n",
    "        dataset.to_parquet(EMBEDDINGS_SAVE_PATH)\n",
    "    gatortron_finetuned_data = pd.read_parquet(EMBEDDINGS_SAVE_PATH)\n",
    "    return gatortron_finetuned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GatorTron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_SAVE_PATH = \"ft_all_data_with_embeddings.parquet\"\n",
    "PRE_TRAINED_MODEL = \"UFNLP/gatortron-medium\"\n",
    "FINE_TUNED_MODEL = \"./gatortron-base-tcga\"\n",
    "\n",
    "gatortron_pretrained_data = load_dataset(\"Lab-Rasool/TCGA\", \"clinical\", split=\"train\")\n",
    "gatortron_finetuned_data = generate_finetuned_embeddings(gatortron_pretrained_data)\n",
    "gatortron_pretrained_data = gatortron_pretrained_data.to_pandas()\n",
    "\n",
    "X_gatortron_pretrained = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in gatortron_pretrained_data[\"embedding\"]]\n",
    ")\n",
    "y_gatortron_pretrained = gatortron_pretrained_data[\"project_id\"].values\n",
    "X_gatortron_finetuned = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in gatortron_finetuned_data[\"embeddings\"]]\n",
    ")\n",
    "y_gatortron_finetuned = gatortron_finetuned_data[\"project_id\"].values\n",
    "\n",
    "# ------------------- Train a classifier on the embeddings -------------------\n",
    "# Run the experiment multiple times with different random seeds\n",
    "run_experiments(\n",
    "    n_runs=10,\n",
    "    study=\"[UFNLP/gatortron-medium] Pre-trained\",\n",
    "    X=X_gatortron_pretrained,\n",
    "    y=y_gatortron_pretrained,\n",
    ")\n",
    "run_experiments(\n",
    "    n_runs=10,\n",
    "    study=\"[UFNLP/gatortron-medium] Fine-tuned\",\n",
    "    X=X_gatortron_finetuned,\n",
    "    y=y_gatortron_finetuned,\n",
    ")\n",
    "\n",
    "# ------------------- Visualize the embeddings -------------------\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X_gatortron_pretrained)\n",
    "unique_projects = np.unique(y_gatortron_pretrained)\n",
    "palette = sns.color_palette(\"hsv\", len(unique_projects))\n",
    "project_to_color = {project: palette[i] for i, project in enumerate(unique_projects)}\n",
    "colors = [project_to_color[project] for project in y_gatortron_pretrained]\n",
    "plt.figure(figsize=(18, 8), dpi=600)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(\n",
    "    x=X_reduced[:, 0],\n",
    "    y=X_reduced[:, 1],\n",
    "    hue=y_gatortron_pretrained,\n",
    "    legend=None,\n",
    "    palette=palette,\n",
    ")\n",
    "plt.title(\"t-SNE of Pre-trained Embeddings\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=18)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_ft = tsne.fit_transform(X_gatortron_finetuned)\n",
    "colors_ft = [project_to_color[project] for project in y_gatortron_finetuned]\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(\n",
    "    x=X_reduced_ft[:, 0],\n",
    "    y=X_reduced_ft[:, 1],\n",
    "    hue=y_gatortron_finetuned,\n",
    "    legend=\"full\",\n",
    "    palette=palette,\n",
    ")\n",
    "plt.title(\"t-SNE of Fine-tuned Embeddings\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=18)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.0), title=\"Project ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # ------------------- Generate or Load embeddings -------------------\n",
    "# if not os.path.exists(EMBEDDINGS_SAVE_PATH):\n",
    "#     dataset = Dataset.from_pandas(all_data)\n",
    "#     model = AutoModel.from_pretrained(FINE_TUNED_MODEL)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     def tokenize_function(examples):\n",
    "#         return tokenizer(\n",
    "#             examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "#         )\n",
    "#     tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "#     tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "#     dataloader = DataLoader(tokenized_dataset, batch_size=16)\n",
    "#     embeddings = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Generating Embeddings\"):\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#             batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#             embeddings.extend(batch_embeddings)\n",
    "#     all_data[\"embeddings\"] = list(map(list, embeddings))\n",
    "#     all_data.to_parquet(EMBEDDINGS_SAVE_PATH)\n",
    "# else:\n",
    "#     all_data = pd.read_parquet(EMBEDDINGS_SAVE_PATH)\n",
    "#     X = np.array([np.frombuffer(e, dtype=np.float32) for e in all_data[\"embeddings\"]])\n",
    "#     y = all_data[\"project_id\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_PATH = \"bert_all_data_with_embeddings.parquet\"\n",
    "EMBEDDINGS_SAVE_PATH = \"ft_bert_all_data_with_embeddings.parquet\"\n",
    "PRE_TRAINED_MODEL = \"bert-base-uncased\"\n",
    "FINE_TUNED_MODEL = \"./bert-base-uncased-tcga\"\n",
    "\n",
    "bert_pretrained_data = pd.read_parquet(ALL_DATA_PATH)\n",
    "bert_finetuned_data = generate_finetuned_embeddings(bert_pretrained_data)\n",
    "\n",
    "X_bert_pretrained = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in bert_pretrained_data[\"embedding\"]]\n",
    ")\n",
    "y_bert_pretrained = bert_pretrained_data[\"project_id\"].values\n",
    "X_bert_finetuned = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in bert_finetuned_data[\"embeddings\"]]\n",
    ")\n",
    "y_bert_finetuned = bert_finetuned_data[\"project_id\"].values\n",
    "\n",
    "# ------------------- Train a classifier on the embeddings -------------------\n",
    "# Run the experiment multiple times with different random seeds\n",
    "run_experiments(\n",
    "    n_runs=10,\n",
    "    study=\"[bert-base-uncased] Pre-trained\",\n",
    "    X=X_bert_pretrained,\n",
    "    y=y_bert_pretrained,\n",
    ")\n",
    "run_experiments(\n",
    "    n_runs=10,\n",
    "    study=\"[bert-base-uncased] Fine-tuned\",\n",
    "    X=X_bert_finetuned,\n",
    "    y=y_bert_finetuned,\n",
    ")\n",
    "\n",
    "# ------------------- Visualize the embeddings -------------------\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X_bert_pretrained)\n",
    "unique_projects = np.unique(y_bert_pretrained)\n",
    "palette = sns.color_palette(\"hsv\", len(unique_projects))\n",
    "project_to_color = {project: palette[i] for i, project in enumerate(unique_projects)}\n",
    "colors = [project_to_color[project] for project in y_bert_pretrained]\n",
    "plt.figure(figsize=(18, 8), dpi=600)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(\n",
    "    x=X_reduced[:, 0],\n",
    "    y=X_reduced[:, 1],\n",
    "    hue=y_bert_pretrained,\n",
    "    legend=None,\n",
    "    palette=palette,\n",
    ")\n",
    "plt.title(\"t-SNE of Pre-trained Embeddings\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=18)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_ft = tsne.fit_transform(X_bert_finetuned)\n",
    "colors_ft = [project_to_color[project] for project in y_gatortron_finetuned]\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(\n",
    "    x=X_reduced_ft[:, 0],\n",
    "    y=X_reduced_ft[:, 1],\n",
    "    hue=y_bert_finetuned,\n",
    "    legend=\"full\",\n",
    "    palette=palette,\n",
    ")\n",
    "plt.title(\"t-SNE of Fine-tuned Embeddings\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=18)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.0), title=\"Project ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreival Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49031596a1064cd5857470c98b7cbdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gatortron pretrained embeddings: 0.6937 Â± 0.0336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff401a7787d4cfb94126fcf42703468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gatortron fine-tuned embeddings: 0.9433 Â± 0.0157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be713f65af443a29784f422c14df112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT pretrained embeddings: 0.6890 Â± 0.0333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b70798164844126b316749a43bd7b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT fine-tuned embeddings: 0.8596 Â± 0.0286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Constants for Gatortron\n",
    "GATOR_EMBEDDINGS_SAVE_PATH = \"ft_all_data_with_embeddings.parquet\"\n",
    "GATOR_PRE_TRAINED_MODEL = \"UFNLP/gatortron-medium\"\n",
    "GATOR_FINE_TUNED_MODEL = \"./gatortron-base-tcga\"\n",
    "\n",
    "# Constants for BERT\n",
    "ALL_DATA_PATH = \"bert_all_data_with_embeddings.parquet\"\n",
    "BERT_EMBEDDINGS_SAVE_PATH = \"ft_bert_all_data_with_embeddings.parquet\"\n",
    "BERT_PRE_TRAINED_MODEL = \"bert-base-uncased\"\n",
    "BERT_FINE_TUNED_MODEL = \"./bert-base-uncased-tcga\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Function to generate fine-tuned embeddings\n",
    "def generate_finetuned_embeddings(\n",
    "    pretrained_data, pretrained_model, fine_tuned_model, embeddings_save_path\n",
    "):\n",
    "    if not os.path.exists(embeddings_save_path):\n",
    "        dataset = Dataset.from_pandas(pretrained_data)\n",
    "        model = AutoModel.from_pretrained(fine_tuned_model)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "            )\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "        dataloader = DataLoader(tokenized_dataset, batch_size=16)\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating Embeddings\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        dataset = dataset.to_pandas()\n",
    "        dataset[\"embeddings\"] = list(map(list, embeddings))\n",
    "        dataset.to_parquet(embeddings_save_path)\n",
    "    finetuned_data = pd.read_parquet(embeddings_save_path)\n",
    "    return finetuned_data\n",
    "\n",
    "\n",
    "# Load the Gatortron dataset\n",
    "gatortron_pretrained_data = load_dataset(\"Lab-Rasool/TCGA\", \"clinical\", split=\"train\")\n",
    "gatortron_finetuned_data = generate_finetuned_embeddings(\n",
    "    gatortron_pretrained_data.to_pandas(),\n",
    "    GATOR_PRE_TRAINED_MODEL,\n",
    "    GATOR_FINE_TUNED_MODEL,\n",
    "    GATOR_EMBEDDINGS_SAVE_PATH,\n",
    ")\n",
    "gatortron_pretrained_data = gatortron_pretrained_data.to_pandas()\n",
    "\n",
    "# Convert embeddings from the Gatortron dataset\n",
    "X_gatortron_pretrained = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in gatortron_pretrained_data[\"embedding\"]]\n",
    ")\n",
    "y_gatortron_pretrained = gatortron_pretrained_data[\"project_id\"].values\n",
    "X_gatortron_finetuned = np.array(\n",
    "    [np.array(e, dtype=np.float32) for e in gatortron_finetuned_data[\"embeddings\"]]\n",
    ")\n",
    "y_gatortron_finetuned = gatortron_finetuned_data[\"project_id\"].values\n",
    "\n",
    "# Load the BERT dataset\n",
    "bert_pretrained_data = pd.read_parquet(ALL_DATA_PATH)\n",
    "bert_finetuned_data = generate_finetuned_embeddings(\n",
    "    bert_pretrained_data,\n",
    "    BERT_PRE_TRAINED_MODEL,\n",
    "    BERT_FINE_TUNED_MODEL,\n",
    "    BERT_EMBEDDINGS_SAVE_PATH,\n",
    ")\n",
    "\n",
    "# Convert embeddings from the BERT dataset\n",
    "X_bert_pretrained = np.array(\n",
    "    [np.frombuffer(e, dtype=np.float32) for e in bert_pretrained_data[\"embedding\"]]\n",
    ")\n",
    "y_bert_pretrained = bert_pretrained_data[\"project_id\"].values\n",
    "X_bert_finetuned = np.array(\n",
    "    [np.array(e, dtype=np.float32) for e in bert_finetuned_data[\"embeddings\"]]\n",
    ")\n",
    "y_bert_finetuned = bert_finetuned_data[\"project_id\"].values\n",
    "\n",
    "\n",
    "# Function to perform similarity search and evaluate matching project_ids\n",
    "def run_benchmark(X_embeddings, y_labels, num_trials=10):\n",
    "    # Build the FAISS index\n",
    "    dimension = X_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance\n",
    "    index.add(X_embeddings)\n",
    "\n",
    "    total_matching_count = 0\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        # Pick a random patient embedding for evaluation\n",
    "        random_index = random.randint(0, len(X_embeddings) - 1)\n",
    "        query_embedding = X_embeddings[random_index]\n",
    "        query_project_id = y_labels[random_index]\n",
    "\n",
    "        # Retrieve the next 10 closest patients\n",
    "        distances, indices = index.search(\n",
    "            query_embedding.reshape(1, -1), k=11\n",
    "        )  # k=11 to include the query patient itself\n",
    "\n",
    "        # Remove the first index (the query patient itself)\n",
    "        indices = indices[0][1:]\n",
    "        distances = distances[0][1:]\n",
    "\n",
    "        # Check for matching \"project_id\"\n",
    "        matching_count = 0\n",
    "        for idx in indices:\n",
    "            if y_labels[int(idx)] == query_project_id:  # Convert idx to int\n",
    "                matching_count += 1\n",
    "\n",
    "        total_matching_count += matching_count\n",
    "\n",
    "    average_matching_count = total_matching_count / num_trials\n",
    "    average_matching_count = (\n",
    "        average_matching_count / 10\n",
    "    )  # Divide by 10 to get the percentage\n",
    "    return average_matching_count\n",
    "\n",
    "\n",
    "def multi_run_benchmark(X_embeddings, y_labels, num_trials=10, num_runs=10):\n",
    "    results = []\n",
    "    for _ in tqdm(range(num_runs), leave=False):\n",
    "        results.append(run_benchmark(X_embeddings, y_labels, num_trials))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run benchmarks\n",
    "num_trials = 100\n",
    "num_runs = 100\n",
    "results_gatortron_pretrained = multi_run_benchmark(\n",
    "    X_gatortron_pretrained,\n",
    "    y_gatortron_pretrained,\n",
    "    num_trials=num_trials,\n",
    "    num_runs=num_runs,\n",
    ")\n",
    "print(\n",
    "    f\"Gatortron pretrained embeddings: {np.mean(results_gatortron_pretrained):.4f} Â± {np.std(results_gatortron_pretrained):.4f}\"\n",
    ")\n",
    "results_gatortron_finetuned = multi_run_benchmark(\n",
    "    X_gatortron_finetuned,\n",
    "    y_gatortron_finetuned,\n",
    "    num_trials=num_trials,\n",
    "    num_runs=num_runs,\n",
    ")\n",
    "print(\n",
    "    f\"Gatortron fine-tuned embeddings: {np.mean(results_gatortron_finetuned):.4f} Â± {np.std(results_gatortron_finetuned):.4f}\"\n",
    ")\n",
    "results_bert_pretrained = multi_run_benchmark(\n",
    "    X_bert_pretrained,\n",
    "    y_bert_pretrained,\n",
    "    num_trials=num_trials,\n",
    "    num_runs=num_runs,\n",
    ")\n",
    "print(\n",
    "    f\"BERT pretrained embeddings: {np.mean(results_bert_pretrained):.4f} Â± {np.std(results_bert_pretrained):.4f}\"\n",
    ")\n",
    "results_bert_finetuned = multi_run_benchmark(\n",
    "    X_bert_finetuned,\n",
    "    y_bert_finetuned,\n",
    "    num_trials=num_trials,\n",
    "    num_runs=num_runs,\n",
    ")\n",
    "print(\n",
    "    f\"BERT fine-tuned embeddings: {np.mean(results_bert_finetuned):.4f} Â± {np.std(results_bert_finetuned):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the gatortron-base model on the TCGA dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def prepare_data_and_tokenize(all_data, tokenizer):\n",
    "    # Ensure 'project_id' is used as a label and it is in an appropriate format\n",
    "    unique_labels = all_data[\"project_id\"].unique().tolist()\n",
    "    label_dict = {label: i for i, label in enumerate(unique_labels)}\n",
    "    all_data[\"labels\"] = all_data[\"project_id\"].map(label_dict)\n",
    "\n",
    "    # Convert DataFrame to a Dataset object\n",
    "    dataset = Dataset.from_pandas(all_data[[\"text\", \"labels\"]])\n",
    "\n",
    "    # Tokenization function to prepare input data\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "        )\n",
    "        model_inputs[\"labels\"] = examples[\n",
    "            \"labels\"\n",
    "        ]  # Ensure labels are included for the model to compute loss\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply tokenization\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Split the dataset\n",
    "    tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "    return tokenized_datasets[\"train\"], tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "unique_labels = all_data[\"project_id\"].unique().tolist()\n",
    "\n",
    "# Load tokenizer and model\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UFNLP/gatortron-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"UFNLP/gatortron-base\",\n",
    "    num_labels=len(unique_labels),\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"key\", \"query\", \"value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "# Prepare and tokenize data\n",
    "train_dataset, test_dataset = prepare_data_and_tokenize(all_data, tokenizer)\n",
    "\n",
    "# Updated training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Store all outputs and model checkpoints here.\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation is done at the end of each epoch.\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch to align with evaluations.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    logging_steps=10,  # Log training info every 10 steps.\n",
    "    load_best_model_at_end=True,  # Load the best model found during training at the end.\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy to determine the best model.\n",
    "    fp16=True,  # Use mixed precision training.\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Function to compute metrics during evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training results\n",
    "print(\"Training completed. Here are the stats:\")\n",
    "print(train_result)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "print(classification_report(test_dataset[\"labels\"], y_pred))\n",
    "\n",
    "MODEL_SAVE_FOLDER_NAME = \"gatortron-base-tcga\"\n",
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fine-tune the bert-base-uncased model on the TCGA dataset\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     AutoModelForSequenceClassification,\n",
    "# )\n",
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score,\n",
    "#     precision_recall_fscore_support,\n",
    "#     classification_report,\n",
    "# )\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# import torch\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "#         labels, predictions, average=\"weighted\"\n",
    "#     )\n",
    "#     acc = accuracy_score(labels, predictions)\n",
    "#     return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "# def prepare_data_and_tokenize(all_data, tokenizer):\n",
    "#     # Ensure 'project_id' is used as a label and it is in an appropriate format\n",
    "#     unique_labels = all_data[\"project_id\"].unique().tolist()\n",
    "#     label_dict = {label: i for i, label in enumerate(unique_labels)}\n",
    "#     all_data[\"labels\"] = all_data[\"project_id\"].map(label_dict)\n",
    "\n",
    "#     # Convert DataFrame to a Dataset object\n",
    "#     dataset = Dataset.from_pandas(all_data[[\"text\", \"labels\"]])\n",
    "\n",
    "#     # Tokenization function to prepare input data\n",
    "#     def tokenize_function(examples):\n",
    "#         model_inputs = tokenizer(\n",
    "#             examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "#         )\n",
    "#         model_inputs[\"labels\"] = examples[\n",
    "#             \"labels\"\n",
    "#         ]  # Ensure labels are included for the model to compute loss\n",
    "#         return model_inputs\n",
    "\n",
    "#     # Apply tokenization\n",
    "#     tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#     # Split the dataset\n",
    "#     tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "#     return tokenized_datasets[\"train\"], tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "# unique_labels = all_data[\"project_id\"].unique().tolist()\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     num_labels=len(unique_labels),\n",
    "#     torch_dtype=torch.float16,\n",
    "#     quantization_config=quantization_config,\n",
    "# )\n",
    "# config = LoraConfig(\n",
    "#     r=4,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=[\"key\", \"query\", \"value\"],\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "# lora_model = get_peft_model(model, config)\n",
    "\n",
    "# # Prepare and tokenize data\n",
    "# train_dataset, test_dataset = prepare_data_and_tokenize(all_data, tokenizer)\n",
    "\n",
    "# # Updated training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",  # Store all outputs and model checkpoints here.\n",
    "#     evaluation_strategy=\"epoch\",  # Evaluation is done at the end of each epoch.\n",
    "#     save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch to align with evaluations.\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",  # Directory for storing logs\n",
    "#     logging_steps=10,  # Log training info every 10 steps.\n",
    "#     load_best_model_at_end=True,  # Load the best model found during training at the end.\n",
    "#     metric_for_best_model=\"accuracy\",  # Use accuracy to determine the best model.\n",
    "#     fp16=True,  # Use mixed precision training.\n",
    "# )\n",
    "\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=lora_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     compute_metrics=compute_metrics,  # Function to compute metrics during evaluation\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# train_result = trainer.train()\n",
    "\n",
    "# # Print training results\n",
    "# print(\"Training completed. Here are the stats:\")\n",
    "# print(train_result)\n",
    "\n",
    "# # Evaluate the model\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "# y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "# print(classification_report(test_dataset[\"labels\"], y_pred))\n",
    "\n",
    "# MODEL_SAVE_FOLDER_NAME = \"bert-base-uncased-tcga\"\n",
    "# trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "# trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "# trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained(\"./gatortron-base-tcga\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"UFNLP/gatortron-base\")\n",
    "# model.push_to_hub(\"gatortron-base-tcga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honeybee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
