{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoneyBee Workshop Part 5: Retrieval Evaluation\n",
    "\n",
    "## Overview\n",
    "In this workshop, you'll learn how to:\n",
    "1. Implement similarity-based retrieval using embeddings\n",
    "2. Evaluate retrieval performance with Precision@k metrics\n",
    "3. Analyze retrieval failures and confusion patterns\n",
    "4. Compare retrieval across different modalities\n",
    "\n",
    "**Duration**: 30 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed Parts 1-4 or access to pre-computed embeddings\n",
    "- Understanding of information retrieval concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Similarity and evaluation imports\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Embeddings and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings (using clinical as example)\n",
    "local_path = Path(\"/mnt/f/Projects/HoneyBee/results/shared_data/embeddings\")\n",
    "\n",
    "if local_path.exists():\n",
    "    print(\"Loading from local pre-computed embeddings...\")\n",
    "    clinical_emb_path = local_path / \"clinical_embeddings_tcga.pkl\"\n",
    "    if clinical_emb_path.exists():\n",
    "        embeddings_df = pd.read_pickle(clinical_emb_path)\n",
    "    labels_path = local_path.parent / \"patient_cancer_types.csv\"\n",
    "    if labels_path.exists():\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "else:\n",
    "    # Create mock data\n",
    "    print(\"Creating mock data for demonstration...\")\n",
    "    n_samples = 500\n",
    "    n_features = 768\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(\n",
    "        np.random.randn(n_samples, n_features),\n",
    "        index=[f\"TCGA-{i:04d}\" for i in range(n_samples)]\n",
    "    )\n",
    "    \n",
    "    cancer_types = ['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD']\n",
    "    labels_df = pd.DataFrame({\n",
    "        'patient_id': embeddings_df.index,\n",
    "        'cancer_type': np.random.choice(cancer_types, n_samples)\n",
    "    })\n",
    "\n",
    "# Align data\n",
    "common_patients = list(set(embeddings_df.index) & set(labels_df['patient_id']))\n",
    "embeddings = embeddings_df.loc[common_patients].values\n",
    "labels = labels_df[labels_df['patient_id'].isin(common_patients)]['cancer_type'].values\n",
    "patient_ids = common_patients\n",
    "\n",
    "print(f\"Dataset shape: {embeddings.shape}\")\n",
    "print(f\"Unique cancer types: {np.unique(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingRetriever:\n",
    "    \"\"\"\n",
    "    Similarity-based retrieval system for embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, labels, metric='cosine'):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.metric = metric\n",
    "        \n",
    "        # Build nearest neighbors index\n",
    "        self.nn = NearestNeighbors(\n",
    "            n_neighbors=min(50, len(embeddings)),\n",
    "            metric=metric,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.nn.fit(embeddings)\n",
    "    \n",
    "    def retrieve(self, query_idx, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve k most similar items to query\n",
    "        \"\"\"\n",
    "        query = self.embeddings[query_idx].reshape(1, -1)\n",
    "        distances, indices = self.nn.kneighbors(query, n_neighbors=k+1)\n",
    "        \n",
    "        # Remove self from results\n",
    "        return indices[0][1:], distances[0][1:]\n",
    "    \n",
    "    def batch_retrieve(self, query_indices, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve for multiple queries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for idx in query_indices:\n",
    "            retrieved_indices, distances = self.retrieve(idx, k)\n",
    "            results.append({\n",
    "                'query_idx': idx,\n",
    "                'query_label': self.labels[idx],\n",
    "                'retrieved_indices': retrieved_indices,\n",
    "                'retrieved_labels': self.labels[retrieved_indices],\n",
    "                'distances': distances\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = EmbeddingRetriever(embeddings, labels, metric='cosine')\n",
    "print(\"Retrieval system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Precision@k Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(retriever, k_values=[1, 5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k for different k values\n",
    "    \"\"\"\n",
    "    n_queries = len(retriever.labels)\n",
    "    precisions = {k: [] for k in k_values}\n",
    "    \n",
    "    print(\"Evaluating retrieval performance...\")\n",
    "    for query_idx in tqdm(range(n_queries)):\n",
    "        query_label = retriever.labels[query_idx]\n",
    "        \n",
    "        # Retrieve top-k\n",
    "        max_k = max(k_values)\n",
    "        retrieved_indices, _ = retriever.retrieve(query_idx, k=max_k)\n",
    "        retrieved_labels = retriever.labels[retrieved_indices]\n",
    "        \n",
    "        # Calculate precision for each k\n",
    "        for k in k_values:\n",
    "            if k <= len(retrieved_indices):\n",
    "                relevant = (retrieved_labels[:k] == query_label).sum()\n",
    "                precision = relevant / k\n",
    "                precisions[k].append(precision)\n",
    "    \n",
    "    # Calculate mean precision\n",
    "    mean_precisions = {k: np.mean(p) for k, p in precisions.items()}\n",
    "    return mean_precisions, precisions\n",
    "\n",
    "# Calculate Precision@k\n",
    "k_values = [1, 5, 10, 20, 30]\n",
    "mean_precisions, all_precisions = calculate_precision_at_k(retriever, k_values)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrecision@k Results:\")\n",
    "for k, precision in mean_precisions.items():\n",
    "    print(f\"Precision@{k}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision@k curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_vals = sorted(mean_precisions.keys())\n",
    "precisions = [mean_precisions[k] for k in k_vals]\n",
    "\n",
    "plt.plot(k_vals, precisions, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('k', fontsize=12)\n",
    "plt.ylabel('Precision@k', fontsize=12)\n",
    "plt.title('Retrieval Performance: Precision@k', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for k, p in zip(k_vals, precisions):\n",
    "    plt.annotate(f'{p:.3f}', (k, p), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot by cancer type\n",
    "plt.figure(figsize=(12, 6))\n",
    "k = 10  # Focus on Precision@10\n",
    "precision_by_type = {}\n",
    "\n",
    "for cancer_type in np.unique(labels):\n",
    "    type_indices = np.where(labels == cancer_type)[0]\n",
    "    type_precisions = [all_precisions[k][i] for i in type_indices]\n",
    "    precision_by_type[cancer_type] = type_precisions\n",
    "\n",
    "df_box = pd.DataFrame(precision_by_type)\n",
    "df_box.boxplot(figsize=(10, 6))\n",
    "plt.ylabel(f'Precision@{k}')\n",
    "plt.title(f'Retrieval Performance by Cancer Type (Precision@{k})')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Retrieval Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hard cases (low precision queries)\n",
    "k = 10\n",
    "precision_threshold = 0.5\n",
    "\n",
    "hard_cases = []\n",
    "for idx, precision in enumerate(all_precisions[k]):\n",
    "    if precision < precision_threshold:\n",
    "        hard_cases.append({\n",
    "            'index': idx,\n",
    "            'patient_id': patient_ids[idx],\n",
    "            'label': labels[idx],\n",
    "            'precision': precision\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(hard_cases)} hard cases (Precision@{k} < {precision_threshold})\")\n",
    "\n",
    "# Analyze confusion patterns\n",
    "confusion_matrix = np.zeros((len(np.unique(labels)), len(np.unique(labels))))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "\n",
    "for query_idx in range(len(labels)):\n",
    "    query_label = labels[query_idx]\n",
    "    retrieved_indices, _ = retriever.retrieve(query_idx, k=k)\n",
    "    retrieved_labels = labels[retrieved_indices]\n",
    "    \n",
    "    for ret_label in retrieved_labels:\n",
    "        confusion_matrix[label_to_idx[query_label], label_to_idx[ret_label]] += 1\n",
    "\n",
    "# Normalize confusion matrix\n",
    "confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.title('Retrieval Confusion Matrix (Normalized)')\n",
    "plt.ylabel('Query Cancer Type')\n",
    "plt.xlabel('Retrieved Cancer Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Retrieval Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific retrieval examples\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select a few query examples\n",
    "n_examples = 3\n",
    "query_indices = np.random.choice(len(labels), n_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_examples, figsize=(15, 5))\n",
    "if n_examples == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax_idx, query_idx in enumerate(query_indices):\n",
    "    # Get query and retrieved items\n",
    "    retrieved_indices, distances = retriever.retrieve(query_idx, k=10)\n",
    "    all_indices = np.concatenate([[query_idx], retrieved_indices])\n",
    "    \n",
    "    # Get embeddings for visualization\n",
    "    vis_embeddings = embeddings[all_indices]\n",
    "    \n",
    "    # Reduce dimensionality\n",
    "    if vis_embeddings.shape[1] > 50:\n",
    "        pca = PCA(n_components=50, random_state=42)\n",
    "        vis_embeddings = pca.fit_transform(vis_embeddings)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    vis_2d = tsne.fit_transform(vis_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    # Plot retrieved items\n",
    "    colors = ['green' if labels[idx] == labels[query_idx] else 'red' \n",
    "              for idx in retrieved_indices]\n",
    "    ax.scatter(vis_2d[1:, 0], vis_2d[1:, 1], c=colors, alpha=0.6, s=100)\n",
    "    \n",
    "    # Plot query\n",
    "    ax.scatter(vis_2d[0, 0], vis_2d[0, 1], c='blue', s=200, \n",
    "              marker='*', edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_title(f'Query: {labels[query_idx]}')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='*', color='w', markerfacecolor='blue', \n",
    "               markersize=10, label='Query'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='green', \n",
    "               markersize=10, label='Correct'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', \n",
    "               markersize=10, label='Incorrect')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Different Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different distance metrics\n",
    "metrics = ['cosine', 'euclidean', 'manhattan']\n",
    "metric_results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\nEvaluating {metric} distance...\")\n",
    "    \n",
    "    # Create retriever with specific metric\n",
    "    metric_retriever = EmbeddingRetriever(embeddings, labels, metric=metric)\n",
    "    \n",
    "    # Calculate Precision@10\n",
    "    mean_prec, _ = calculate_precision_at_k(metric_retriever, k_values=[10])\n",
    "    metric_results[metric] = mean_prec[10]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics_list = list(metric_results.keys())\n",
    "precisions = list(metric_results.values())\n",
    "\n",
    "bars = plt.bar(metrics_list, precisions)\n",
    "plt.xlabel('Distance Metric')\n",
    "plt.ylabel('Precision@10')\n",
    "plt.title('Retrieval Performance by Distance Metric')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, precision in zip(bars, precisions):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{precision:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Modal Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock multi-modal embeddings\n",
    "modalities = ['clinical', 'pathology', 'radiology']\n",
    "modality_embeddings = {}\n",
    "\n",
    "for i, modality in enumerate(modalities):\n",
    "    if modality == 'clinical':\n",
    "        modality_embeddings[modality] = embeddings\n",
    "    else:\n",
    "        # Create correlated mock embeddings\n",
    "        noise = np.random.randn(*embeddings.shape) * 0.5\n",
    "        modality_embeddings[modality] = embeddings + noise\n",
    "\n",
    "# Evaluate each modality\n",
    "modality_results = {}\n",
    "\n",
    "for modality, mod_embeddings in modality_embeddings.items():\n",
    "    print(f\"\\nEvaluating {modality} modality...\")\n",
    "    mod_retriever = EmbeddingRetriever(mod_embeddings, labels)\n",
    "    mean_prec, _ = calculate_precision_at_k(mod_retriever, k_values=[1, 5, 10, 20])\n",
    "    modality_results[modality] = mean_prec\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_values = sorted(list(modality_results.values())[0].keys())\n",
    "\n",
    "for modality, results in modality_results.items():\n",
    "    precisions = [results[k] for k in k_values]\n",
    "    plt.plot(k_values, precisions, 'o-', label=modality, linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.title('Retrieval Performance Across Modalities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Adjusted Mutual Information (AMI) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AMI scores for different k values\n",
    "def calculate_ami_scores(retriever, k_values=[5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Calculate Adjusted Mutual Information scores\n",
    "    \"\"\"\n",
    "    ami_scores = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"Calculating AMI for k={k}...\")\n",
    "        ami_values = []\n",
    "        \n",
    "        for query_idx in range(len(retriever.labels)):\n",
    "            # Get query cluster (label)\n",
    "            query_label = retriever.labels[query_idx]\n",
    "            \n",
    "            # Get retrieved items\n",
    "            retrieved_indices, _ = retriever.retrieve(query_idx, k=k)\n",
    "            \n",
    "            # Create cluster assignments\n",
    "            true_labels = [query_label] + list(retriever.labels[retrieved_indices])\n",
    "            pred_labels = [0] + [1] * k  # Query vs retrieved\n",
    "            \n",
    "            # Calculate AMI\n",
    "            ami = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "            ami_values.append(ami)\n",
    "        \n",
    "        ami_scores[k] = np.mean(ami_values)\n",
    "    \n",
    "    return ami_scores\n",
    "\n",
    "# Calculate AMI scores\n",
    "ami_scores = calculate_ami_scores(retriever)\n",
    "\n",
    "# Plot AMI scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "k_vals = sorted(ami_scores.keys())\n",
    "scores = [ami_scores[k] for k in k_vals]\n",
    "\n",
    "plt.plot(k_vals, scores, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('AMI Score')\n",
    "plt.title('Adjusted Mutual Information vs k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for k, score in zip(k_vals, scores):\n",
    "    plt.annotate(f'{score:.3f}', (k, score), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this workshop, you learned to:\n",
    "1. ✅ Implement similarity-based retrieval systems\n",
    "2. ✅ Evaluate retrieval with Precision@k metrics\n",
    "3. ✅ Analyze retrieval failures and confusion patterns\n",
    "4. ✅ Compare different distance metrics and modalities\n",
    "5. ✅ Calculate AMI scores for clustering quality\n",
    "\n",
    "**Next Workshop**: Part 6 - Survival Analysis\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Embeddings enable effective similarity-based retrieval\n",
    "- Cosine similarity often works best for normalized embeddings\n",
    "- Retrieval performance varies by cancer type and modality\n",
    "- Failure analysis reveals which cancer types are easily confused\n",
    "\n",
    "**Exercise**: Try implementing re-ranking strategies or learning-to-rank approaches!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}