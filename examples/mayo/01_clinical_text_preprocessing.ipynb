{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoneyBee Workshop Part 1: Clinical Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoneyBee clinical processing modules loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add HoneyBee to path\n",
    "sys.path.append('/mnt/f/Projects/HoneyBee')\n",
    "\n",
    "# Import HoneyBee components\n",
    "from honeybee.loaders import Reader\n",
    "from honeybee.processors import ClinicalProcessor\n",
    "from honeybee.models import HuggingFaceEmbedder\n",
    "\n",
    "print(\"HoneyBee clinical processing modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Clinical Data\n",
    "\n",
    "We'll use the sample PDF clinical report provided in the examples folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clinical report from: ../samples/sample.pdf\n",
      "\n",
      "First 500 characters of the clinical report:\n",
      "--------------------------------------------------\n",
      "Patient Name:  PATIENT P.N 1 AGESEX: M :RIN NAME :  AGESEX: PHYSICIAN:MATH.NO MED. REC. NO: SURGERY DATE: RECEIVE DATE:UUID:4854A37F- 5F68-4EA0-99F7-E0572EA9533F TCGA-06-0150-01A-PR Redacted iii 111111111111111111111111a111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 1111 111 III ---------------------------------------------------------------- PATHOLOGICAL DIAGNOSIS: BRAIN BIOPSY: GLIOBLASTOMA \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Path to sample clinical report\n",
    "sample_pdf = \"../samples/sample.pdf\"\n",
    "\n",
    "# Initialize the clinical data loader\n",
    "reader = Reader.reader.PDF()\n",
    "\n",
    "# Load the PDF\n",
    "print(f\"Loading clinical report from: {sample_pdf}\")\n",
    "clinical_text = reader.read(sample_pdf)\n",
    "\n",
    "# Display first 500 characters\n",
    "print(\"\\nFirst 500 characters of the clinical report:\")\n",
    "print(\"-\" * 50)\n",
    "print(clinical_text[:500])\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Clinical Text\n",
    "\n",
    "The ClinicalProcessor handles:\n",
    "- Entity extraction (diseases, medications, procedures)\n",
    "- Medical code normalization (SNOMED-CT, RxNorm, ICD-O-3)\n",
    "- Temporal information extraction\n",
    "- Text cleaning and structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing clinical text...\n",
      "\n",
      "Extracted Medical Entities:\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"text\": \"Patient Name:  PATIENT P.N 1 AGESEX: M :RIN NAME :  AGESEX: PHYSICIAN:MATH.NO MED. REC. NO: SURGERY DATE: RECEIVE DATE:UUID:4854A37F- 5F68-4EA0-99F7-E0572EA9533F TCGA-06-0150-01A-PR Redacted iii 111111111111111111111111a111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 1111 111 III ---------------------------------------------------------------- PATHOLOGICAL DIAGNOSIS: BRAIN BIOPSY: GLIOBLASTOMA PROLIFERATIVE INDEX, MIB-1: MORE THAN 40. ---------------------------------------------------------------- OperationSpecimen: Brain tumor. F.S. Clinical History and Pre-Op Dx: None given. GROSS PATHOLOGY: The specimen is labeled brain biopsy consiting of 0.2 x 0.4 cm pink-white tissue. A portion of the specimen has been examined by frozen section and is submitted in cassette 1. The remaining tissue is submitted in cassette 2. A.OPERATIVE CONSULTATION: Frozen section diagnosis: High grade gl i oma . MICROSCOPIC: Sections show a glioblastoma characterized by high cellular density, frequent nuclear pleomorphism, mitotic figures, endothelial-vascular proliferation with thrombosis and patchy necrosis. The astrocytic.lineage of the tumor cells is confirmed by positive GFAP stain. The proliferative index as measured by MIB-1 stain is more than 40. TISSUE COMMITTEE CODE: BILLING CODES-- Qaatomic Pathology Cytology Sex: Muid RoomlBed: Requested by:Page 1\",\n",
      "  \"document_type\": \"unknown\",\n",
      "  \"processing_timestamp\": \"2025-07-06T19:36:13.856427\",\n",
      "  \"document_structure\": {\n",
      "    \"sections\": {\n",
      "      \"physical_exam\": \"Patient Name:  PATIENT P.N 1 AGESEX: M :RIN NAME :  AGESEX: PHYSICIAN:MATH.NO MED. REC. NO: SURGERY DATE: RECEIVE DATE:UUID:4854A37F- 5F68-4EA0-99F7-E0572EA9533F TCGA-06-0150-01A-PR Redacted iii 111111111111111111111111a111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 1111 111 III ---------------------------------------------------------------- PATHOLOGICAL DIAGNOSIS: BRAIN BIOPSY: GLIOBLASTOMA PROLIFERATIVE INDEX, MIB-1: MORE THAN 40. ---------------------------------------------------------------- OperationSpecimen: Brain tumor. F.S. Clinical History and Pre-Op Dx: None given. GROSS PATHOLOGY: The specimen is labeled brain biopsy consiting of 0.2 x 0.4 cm pink-white tissue. A portion of the specimen has been examined by frozen section and is submitted in cassette 1. The remaining tissue is submitted in cassette 2. A.OPERATIVE CONSULTATION: Frozen section diagnosis: High grade gl i oma . MICROSCOPIC: Sections show a glioblastoma characterized by high cellular density, frequent nuclear pleomorphism, mitotic figures, endothelial-vascular proliferation with thrombosis and patchy necrosis. The astrocytic.lineage of the tumor cells is confirmed by positive GFAP stain. The proliferative index as measured by MIB-1 stain is more than 40. TISSUE COMMITTEE CODE: BILLING CODES-- Qaatomic Pathology Cytology Sex: Muid RoomlBed: Requested by:Page 1\"\n",
      "    },\n",
      "    \"headers\": [\n",
      "      \"physical_exam\"\n",
      "    ],\n",
      "    \"num_sections\": 1\n",
      "  },\n",
      "  \"tokenization\": {\n",
      "    \"input_ids\": [\n",
      "      101,\n",
      "      324,\n",
      "      2924,\n",
      "      121,\n",
      "      324,\n",
      "      122,\n",
      "      112,\n",
      "      155,\n",
      "      147,\n",
      "      12106,\n",
      "      786,\n",
      "      121,\n",
      "      128,\n",
      "      121,\n",
      "      249,\n",
      "      105,\n",
      "      2924,\n",
      "      121,\n",
      "      12106,\n",
      "      786,\n",
      "      121,\n",
      "      2316,\n",
      "      121,\n",
      "      15531,\n",
      "      112,\n",
      "      289,\n",
      "      1049,\n",
      "      112,\n",
      "      477,\n",
      "      112,\n",
      "      289,\n",
      "      121,\n",
      "      1464,\n",
      "      1791,\n",
      "      121,\n",
      "      3451,\n",
      "      1791,\n",
      "      121,\n",
      "      263,\n",
      "      33890,\n",
      "      50018,\n",
      "      121,\n",
      "      36427,\n",
      "      50061,\n",
      "      50011,\n",
      "      4826,\n",
      "      50025,\n",
      "      141,\n",
      "      254,\n",
      "      50025,\n",
      "      7975,\n",
      "      141,\n",
      "      244,\n",
      "      39474,\n",
      "      50044,\n",
      "      141,\n",
      "      1319,\n",
      "      50025,\n",
      "      50069,\n",
      "      141,\n",
      "      175,\n",
      "      3867,\n",
      "      8657,\n",
      "      39474,\n",
      "      12751,\n",
      "      6356,\n",
      "      50025,\n",
      "      107,\n",
      "      3203,\n",
      "      50011,\n",
      "      141,\n",
      "      1756,\n",
      "      141,\n",
      "      45603,\n",
      "      141,\n",
      "      2831,\n",
      "      50011,\n",
      "      141,\n",
      "      2998,\n",
      "      2224,\n",
      "      340,\n",
      "      118,\n",
      "      18784,\n",
      "      100,\n",
      "      22989,\n",
      "      6592,\n",
      "      18784,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      12089,\n",
      "      2368,\n",
      "      121,\n",
      "      2443,\n",
      "      3233,\n",
      "      121,\n",
      "      29739,\n",
      "      19452,\n",
      "      3146,\n",
      "      119,\n",
      "      4287,\n",
      "      50031,\n",
      "      141,\n",
      "      147,\n",
      "      121,\n",
      "      860,\n",
      "      815,\n",
      "      781,\n",
      "      112,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      141,\n",
      "      15328,\n",
      "      3933,\n",
      "      2395,\n",
      "      121,\n",
      "      2443,\n",
      "      2711,\n",
      "      112,\n",
      "      131,\n",
      "      112,\n",
      "      117,\n",
      "      112,\n",
      "      1986,\n",
      "      663,\n",
      "      150,\n",
      "      459,\n",
      "      141,\n",
      "      1354,\n",
      "      10402,\n",
      "      121,\n",
      "      3169,\n",
      "      1286,\n",
      "      112,\n",
      "      3414,\n",
      "      6232,\n",
      "      121,\n",
      "      134,\n",
      "      6220,\n",
      "      200,\n",
      "      7873,\n",
      "      2443,\n",
      "      3233,\n",
      "      621,\n",
      "      1365,\n",
      "      154,\n",
      "      221,\n",
      "      112,\n",
      "      173,\n",
      "      632,\n",
      "      221,\n",
      "      112,\n",
      "      244,\n",
      "      972,\n",
      "      5615,\n",
      "      141,\n",
      "      2899,\n",
      "      1938,\n",
      "      112,\n",
      "      106,\n",
      "      5949,\n",
      "      154,\n",
      "      134,\n",
      "      6220,\n",
      "      394,\n",
      "      636,\n",
      "      3328,\n",
      "      255,\n",
      "      12134,\n",
      "      3609,\n",
      "      150,\n",
      "      200,\n",
      "      9024,\n",
      "      145,\n",
      "      13839,\n",
      "      147,\n",
      "      112,\n",
      "      134,\n",
      "      7063,\n",
      "      1938,\n",
      "      200,\n",
      "      9024,\n",
      "      145,\n",
      "      13839,\n",
      "      173,\n",
      "      112,\n",
      "      106,\n",
      "      112,\n",
      "      4458,\n",
      "      7419,\n",
      "      121,\n",
      "      12134,\n",
      "      3609,\n",
      "      2368,\n",
      "      121,\n",
      "      1080,\n",
      "      4386,\n",
      "      1065,\n",
      "      2299,\n",
      "      6022,\n",
      "      50011,\n",
      "      112,\n",
      "      13862,\n",
      "      121,\n",
      "      8117,\n",
      "      849,\n",
      "      106,\n",
      "      29739,\n",
      "      6518,\n",
      "      255,\n",
      "      1080,\n",
      "      6237,\n",
      "      3815,\n",
      "      119,\n",
      "      5878,\n",
      "      7299,\n",
      "      1886,\n",
      "      26800,\n",
      "      1653,\n",
      "      119,\n",
      "      23092,\n",
      "      17841,\n",
      "      119,\n",
      "      10018,\n",
      "      141,\n",
      "      3696,\n",
      "      7065,\n",
      "      189,\n",
      "      8007,\n",
      "      150,\n",
      "      13184,\n",
      "      10286,\n",
      "      112,\n",
      "      134,\n",
      "      35354,\n",
      "      129,\n",
      "      112,\n",
      "      16917,\n",
      "      154,\n",
      "      134,\n",
      "      2711,\n",
      "      935,\n",
      "      200,\n",
      "      3476,\n",
      "      255,\n",
      "      1705,\n",
      "      213,\n",
      "      16382,\n",
      "      50023,\n",
      "      12872,\n",
      "      112,\n",
      "      134,\n",
      "      19452,\n",
      "      3146,\n",
      "      233,\n",
      "      3495,\n",
      "      255,\n",
      "      4287,\n",
      "      50031,\n",
      "      141,\n",
      "      147,\n",
      "      12872,\n",
      "      200,\n",
      "      860,\n",
      "      815,\n",
      "      781,\n",
      "      112,\n",
      "      1938,\n",
      "      20409,\n",
      "      2716,\n",
      "      121,\n",
      "      21525,\n",
      "      14636,\n",
      "      141,\n",
      "      141,\n",
      "      2356,\n",
      "      50011,\n",
      "      9476,\n",
      "      9904,\n",
      "      6232,\n",
      "      19262,\n",
      "      3055,\n",
      "      121,\n",
      "      22691,\n",
      "      187,\n",
      "      3135,\n",
      "      39283,\n",
      "      118,\n",
      "      121,\n",
      "      5293,\n",
      "      255,\n",
      "      121,\n",
      "      4608,\n",
      "      147,\n",
      "      102\n",
      "    ],\n",
      "    \"attention_mask\": [\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      1\n",
      "    ],\n",
      "    \"num_tokens\": 417,\n",
      "    \"tokenizer_name\": \"gatortron\"\n",
      "  },\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"UUID:4854A\",\n",
      "      \"type\": \"measurement\",\n",
      "      \"start\": 119,\n",
      "      \"end\": 129,\n",
      "      \"properties\": {\n",
      "        \"test_name\": \"UUID\",\n",
      "        \"value\": \"4854\",\n",
      "        \"unit\": \"A\",\n",
      "        \"source\": \"rule-based\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"PR\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 179,\n",
      "      \"end\": 181,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"BRAIN\",\n",
      "      \"type\": \"tumor\",\n",
      "      \"start\": 473,\n",
      "      \"end\": 478,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"tumor_location\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"PR\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 500,\n",
      "      \"end\": 502,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Brain\",\n",
      "      \"type\": \"tumor\",\n",
      "      \"start\": 626,\n",
      "      \"end\": 631,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"tumor_location\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Pr\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 665,\n",
      "      \"end\": 667,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"brain\",\n",
      "      \"type\": \"tumor\",\n",
      "      \"start\": 729,\n",
      "      \"end\": 734,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"tumor_location\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"0.4 cm\",\n",
      "      \"type\": \"measurement\",\n",
      "      \"start\": 761,\n",
      "      \"end\": 767,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"tumor_size\",\n",
      "        \"source\": \"cancer-specific\",\n",
      "        \"size\": \"0.4\",\n",
      "        \"unit\": \"cm\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"CR\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 1006,\n",
      "      \"end\": 1008,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"pr\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 1155,\n",
      "      \"end\": 1157,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"cr\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 1198,\n",
      "      \"end\": 1200,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"pr\",\n",
      "      \"type\": \"response\",\n",
      "      \"start\": 1289,\n",
      "      \"end\": 1291,\n",
      "      \"properties\": {\n",
      "        \"pattern\": \"treatment_response\",\n",
      "        \"source\": \"cancer-specific\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"entity_relationships\": [],\n",
      "  \"temporal_timeline\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Initialize the clinical processor\n",
    "processor = ClinicalProcessor()\n",
    "\n",
    "# Process the clinical text\n",
    "print(\"Processing clinical text...\")\n",
    "processed_data = processor.process_text(clinical_text)\n",
    "\n",
    "# Display extracted entities\n",
    "print(\"\\nExtracted Medical Entities:\")\n",
    "print(\"-\" * 50)\n",
    "print(json.dumps(processed_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Clinical Embeddings\n",
    "\n",
    "HoneyBee supports multiple clinical embedding models:\n",
    "- **GatorTron**: Large clinical language model\n",
    "- **BioBERT**: Biomedical BERT\n",
    "- **PubMedBERT**: BERT trained on PubMed abstracts\n",
    "- **Clinical-T5**: T5 model for clinical text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedder with BioBERT (lighter weight for demo)\n",
    "embedder = HuggingFaceEmbedder(model_name=\"UFNLP/gatortron-base\", pooling_method='pooler_output')\n",
    "\n",
    "embeddings = embedder.generate_embeddings(clinical_text)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Multiple Files\n",
    "\n",
    "In real scenarios, you'll process multiple clinical documents. Here's how to handle batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:honeybee.processors.clinical_processor:Processing document: Patient presents with adenocarcinoma of the lung. Started on carboplatin and paclitaxel.\n",
      "ERROR:honeybee.processors.clinical_processor:Error processing Patient presents with adenocarcinoma of the lung. Started on carboplatin and paclitaxel.: Unsupported file format: \n",
      "INFO:honeybee.processors.clinical_processor:Processing document: Follow-up CT scan shows partial response to treatment. No new metastases identified.\n",
      "ERROR:honeybee.processors.clinical_processor:Error processing Follow-up CT scan shows partial response to treatment. No new metastases identified.: Unsupported file format: \n",
      "INFO:honeybee.processors.clinical_processor:Processing document: Pathology report confirms invasive ductal carcinoma, ER+/PR+/HER2-. Grade 2.\n",
      "ERROR:honeybee.processors.clinical_processor:Error processing Pathology report confirms invasive ductal carcinoma, ER+/PR+/HER2-. Grade 2.: Unsupported file format: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing text 1...\n",
      "\n",
      "Processing text 2...\n",
      "\n",
      "Processing text 3...\n",
      "\n",
      "Total embeddings shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Example: Process multiple clinical texts\n",
    "# In practice, you would have multiple files\n",
    "sample_texts = [\n",
    "    \"Patient presents with adenocarcinoma of the lung. Started on carboplatin and paclitaxel.\",\n",
    "    \"Follow-up CT scan shows partial response to treatment. No new metastases identified.\",\n",
    "    \"Pathology report confirms invasive ductal carcinoma, ER+/PR+/HER2-. Grade 2.\"\n",
    "]\n",
    "\n",
    "# Process and embed each text\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nProcessing text {i+1}...\")\n",
    "    \n",
    "    # Process\n",
    "    processed = processor.process(text)\n",
    "    \n",
    "    # Generate embedding\n",
    "    embedding = embedder.generate_embeddings(text)\n",
    "    \n",
    "    all_embeddings.append(embedding)\n",
    "    all_metadata.append({\n",
    "        'text_id': f'sample_{i+1}',\n",
    "        'text_preview': text[:50] + '...',\n",
    "        'entities': processed.get('entities', {})\n",
    "    })\n",
    "\n",
    "# Stack embeddings\n",
    "embeddings_matrix = np.vstack(all_embeddings)\n",
    "print(f\"\\nTotal embeddings shape: {embeddings_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Embeddings for Downstream Tasks\n",
    "\n",
    "Save embeddings in formats compatible with downstream analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to: /mnt/f/Projects/HoneyBee/examples/mayo/outputs/clinical_embeddings.npy\n",
      "Saved metadata to: /mnt/f/Projects/HoneyBee/examples/mayo/outputs/clinical_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"/mnt/f/Projects/HoneyBee/examples/mayo/outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save embeddings as numpy array\n",
    "np.save(output_dir / \"clinical_embeddings.npy\", embeddings_matrix)\n",
    "print(f\"Saved embeddings to: {output_dir / 'clinical_embeddings.npy'}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_df = pd.DataFrame(all_metadata)\n",
    "metadata_df.to_csv(output_dir / \"clinical_metadata.csv\", index=False)\n",
    "print(f\"Saved metadata to: {output_dir / 'clinical_metadata.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Using Different Embedding Models\n",
    "\n",
    "Let's compare embeddings from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings with BioBERT...\n",
      "  Shape: (1, 768)\n",
      "\n",
      "Generating embeddings with ClinicalBERT...\n",
      "  Shape: (1, 768)\n",
      "\n",
      "Embedding generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Compare different clinical embedding models\n",
    "models_to_compare = [\n",
    "    (\"dmis-lab/biobert-v1.1\", \"BioBERT\"),\n",
    "    (\"emilyalsentzer/Bio_ClinicalBERT\", \"ClinicalBERT\"),\n",
    "    # Add more models as needed\n",
    "]\n",
    "\n",
    "model_embeddings = {}\n",
    "sample_text = sample_texts[0]  # Use first sample\n",
    "\n",
    "for model_name, display_name in models_to_compare:\n",
    "    try:\n",
    "        print(f\"\\nGenerating embeddings with {display_name}...\")\n",
    "        embedder = HuggingFaceEmbedder(model_name=model_name, pooling_method='pooler_output')\n",
    "        embedding = embedder.generate_embeddings(sample_text)\n",
    "        model_embeddings[display_name] = embedding\n",
    "        print(f\"  Shape: {embedding.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nEmbedding generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with HuggingFace Datasets\n",
    "\n",
    "For larger scale processing, we can load pre-computed TCGA embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0 available.\n",
      "INFO:datasets:Duckdb version 1.2.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TCGA clinical embeddings from HuggingFace...\n",
      "Dataset available at: https://huggingface.co/datasets/Lab-Rasool/TCGA\n",
      "\n",
      "TCGA dataset structure:\n",
      "- Clinical embeddings\n",
      "- Patient metadata\n",
      "- Cancer type labels\n",
      "- Survival information\n"
     ]
    }
   ],
   "source": [
    "# Example of loading pre-computed TCGA embeddings\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TCGA clinical embeddings from HuggingFace...\")\n",
    "print(\"Dataset available at: https://huggingface.co/datasets/Lab-Rasool/TCGA\")\n",
    "\n",
    "# This would load the actual dataset\n",
    "# clinical_dataset = load_dataset(\"Lab-Rasool/TCGA\", \"clinical\", split=\"gatortron\")\n",
    "\n",
    "# For now, let's show the structure\n",
    "print(\"\\nTCGA dataset structure:\")\n",
    "print(\"- Clinical embeddings\")\n",
    "print(\"- Patient metadata\")\n",
    "print(\"- Cancer type labels\")\n",
    "print(\"- Survival information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this workshop, you learned to:\n",
    "1. ✅ Load clinical text from PDFs and text files\n",
    "2. ✅ Extract and normalize medical entities\n",
    "3. ✅ Generate embeddings using clinical language models\n",
    "4. ✅ Save embeddings for downstream analysis\n",
    "\n",
    "**Next Workshop**: Part 2 - Radiology DICOM Processing\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Clinical text requires specialized processing for medical entities\n",
    "- Different embedding models capture different aspects of clinical language\n",
    "- Proper preprocessing improves downstream task performance\n",
    "\n",
    "**Exercise**: Try processing your own clinical text files and comparing different embedding models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HoneyBee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
