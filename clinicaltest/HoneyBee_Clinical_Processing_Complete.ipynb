{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoneyBee Clinical Processing - Complete Guide\n",
    "\n",
    "This comprehensive notebook demonstrates all capabilities of the HoneyBee clinical processing system, including:\n",
    "\n",
    "- üìÑ **PDF Processing** with OCR\n",
    "- üìù **Text Processing** with entity extraction\n",
    "- üß¨ **Cancer-Specific Entity Recognition**\n",
    "- ‚è∞ **Temporal Timeline Extraction**\n",
    "- ü§ñ **Embedding Generation** with multiple biomedical models\n",
    "- ‚öôÔ∏è **Advanced Configuration** options\n",
    "- üì¶ **Batch Processing** capabilities\n",
    "- üîó **HoneyBee API** integration\n",
    "\n",
    "---\n",
    "\n",
    "**Documentation:** https://lab-rasool.github.io/HoneyBee/docs/clinical-processing/\n",
    "\n",
    "**Author:** HoneyBee Development Team  \n",
    "**Version:** 1.0  \n",
    "**Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "### System Dependencies\n",
    "```bash\n",
    "# Ubuntu/Debian\n",
    "sudo apt-get install openslide-tools tesseract-ocr\n",
    "\n",
    "# macOS\n",
    "brew install openslide tesseract\n",
    "```\n",
    "\n",
    "### Python Dependencies\n",
    "```bash\n",
    "pip install torch transformers pytesseract pillow PyPDF2 pdf2image nltk opencv-python\n",
    "```\n",
    "\n",
    "### NLTK Data\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import HoneyBee components\n",
    "from honeybee import HoneyBee\n",
    "from honeybee.processors import ClinicalProcessor\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÑ Section 1: Basic PDF Processing\n",
    "\n",
    "Process clinical documents (PDF, images) with OCR support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Clinical Processor\n",
    "processor = ClinicalProcessor()\n",
    "\n",
    "# Path to sample PDF\n",
    "pdf_path = Path(\"sample.PDF\")\n",
    "\n",
    "if pdf_path.exists():\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Process the PDF\n",
    "    result = processor.process(pdf_path, save_output=False)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úì PDF Processed Successfully\")\n",
    "    print(f\"\\nExtraction Details:\")\n",
    "    print(f\"  Method: {result['document_processing']['method']}\")\n",
    "    print(f\"  Text Length: {len(result['text'])} characters\")\n",
    "    print(f\"  Entities Extracted: {len(result['entities'])}\")\n",
    "    print(f\"  Document Sections: {result['document_structure']['num_sections']}\")\n",
    "    \n",
    "    # Show first 500 characters\n",
    "    print(f\"\\nExtracted Text (first 500 chars):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(result['text'][:500] + \"...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show extracted entities\n",
    "    print(f\"\\nExtracted Entities (first 10):\")\n",
    "    for i, entity in enumerate(result['entities'][:10], 1):\n",
    "        print(f\"  {i}. {entity['text']:30s} [{entity['type']}]\")\n",
    "else:\n",
    "    print(f\"‚ö† PDF not found: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Section 2: Direct Text Processing\n",
    "\n",
    "Process clinical text directly without files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample clinical text\n",
    "clinical_text = \"\"\"\n",
    "PATHOLOGY REPORT\n",
    "\n",
    "Patient: Jane Smith\n",
    "Date: March 15, 2024\n",
    "\n",
    "DIAGNOSIS: Invasive Ductal Carcinoma, Right Breast\n",
    "\n",
    "CLINICAL HISTORY:\n",
    "Patient presented on February 20, 2024 with a palpable 2.5 cm mass in the right\n",
    "upper outer quadrant. Core biopsy on February 28, 2024 confirmed invasive carcinoma.\n",
    "\n",
    "MICROSCOPIC FINDINGS:\n",
    "Invasive ductal carcinoma, Grade 2 (Nottingham score 6)\n",
    "- Tubule formation: 20% (score 2)\n",
    "- Nuclear pleomorphism: Moderate (score 2)\n",
    "- Mitotic rate: 8 per 10 HPF (score 2)\n",
    "\n",
    "IMMUNOHISTOCHEMISTRY:\n",
    "ER: Positive (95%, strong)\n",
    "PR: Positive (80%, moderate)\n",
    "HER2: Negative (IHC 1+)\n",
    "Ki-67: 18%\n",
    "\n",
    "TNM: pT2 N0 M0, Stage IIA\n",
    "\n",
    "TREATMENT:\n",
    "Started tamoxifen 20 mg PO daily on March 25, 2024\n",
    "Follow-up: May 1, 2024\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing Clinical Text...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process the text\n",
    "result = processor.process_text(clinical_text, document_type=\"pathology_report\")\n",
    "\n",
    "print(f\"\\n‚úì Text Processed Successfully\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Document Type: {result['document_type']}\")\n",
    "print(f\"  Text Length: {len(result['text'])} characters\")\n",
    "print(f\"  Entities: {len(result['entities'])}\")\n",
    "print(f\"  Timeline Events: {len(result['temporal_timeline'])}\")\n",
    "print(f\"  Entity Relationships: {len(result['entity_relationships'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display entity breakdown by type\n",
    "entity_types = {}\n",
    "for entity in result['entities']:\n",
    "    et = entity['type']\n",
    "    entity_types[et] = entity_types.get(et, 0) + 1\n",
    "\n",
    "print(\"\\nEntity Type Breakdown:\")\n",
    "print(\"-\" * 40)\n",
    "for entity_type in sorted(entity_types.keys()):\n",
    "    count = entity_types[entity_type]\n",
    "    bar = \"‚ñà\" * min(count, 30)\n",
    "    print(f\"{entity_type:20s} {count:3d} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display temporal timeline\n",
    "if result['temporal_timeline']:\n",
    "    print(\"\\nTemporal Timeline:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, event in enumerate(result['temporal_timeline'], 1):\n",
    "        print(f\"\\n{i}. {event['temporal_text']}\")\n",
    "        if event.get('normalized_date'):\n",
    "            print(f\"   Normalized: {event['normalized_date']}\")\n",
    "        \n",
    "        related = event.get('related_entities', [])\n",
    "        if related:\n",
    "            print(f\"   Related entities: {len(related)}\")\n",
    "            for idx in related[:3]:  # Show first 3\n",
    "                if idx < len(result['entities']):\n",
    "                    e = result['entities'][idx]\n",
    "                    print(f\"     ‚Ä¢ {e['text']} ({e['type']})\")\n",
    "else:\n",
    "    print(\"\\nNo timeline events found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚öôÔ∏è Section 3: Configuration Options\n",
    "\n",
    "Customize processing with different strategies and options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sentence Segmentation with Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for sentence-based segmentation\n",
    "config_sentence = {\n",
    "    \"tokenization\": {\n",
    "        \"model\": \"gatortron\",\n",
    "        \"segment_strategy\": \"sentence\",\n",
    "        \"long_document_strategy\": \"sliding_window\",\n",
    "        \"stride\": 128\n",
    "    },\n",
    "    \"entity_recognition\": {\n",
    "        \"cancer_specific_extraction\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "processor_sentence = ClinicalProcessor(config_sentence)\n",
    "\n",
    "# Create a longer text for testing\n",
    "long_text = clinical_text * 5\n",
    "\n",
    "result_sentence = processor_sentence.process_text(long_text)\n",
    "\n",
    "print(\"Sentence Segmentation + Sliding Window:\")\n",
    "print(f\"  Text Length: {len(result_sentence['text'])} characters\")\n",
    "print(f\"  Strategy: {result_sentence.get('tokenization', {}).get('tokenization_strategy', 'N/A')}\")\n",
    "if 'num_windows' in result_sentence.get('tokenization', {}):\n",
    "    print(f\"  Windows: {result_sentence['tokenization']['num_windows']}\")\n",
    "print(f\"  Entities: {len(result_sentence['entities'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Important Segments Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for important segments\n",
    "config_important = {\n",
    "    \"tokenization\": {\n",
    "        \"model\": \"gatortron\",\n",
    "        \"long_document_strategy\": \"important_segments\"\n",
    "    }\n",
    "}\n",
    "\n",
    "processor_important = ClinicalProcessor(config_important)\n",
    "result_important = processor_important.process_text(long_text)\n",
    "\n",
    "tokenization = result_important.get('tokenization', {})\n",
    "print(\"\\nImportant Segments Strategy:\")\n",
    "print(f\"  Text Length: {len(result_important['text'])} characters\")\n",
    "print(f\"  Strategy: {tokenization.get('tokenization_strategy', 'N/A')}\")\n",
    "\n",
    "if 'num_segments_selected' in tokenization:\n",
    "    selected = tokenization['num_segments_selected']\n",
    "    total = tokenization['num_segments_total']\n",
    "    percentage = (selected / total * 100) if total > 0 else 0\n",
    "    print(f\"  Segments: {selected}/{total} ({percentage:.1f}% selected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full entity recognition configuration\n",
    "config_full_entities = {\n",
    "    \"entity_recognition\": {\n",
    "        \"use_rules\": True,\n",
    "        \"use_patterns\": True,\n",
    "        \"cancer_specific_extraction\": True,\n",
    "        \"temporal_extraction\": True,\n",
    "        \"abbreviation_expansion\": True,\n",
    "        \"ontologies\": [\"snomed_ct\", \"rxnorm\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "processor_entities = ClinicalProcessor(config_full_entities)\n",
    "result_entities = processor_entities.process_text(clinical_text)\n",
    "\n",
    "print(\"\\nFull Entity Recognition Configuration:\")\n",
    "print(f\"  Entities Found: {len(result_entities['entities'])}\")\n",
    "\n",
    "# Count cancer-specific entities\n",
    "cancer_types = ['tumor', 'staging', 'biomarker', 'measurement']\n",
    "cancer_entities = [e for e in result_entities['entities'] if e['type'] in cancer_types]\n",
    "print(f\"  Cancer-Specific: {len(cancer_entities)}\")\n",
    "\n",
    "# Show sample cancer entities\n",
    "print(\"\\n  Sample Cancer Entities:\")\n",
    "for entity in cancer_entities[:5]:\n",
    "    props = entity.get('properties', {})\n",
    "    pattern = props.get('pattern', 'N/A')\n",
    "    print(f\"    ‚Ä¢ {entity['text']:25s} [{entity['type']:12s}] - {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ Section 4: Embedding Generation\n",
    "\n",
    "Generate embeddings using biomedical language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Single Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for embedding\n",
    "sample_text = \"Patient diagnosed with stage IV lung adenocarcinoma.\"\n",
    "\n",
    "print(\"Generating Embedding...\")\n",
    "print(f\"Text: {sample_text}\\n\")\n",
    "\n",
    "try:\n",
    "    # Generate embedding with GatorTron\n",
    "    embedding = processor.generate_embeddings(\n",
    "        text=sample_text,\n",
    "        model_name=\"gatortron\",\n",
    "        pooling_method=\"mean\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Embedding Generated Successfully\")\n",
    "    print(f\"\\nEmbedding Details:\")\n",
    "    print(f\"  Shape: {embedding.shape}\")\n",
    "    print(f\"  Dimensions: {embedding.shape[1]}\")\n",
    "    print(f\"  Mean: {np.mean(embedding):.4f}\")\n",
    "    print(f\"  Std: {np.std(embedding):.4f}\")\n",
    "    print(f\"  L2 Norm: {np.linalg.norm(embedding):.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Embedding generation requires model download: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple clinical texts\n",
    "clinical_texts = [\n",
    "    \"Patient with breast cancer ER+ HER2-\",\n",
    "    \"Lung adenocarcinoma stage IV with metastases\",\n",
    "    \"Colorectal cancer with liver involvement\",\n",
    "    \"Prostate cancer Gleason score 7\"\n",
    "]\n",
    "\n",
    "print(\"Generating Batch Embeddings...\")\n",
    "print(f\"Number of texts: {len(clinical_texts)}\\n\")\n",
    "\n",
    "try:\n",
    "    # Generate embeddings for all texts\n",
    "    embeddings = processor.generate_embeddings(\n",
    "        text=clinical_texts,\n",
    "        model_name=\"gatortron\",\n",
    "        batch_size=2\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Batch Embeddings Generated\")\n",
    "    print(f\"\\nEmbeddings Shape: {embeddings.shape}\")\n",
    "    print(f\"  - {embeddings.shape[0]} texts\")\n",
    "    print(f\"  - {embeddings.shape[1]} dimensions each\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Compute pairwise similarities\n",
    "    print(\"\\nPairwise Cosine Similarities:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(len(clinical_texts)):\n",
    "        for j in range(i + 1, len(clinical_texts)):\n",
    "            sim = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])\n",
    "            )\n",
    "            print(f\"Text {i+1} ‚Üî Text {j+1}: {sim:.4f}\")\n",
    "            \n",
    "    # Find most similar pair\n",
    "    max_sim = 0\n",
    "    max_pair = (0, 0)\n",
    "    \n",
    "    for i in range(len(clinical_texts)):\n",
    "        for j in range(i + 1, len(clinical_texts)):\n",
    "            sim = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])\n",
    "            )\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                max_pair = (i, j)\n",
    "    \n",
    "    i, j = max_pair\n",
    "    print(f\"\\n‚úì Most Similar Texts (similarity: {max_sim:.4f}):\")\n",
    "    print(f\"  1. {clinical_texts[i]}\")\n",
    "    print(f\"  2. {clinical_texts[j]}\")\n",
    "    \n",
    "except:\n",
    "    print(\"Skipping similarity computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Different Pooling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_methods = [\"mean\", \"cls\", \"max\"]\n",
    "test_text = clinical_texts[0]\n",
    "\n",
    "print(f\"\\nComparing Pooling Methods:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pooling_results = {}\n",
    "\n",
    "for pooling in pooling_methods:\n",
    "    try:\n",
    "        emb = processor.generate_embeddings(\n",
    "            text=test_text,\n",
    "            model_name=\"gatortron\",\n",
    "            pooling_method=pooling\n",
    "        )\n",
    "        pooling_results[pooling] = emb\n",
    "        \n",
    "        print(f\"\\n{pooling.upper():12s}\")\n",
    "        print(f\"  Shape: {emb.shape}\")\n",
    "        print(f\"  Norm:  {np.linalg.norm(emb):.4f}\")\n",
    "        print(f\"  Mean:  {np.mean(emb):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{pooling.upper():12s} - Error: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß¨ Section 5: Advanced Entity Extraction\n",
    "\n",
    "Deep dive into entity extraction capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive clinical text for entity extraction\n",
    "comprehensive_text = \"\"\"\n",
    "PATHOLOGY REPORT\n",
    "\n",
    "Patient: John Doe, Age 58\n",
    "Date: March 20, 2024\n",
    "\n",
    "FINAL DIAGNOSIS: Invasive Ductal Carcinoma, Left Breast, Stage IIA\n",
    "\n",
    "CLINICAL HISTORY:\n",
    "Patient presented on February 15, 2024 with a palpable 1.8 cm mass in the left\n",
    "upper outer quadrant. Core biopsy performed on February 25, 2024 confirmed\n",
    "invasive carcinoma.\n",
    "\n",
    "GROSS DESCRIPTION:\n",
    "Lumpectomy specimen measuring 5 x 4 x 3 cm. Sectioning reveals a 1.8 x 1.5 x 1.2 cm\n",
    "firm, gray-white tumor.\n",
    "\n",
    "MICROSCOPIC FINDINGS:\n",
    "Invasive ductal carcinoma, Grade 2 (Nottingham score 6)\n",
    "- Tubule formation: 20% (score 2)\n",
    "- Nuclear pleomorphism: Moderate (score 2)\n",
    "- Mitotic rate: 8 per 10 HPF (score 2)\n",
    "\n",
    "IMMUNOHISTOCHEMISTRY:\n",
    "ER: Positive (95%, strong)\n",
    "PR: Positive (80%, moderate)\n",
    "HER2: Negative (IHC 1+)\n",
    "Ki-67: 18%\n",
    "\n",
    "MOLECULAR:\n",
    "EGFR: Wild-type\n",
    "KRAS: Wild-type  \n",
    "BRAF: No mutation\n",
    "PD-L1: Positive (15%)\n",
    "\n",
    "TNM: pT1c N0 M0, Stage IA\n",
    "\n",
    "MARGINS: All negative (>2mm)\n",
    "\n",
    "LYMPH NODES: 0/3 sentinel nodes positive\n",
    "\n",
    "TREATMENT:\n",
    "Started tamoxifen 20 mg PO daily on March 25, 2024\n",
    "AC chemotherapy (doxorubicin 60 mg/m¬≤ + cyclophosphamide 600 mg/m¬≤) initiated April 1, 2024\n",
    "Radiation therapy planned for June 2024\n",
    "Follow-up: May 1, 2024\n",
    "\"\"\"\n",
    "\n",
    "# Process with full entity recognition\n",
    "config_comprehensive = {\n",
    "    \"entity_recognition\": {\n",
    "        \"use_rules\": True,\n",
    "        \"use_patterns\": True,\n",
    "        \"cancer_specific_extraction\": True,\n",
    "        \"temporal_extraction\": True,\n",
    "        \"ontologies\": [\"snomed_ct\", \"rxnorm\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "processor_comp = ClinicalProcessor(config_comprehensive)\n",
    "result_comp = processor_comp.process_text(comprehensive_text)\n",
    "\n",
    "print(\"Comprehensive Entity Extraction Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Entities: {len(result_comp['entities'])}\")\n",
    "print(f\"Relationships: {len(result_comp['entity_relationships'])}\")\n",
    "print(f\"Timeline Events: {len(result_comp['temporal_timeline'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cancer-specific entities\n",
    "print(\"\\nCancer-Specific Entities by Type:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cancer_types = ['tumor', 'staging', 'biomarker', 'measurement']\n",
    "\n",
    "for cancer_type in cancer_types:\n",
    "    entities = [e for e in result_comp['entities'] if e['type'] == cancer_type]\n",
    "    \n",
    "    if entities:\n",
    "        print(f\"\\n{cancer_type.upper()} ({len(entities)} found):\")\n",
    "        for entity in entities[:5]:  # Show first 5\n",
    "            text = entity['text']\n",
    "            props = entity.get('properties', {})\n",
    "            pattern = props.get('pattern', 'N/A')\n",
    "            print(f\"  ‚Ä¢ {text:30s} (pattern: {pattern})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display biomarker information\n",
    "biomarkers = [e for e in result_comp['entities'] if e['type'] == 'biomarker']\n",
    "\n",
    "if biomarkers:\n",
    "    print(\"\\nBiomarker Status:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for biomarker in biomarkers:\n",
    "        text = biomarker['text']\n",
    "        props = biomarker.get('properties', {})\n",
    "        \n",
    "        print(f\"\\n{text}\")\n",
    "        if props.get('biomarker'):\n",
    "            print(f\"  Marker: {props['biomarker']}\")\n",
    "        if props.get('status'):\n",
    "            print(f\"  Status: {props['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "stats = processor_comp.get_summary_statistics(result_comp)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text Length: {stats['text_length']} characters\")\n",
    "print(f\"Total Entities: {stats['num_entities']}\")\n",
    "print(f\"Relationships: {stats['num_relationships']}\")\n",
    "print(f\"Timeline Events: {stats['num_timeline_events']}\")\n",
    "\n",
    "print(\"\\nEntity Distribution:\")\n",
    "for entity_type, count in sorted(stats['entity_types'].items()):\n",
    "    percentage = (count / stats['num_entities'] * 100) if stats['num_entities'] > 0 else 0\n",
    "    bar = \"‚ñà\" * min(int(percentage / 2), 50)\n",
    "    print(f\"  {entity_type:15s} {count:3d} ({percentage:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üîó Section 6: HoneyBee Main API\n",
    "\n",
    "Use the high-level HoneyBee API for clinical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HoneyBee with custom configuration\n",
    "honeybee_config = {\n",
    "    \"clinical\": {\n",
    "        \"tokenization\": {\n",
    "            \"model\": \"gatortron\"\n",
    "        },\n",
    "        \"entity_recognition\": {\n",
    "            \"cancer_specific_extraction\": True,\n",
    "            \"temporal_extraction\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "honeybee = HoneyBee(config=honeybee_config)\n",
    "\n",
    "print(\"‚úì HoneyBee Initialized\")\n",
    "print(f\"Configuration: Custom clinical settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Process Clinical Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text using HoneyBee API\n",
    "sample_clinical = \"Patient diagnosed with stage III breast cancer, ER+/PR+/HER2-, started on tamoxifen therapy.\"\n",
    "\n",
    "result_hb = honeybee.process_clinical(text=sample_clinical)\n",
    "\n",
    "print(\"HoneyBee API - Process Clinical Text\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput: {sample_clinical}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Entities Found: {len(result_hb['entities'])}\")\n",
    "\n",
    "if result_hb['entities']:\n",
    "    print(f\"\\n  Extracted Entities:\")\n",
    "    for entity in result_hb['entities']:\n",
    "        print(f\"    ‚Ä¢ {entity['text']:30s} [{entity['type']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Single text embedding\n",
    "    embedding_single = honeybee.generate_embeddings(\n",
    "        data=sample_clinical,\n",
    "        modality=\"clinical\",\n",
    "        model_name=\"gatortron\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHoneyBee API - Generate Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Single text embedding: {embedding_single.shape}\")\n",
    "    \n",
    "    # Batch embeddings\n",
    "    texts = [\n",
    "        \"Patient with lung cancer\",\n",
    "        \"Started chemotherapy treatment\",\n",
    "        \"Follow-up shows partial response\"\n",
    "    ]\n",
    "    \n",
    "    embeddings_batch = honeybee.generate_embeddings(\n",
    "        data=texts,\n",
    "        modality=\"clinical\",\n",
    "        model_name=\"gatortron\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch embeddings: {embeddings_batch.shape}\")\n",
    "    print(f\"  - {embeddings_batch.shape[0]} texts\")\n",
    "    print(f\"  - {embeddings_batch.shape[1]} dimensions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Embeddings: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Multimodal Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate embeddings from different sources\n",
    "    emb1 = honeybee.generate_embeddings(\n",
    "        data=\"Patient with breast cancer\",\n",
    "        modality=\"clinical\"\n",
    "    )\n",
    "    \n",
    "    emb2 = honeybee.generate_embeddings(\n",
    "        data=\"ER positive HER2 negative\",\n",
    "        modality=\"clinical\"\n",
    "    )\n",
    "    \n",
    "    # Integrate embeddings\n",
    "    integrated = honeybee.integrate_embeddings([emb1, emb2])\n",
    "    \n",
    "    print(\"\\nMultimodal Integration:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Embedding 1: {emb1.shape}\")\n",
    "    print(f\"Embedding 2: {emb2.shape}\")\n",
    "    print(f\"Integrated:  {integrated.shape}\")\n",
    "    print(f\"\\n‚úì Successfully integrated {len([emb1, emb2])} embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Integration: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üì¶ Section 7: Batch Processing\n",
    "\n",
    "Process multiple documents efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example uses processor.process_batch()\n",
    "# For demonstration, we'll show how it would be used\n",
    "\n",
    "print(\"Batch Processing Example\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "To process multiple documents:\n",
    "\n",
    "```python\n",
    "# Process all PDFs in a directory\n",
    "results = processor.process_batch(\n",
    "    input_dir=\"./clinical_reports\",\n",
    "    file_pattern=\"*.pdf\",\n",
    "    save_output=True\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "for result in results:\n",
    "    if 'error' not in result:\n",
    "        print(f\"File: {result['file_name']}\")\n",
    "        print(f\"  Entities: {len(result['entities'])}\")\n",
    "        print(f\"  Timeline Events: {len(result['temporal_timeline'])}\")\n",
    "```\n",
    "\n",
    "Features:\n",
    "- Automatic file discovery with glob patterns\n",
    "- Parallel processing support\n",
    "- JSON output generation\n",
    "- Error handling per file\n",
    "- Progress tracking\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ Section 8: Complete Workflow Examples\n",
    "\n",
    "End-to-end examples for common use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Cancer Patient Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cancer_patient(text_or_path):\n",
    "    \"\"\"\n",
    "    Complete pipeline for cancer patient analysis\n",
    "    \"\"\"\n",
    "    # Initialize with full configuration\n",
    "    config = {\n",
    "        \"entity_recognition\": {\n",
    "            \"cancer_specific_extraction\": True,\n",
    "            \"temporal_extraction\": True,\n",
    "            \"use_rules\": True,\n",
    "            \"use_patterns\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    processor = ClinicalProcessor(config)\n",
    "    \n",
    "    # Process input\n",
    "    if Path(text_or_path).exists():\n",
    "        result = processor.process(text_or_path)\n",
    "    else:\n",
    "        result = processor.process_text(text_or_path)\n",
    "    \n",
    "    # Extract key information\n",
    "    analysis = {\n",
    "        'tumor_entities': [e for e in result['entities'] if e['type'] == 'tumor'],\n",
    "        'staging': [e for e in result['entities'] if e['type'] == 'staging'],\n",
    "        'biomarkers': [e for e in result['entities'] if e['type'] == 'biomarker'],\n",
    "        'timeline': result['temporal_timeline'],\n",
    "        'total_entities': len(result['entities'])\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage\n",
    "example_text = \"\"\"\n",
    "Patient with invasive ductal carcinoma, ER+/PR+/HER2-, Grade 2, Stage IIA.\n",
    "Tumor size 2.3 cm. Started on tamoxifen 20mg daily.\n",
    "\"\"\"\n",
    "\n",
    "analysis = analyze_cancer_patient(example_text)\n",
    "\n",
    "print(\"Cancer Patient Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Entities: {analysis['total_entities']}\")\n",
    "print(f\"Tumor Mentions: {len(analysis['tumor_entities'])}\")\n",
    "print(f\"Staging Info: {len(analysis['staging'])}\")\n",
    "print(f\"Biomarkers: {len(analysis['biomarkers'])}\")\n",
    "print(f\"Timeline Events: {len(analysis['timeline'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Similarity Search Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_cases(query_text, case_database, top_k=3):\n",
    "    \"\"\"\n",
    "    Find similar clinical cases using embeddings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processor = ClinicalProcessor()\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_emb = processor.generate_embeddings(\n",
    "            text=query_text,\n",
    "            model_name=\"gatortron\"\n",
    "        )\n",
    "        \n",
    "        # Generate database embeddings\n",
    "        db_embs = processor.generate_embeddings(\n",
    "            text=case_database,\n",
    "            model_name=\"gatortron\"\n",
    "        )\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for i, db_emb in enumerate(db_embs):\n",
    "            sim = np.dot(query_emb.flatten(), db_emb) / (\n",
    "                np.linalg.norm(query_emb) * np.linalg.norm(db_emb)\n",
    "            )\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        # Sort and get top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:100]}\")\n",
    "        return []\n",
    "\n",
    "# Example database\n",
    "case_db = [\n",
    "    \"Breast cancer ER+ HER2- stage II\",\n",
    "    \"Lung adenocarcinoma EGFR mutant\",\n",
    "    \"Invasive ductal carcinoma grade 2\",\n",
    "    \"Colorectal cancer with liver mets\"\n",
    "]\n",
    "\n",
    "query = \"Breast carcinoma hormone receptor positive\"\n",
    "\n",
    "print(\"Similarity Search Example\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "similar = find_similar_cases(query, case_db, top_k=3)\n",
    "\n",
    "if similar:\n",
    "    print(\"Top Similar Cases:\")\n",
    "    for idx, (case_idx, similarity) in enumerate(similar, 1):\n",
    "        print(f\"{idx}. {case_db[case_idx]}\")\n",
    "        print(f\"   Similarity: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Summary and Best Practices\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "‚úÖ **Document Processing**\n",
    "- PDF, images with OCR\n",
    "- Direct text processing\n",
    "- Batch processing\n",
    "- EHR format support\n",
    "\n",
    "‚úÖ **Entity Extraction**\n",
    "- Cancer-specific entities\n",
    "- Temporal information\n",
    "- Entity relationships\n",
    "- Ontology mapping\n",
    "\n",
    "‚úÖ **Embeddings**\n",
    "- Multiple biomedical models\n",
    "- Different pooling methods\n",
    "- Batch processing\n",
    "- Similarity computation\n",
    "\n",
    "‚úÖ **Configuration**\n",
    "- Flexible tokenization\n",
    "- Long document strategies\n",
    "- Custom entity recognition\n",
    "- Output customization\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Choose the right model**\n",
    "   - GatorTron: Clinical text (1024 dims)\n",
    "   - BioClinicalBERT: Clinical reports (768 dims)\n",
    "   - PubMedBERT: Research literature (768 dims)\n",
    "\n",
    "2. **Configure appropriately**\n",
    "   - Use `important_segments` for long documents\n",
    "   - Enable `cancer_specific_extraction` for oncology\n",
    "   - Enable `temporal_extraction` for timelines\n",
    "\n",
    "3. **Optimize performance**\n",
    "   - Use batch processing for multiple texts\n",
    "   - Configure appropriate batch sizes\n",
    "   - Cache embeddings when possible\n",
    "\n",
    "4. **Handle errors gracefully**\n",
    "   - Check for OCR quality\n",
    "   - Validate entity extraction\n",
    "   - Monitor processing times\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Documentation**: https://lab-rasool.github.io/HoneyBee/docs/clinical-processing/\n",
    "- **GitHub**: https://github.com/lab-rasool/HoneyBee\n",
    "- **Issues**: https://github.com/lab-rasool/HoneyBee/issues\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
